{"pageProps":{"note":{"id":"bpw92y9ez0959qtnaedllfs","title":"How Pinterest scaled to 11 million users with only 6 engineers","desc":"","updated":1703654866126,"created":1703654655270,"custom":{},"fname":"dev.software-engineering.how-pinterest-scaled-to-11-million","type":"note","vault":{"fsPath":"vault"},"contentHash":"0b6fe1749a1c03be4c02b0769fb68307","links":[],"anchors":{"lessons-from-scaling-pinterest":{"type":"header","text":"Lessons from Scaling Pinterest","value":"lessons-from-scaling-pinterest","line":18,"column":0,"depth":2},"march-2010-closed-beta-launch-1-engineer":{"type":"header","text":"March 2010: Closed beta launch, 1 engineer","value":"march-2010-closed-beta-launch-1-engineer","line":29,"column":0,"depth":2},"january-2011-10000-users-2-engineers":{"type":"header","text":"January 2011: 10,000 users, 2 engineers","value":"january-2011-10000-users-2-engineers","line":35,"column":0,"depth":2},"october-2011-32-million-users-3-engineers":{"type":"header","text":"October 2011: 3.2 million users, 3 engineers","value":"october-2011-32-million-users-3-engineers","line":51,"column":0,"depth":2},"Ô∏è-clustering-gone-wrong":{"type":"header","text":"‚ö†Ô∏è Clustering gone wrong","value":"Ô∏è-clustering-gone-wrong","line":85,"column":0,"depth":3},"january-2012-11-million-users-6-engineers":{"type":"header","text":"January 2012: 11 million users, 6 engineers","value":"january-2012-11-million-users-6-engineers","line":111,"column":0,"depth":2},"how-pinterest-manually-sharded-their-databases":{"type":"header","text":"How Pinterest manually sharded their databases","value":"how-pinterest-manually-sharded-their-databases","line":133,"column":0,"depth":3},"an-small-example-of-manual-sharding":{"type":"header","text":"An small example of manual sharding","value":"an-small-example-of-manual-sharding","line":151,"column":0,"depth":4},"october-2012-22-million-users-40-engineers":{"type":"header","text":"October 2012: 22 million users, 40 engineers","value":"october-2012-22-million-users-40-engineers","line":161,"column":0,"depth":2},"pinterests-database-structuring":{"type":"header","text":"Pinterest‚Äôs Database Structuring","value":"pinterests-database-structuring","line":187,"column":0,"depth":2},"ids":{"type":"header","text":"IDs","value":"ids","line":189,"column":0,"depth":3},"tables":{"type":"header","text":"Tables","value":"tables","line":209,"column":0,"depth":3}},"children":[],"parent":"j0N1aVKxe96dktmyADG9U","data":{}},"body":"<h1 id=\"how-pinterest-scaled-to-11-million-users-with-only-6-engineers\">How Pinterest scaled to 11 million users with only 6 engineers<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#how-pinterest-scaled-to-11-million-users-with-only-6-engineers\"></a></h1>\n<blockquote>\n<p><a href=\"https://read.engineerscodex.com/p/how-pinterest-scaled-to-11-million\">https://read.engineerscodex.com/p/how-pinterest-scaled-to-11-million</a></p>\n</blockquote>\n<p>In January 2012, Pinterest hit 11.7 million monthly unique users with only 6 engineers.</p>\n<p>Having launched in March 2010, it was <a href=\"https://techcrunch.com/2012/02/07/pinterest-monthly-uniques/#:~:text=11.7%20million%20unique%20monthly%20U.S.%20visitors%2C%20crossing%20the%2010%20million%20mark%20faster%20than%20any%20other%20standalone%20site%20in%20history.\">the fastest company to race past 10 million monthly users at the time</a>.</p>\n<p><a href=\"https://pinterest.com/\">Pinterest</a> is an image-heavy social network, where users can save or ‚Äúpin‚Äù images to their boards.</p>\n<blockquote>\n<p>When I say ‚Äúusers‚Äù below, I mean ‚Äúmonthly active users‚Äù (MAUs).</p>\n</blockquote>\n<h2 id=\"lessons-from-scaling-pinterest\"><strong>Lessons from Scaling Pinterest</strong><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lessons-from-scaling-pinterest\"></a></h2>\n<ul>\n<li><strong>Use known, proven technologies.</strong> Pinterest‚Äôs dive into newer technologies at the time led to issues like data corruption.</li>\n<li><strong>Keep it simple.</strong> (A recurring theme!)</li>\n<li><strong>Don‚Äôt get too creative.</strong> The team settled on an architecture where they could add more of the same nodes to scale.</li>\n<li><strong>Limit your options</strong>.</li>\n<li><strong>Sharding databases > clustering.</strong> It reduced data transfer across nodes, which was a good thing.</li>\n<li><strong>Have fun!</strong> New engineers would contribute code in their first week.</li>\n</ul>\n<p><a href=\"https://engineercodex.substack.com/p/how-instagram-scaled-to-14-million\">The Instagram team had similar lessons from scaling to 14 million users with 3 engineers</a>.</p>\n<h2 id=\"march-2010-closed-beta-launch-1-engineer\"><strong>March 2010: Closed beta launch, 1 engineer</strong><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#march-2010-closed-beta-launch-1-engineer\"></a></h2>\n<p>Pinterest launched in March 2010 with 1 small MySQL database, 1 small web server, and 1 engineer (along with the 2 co-founders).</p>\n<p><a href=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d5d064-82c0-4d31-916d-70bfeb1527a9_1124x448.png\">https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d5d064-82c0-4d31-916d-70bfeb1527a9_1124x448.png</a></p>\n<h2 id=\"january-2011-10000-users-2-engineers\"><strong>January 2011: 10,000 users, 2 engineers</strong><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#january-2011-10000-users-2-engineers\"></a></h2>\n<p>Nine months later in January 2011, Pinterest‚Äôs architecture had evolved to handle more users. They were still invite-only and had 2 engineers.</p>\n<p>They had:</p>\n<ul>\n<li>a basic web server stack (Amazon EC2, S3, and CloudFront)\n<ul>\n<li>Django (Python) for their backend</li>\n</ul>\n</li>\n<li>4 web servers for redundancy</li>\n<li>NGINX as their reverse proxy and load balancer.</li>\n<li>1 MySQL database at this point + 1 read-only secondary</li>\n<li>MongoDB for counters</li>\n<li>1 task queue and 2 task processors for asynchronous tasks</li>\n</ul>\n<p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd7c12c-bdc0-494d-ac86-b65f09b2a2f6_1255x615.png\">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd7c12c-bdc0-494d-ac86-b65f09b2a2f6_1255x615.png</a></p>\n<h2 id=\"october-2011-32-million-users-3-engineers\"><strong>October 2011: 3.2 million users, 3 engineers</strong><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#october-2011-32-million-users-3-engineers\"></a></h2>\n<p>From January 2011 to October 2011, Pinterest grew extremely fast, doubling users every month and a half.</p>\n<p>Their iPhone app launch in March 2011 was one of the factors fueling this growth.</p>\n<p>When things grow fast, technology breaks more often than you expect.</p>\n<p>Pinterest made a mistake: <strong>they over-complicated their architecture immensely.</strong></p>\n<p>They had only 3 engineers, but 5 different database technologies for their data.</p>\n<p>They were both manually sharding their MySQL databases and clustering their data using Cassandra and Membase (now Couchbase).</p>\n<p><strong><a href=\"https://www.infoq.com/presentations/Pinterest/\">Their ‚Äúovercomplicated stack\"</a>:</strong></p>\n<ul>\n<li>Web server stack (EC2 + S3 + CloudFront)\n<ul>\n<li><a href=\"https://www.quora.com/What-challenges-has-Pinterest-encountered-with-Flask\">Pinterest started moving to Flask (Python) for their backend</a></li>\n</ul>\n</li>\n<li>16 web servers</li>\n<li>2 API engines</li>\n<li>2 NGINX proxies</li>\n<li>5 manually-sharded MySQL DBs + 9 read-only secondaries</li>\n<li>4 Cassandra Nodes</li>\n<li>15 Membase Nodes (3 separate clusters)</li>\n<li>8 Memcache Nodes</li>\n<li>10 Redis Nodes</li>\n<li>3 Task Routers + 4 Task Processors</li>\n<li>4 Elastic Search Nodes</li>\n<li>3 Mongo Clusters</li>\n</ul>\n<p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F386fd87b-f932-4310-82cf-07860bc36e98_1357x1047.png\">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F386fd87b-f932-4310-82cf-07860bc36e98_1357x1047.png</a></p>\n<p>Subscribe</p>\n<h3 id=\"Ô∏è-clustering-gone-wrong\">‚ö†Ô∏è Clustering gone wrong<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#Ô∏è-clustering-gone-wrong\"></a></h3>\n<blockquote>\n<p><strong>Database clustering</strong> is the process of connecting multiple database servers to work together as a single system.</p>\n</blockquote>\n<p>In theory, clustering automatically scales datastores, provides high availability, free load balancing, and doesn‚Äôt have a single point of failure.</p>\n<p>Unfortunately, in practice, clustering was overly complex, had difficult upgrade mechanisms, and <strong>it had a big single point of failure.</strong></p>\n<p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2198073-6421-434e-be2f-2904aa5ff975_1462x645.png\">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2198073-6421-434e-be2f-2904aa5ff975_1462x645.png</a></p>\n<p>Each DB has a Cluster Management Algorithm that routes from DB to DB.</p>\n<p>When something goes wrong with a DB, a new DB is added to replace it.</p>\n<p>In theory, the Cluster Management Algorithm should handle this just fine.</p>\n<p>In reality, there was a bug in Pinterest‚Äôs Cluster Management Algorithm that <strong>corrupted data on all their nodes, broke their data rebalancing, and created some unfixable problems</strong>.</p>\n<p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffabc91cd-53b2-47f2-9d71-c50e7a3824aa_1306x538.png\">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffabc91cd-53b2-47f2-9d71-c50e7a3824aa_1306x538.png</a></p>\n<p>Pinterest‚Äôs solution? <strong>Remove all clustering tech (Cassandra, Membase) from the system. Go all-in with MySQL + Memcached (more proven).</strong></p>\n<p>MySQL and Memcached are well-proven technologies. <a href=\"https://engineercodex.substack.com/p/how-facebook-scaled-memcached\">Facebook used the two to create the largest Memcached system in the world, which handled billions of requests per second for them with ease.</a></p>\n<p>Subscribe</p>\n<h2 id=\"january-2012-11-million-users-6-engineers\"><strong>January 2012: 11 million users, 6 engineers</strong><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#january-2012-11-million-users-6-engineers\"></a></h2>\n<p>In January 2012, Pinterest was handling ~11 million monthly active users, with anywhere between 12 million to 21 million daily users.</p>\n<p>At this point, Pinterest had taken the time to simplify their architecture.</p>\n<p>They removed less-proven ideas, like clustering and Cassandra at the time, and replaced them with proven ones, like MySQL, Memcache, and sharding.</p>\n<p><strong>Their simplified stack:</strong></p>\n<ul>\n<li>Amazon EC2 + S3 + <a href=\"https://www.akamai.com/\">Akamai</a> (replaced CloudFront)</li>\n<li><a href=\"https://aws.amazon.com/elasticloadbalancing/\">AWS ELB (Elastic Load Balancing)</a></li>\n<li>90 Web Engines + 50 API Engines (<a href=\"https://www.quora.com/What-challenges-has-Pinterest-encountered-with-Flask\">using Flask</a>)</li>\n<li>66 MySQL DBs + 66 secondaries</li>\n<li>59 Redis Instances</li>\n<li>51 Memcache Instances</li>\n<li>1 Redis Task Manager + 25 Task Processors</li>\n<li>Sharded <a href=\"https://solr.apache.org/\">Apache Solr</a> (replaced Elasticsearch)</li>\n<li><strong>Removed Cassanda, Membase, Elasticsearch, MongoDB, NGINX</strong></li>\n</ul>\n<p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34e259af-7bfe-4734-ab56-f793dabe2cb2_1608x767.png\">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34e259af-7bfe-4734-ab56-f793dabe2cb2_1608x767.png</a></p>\n<h3 id=\"how-pinterest-manually-sharded-their-databases\">How Pinterest manually sharded their databases<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#how-pinterest-manually-sharded-their-databases\"></a></h3>\n<blockquote>\n<p><strong>Database sharding</strong> is a method of splitting a single dataset into multiple databases.</p>\n<p><strong>Benefits:</strong> high availability, load balancing, simple algorithm for placing data, easy to split databases to add more capacity, easy to locate data</p>\n</blockquote>\n<p>When Pinterest first sharded their databases, they had a feature freeze. Over the span of a few months, <strong>they sharded their databases incrementally and manually:</strong></p>\n<p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d7cfda-8807-4adf-a927-7e7f3e9faaf5_789x443.png\">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d7cfda-8807-4adf-a927-7e7f3e9faaf5_789x443.png</a></p>\n<p><a href=\"https://www.infoq.com/presentations/Pinterest/\">Source: Scaling Pinterest</a></p>\n<p>The team removed table joins and complex queries from the database layer. They added lots of caching.</p>\n<p>Since it was extra effort to maintain unique constraints across databases, they kept data like usernames and emails in a huge, unsharded database.</p>\n<p>All their tables existed on all their shards.</p>\n<h4 id=\"an-small-example-of-manual-sharding\">An small example of manual sharding<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#an-small-example-of-manual-sharding\"></a></h4>\n<p>Since they had billions of ‚Äúpins‚Äù, their database indexes ran out of memory.</p>\n<p>They would take the largest table on the database and move it to its own database.</p>\n<p>Then, when that database ran out of space, they would shard.</p>\n<p>Subscribe</p>\n<h2 id=\"october-2012-22-million-users-40-engineers\"><strong>October 2012: 22 million users, 40 engineers</strong><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#october-2012-22-million-users-40-engineers\"></a></h2>\n<p>In October 2012, Pinterest had around 22 million monthly users, but their engineering team had quadrupled to 40 engineers.</p>\n<p><strong>The architecture was the same. They just added more of the same systems.</strong></p>\n<ul>\n<li>Amazon EC2 + S3 + CDNs (EdgeCast, Akamai, Level 3)</li>\n<li>180 web servers + 240 API engines (using Flask)</li>\n<li>88 MySQL DBs + 88 secondaries each</li>\n<li>110 Redis instances</li>\n<li>200 Memcache instances</li>\n<li>4 Redis Task Managers + 80 Task Processors</li>\n<li>Sharded Apache Solr</li>\n</ul>\n<p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff18b5dd-2d71-4d8b-864a-4455e374bc62_1608x767.png\">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff18b5dd-2d71-4d8b-864a-4455e374bc62_1608x767.png</a></p>\n<p>They started moving from hard disk drives to SSDs.</p>\n<p>An important lesson learned: <strong>limited, proven choices was a good thing</strong>.</p>\n<p>Sticking with EC2 and S3 meant they had limited configuration choices, leading to less headaches and more simplicity.</p>\n<p><strong>However, new instances could be ready in seconds.</strong> This meant that they could add 10 Memcache instances in a matter of minutes.</p>\n<p>Subscribe</p>\n<h2 id=\"pinterests-database-structuring\"><strong>Pinterest‚Äôs Database Structuring</strong><a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pinterests-database-structuring\"></a></h2>\n<h3 id=\"ids\">IDs<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#ids\"></a></h3>\n<p><a href=\"https://engineercodex.substack.com/p/how-instagram-scaled-to-14-million\">Like Instagram</a>, Pinterest had a unique ID structure because they had sharded databases.</p>\n<p>Their 64-bit ID looked like:</p>\n<p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F269be284-a074-4898-857b-2a8903ba4b48_590x110.png\">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F269be284-a074-4898-857b-2a8903ba4b48_590x110.png</a></p>\n<p><a href=\"https://www.infoq.com/presentations/Pinterest/\">Source: Scaling Pinterest</a></p>\n<blockquote>\n<p><strong>Shard ID:</strong> which shard (16 bits)</p>\n<p><strong>Type:</strong> object type, such as pins (10 bits)</p>\n<p><strong>Local ID:</strong> position in table (38 bits)</p>\n</blockquote>\n<p>The lookup structure for these IDs was <strong>a simple Python dictionary.</strong></p>\n<hr>\n<h3 id=\"tables\">Tables<a aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tables\"></a></h3>\n<p>They had Object tables and Mapping tables.</p>\n<p><strong>Object tables were for pins, boards, comments, users, and more.</strong> They had a Local ID mapped to a MySQL blob, like JSON.</p>\n<p><strong>Mapping tables were for relational data between objects, like mapping boards to a user or likes to a pin.</strong> They had a Full ID mapped to a Full ID and a timestamp.</p>\n<p>All queries were PK (primary key) or index lookups for efficiency. They cut out all JOINs.</p>\n<hr>\n<p><strong>This article is based on <a href=\"https://www.infoq.com/presentations/Pinterest/\">Scaling Pinterest</a>, a talk given by the Pinterest team in 2012.</strong></p>","noteIndex":{"id":"Iy0MoL0KnL55Br3AfTS2C","title":"Luke","desc":"","updated":1766965759366,"created":1644449449778,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"a724de3efd251cf89fe82a5860d9008b","links":[{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"life-tips","position":{"start":{"line":43,"column":5,"offset":2710},"end":{"line":43,"column":29,"offset":2734},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"life-tips","anchorHeader":"wodenokoto"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2026.articles","alias":"What I read in 2026","position":{"start":{"line":72,"column":3,"offset":4440},"end":{"line":72,"column":45,"offset":4482},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2026.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2025.articles","alias":"2025","position":{"start":{"line":73,"column":5,"offset":4487},"end":{"line":73,"column":32,"offset":4514},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2025.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2024.articles","alias":"2024","position":{"start":{"line":74,"column":5,"offset":4519},"end":{"line":74,"column":32,"offset":4546},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2024.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2023.articles","alias":"2023","position":{"start":{"line":75,"column":5,"offset":4551},"end":{"line":75,"column":32,"offset":4578},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2023.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2022.articles","alias":"2022","position":{"start":{"line":76,"column":5,"offset":4583},"end":{"line":76,"column":32,"offset":4610},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2022.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-struggled-brag-in","position":{"start":{"line":82,"column":3,"offset":4746},"end":{"line":82,"column":39,"offset":4782},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-struggled-brag-in"}}],"anchors":{"what-i-read-in-past":{"type":"header","text":"What I read in past","value":"what-i-read-in-past","line":76,"column":0,"depth":2}},"children":["zd4mq442jike0pr0wba1u3m","6hzeqsofq67gdk88flxlkhp","778ijii93yu5uwnrwmn5zi4","g1fngdjl25nes6fs3lip602","ZbdkdApFqLdks4Moq92R9","uoc5hhki3o4py15cesddu8q","9qf7j06jtdkm6rnx9ymvwb0","5zn10cvj7ajy2gh2is5nqmg","4qo9ma0z0yu1czns6pxl7y5","ok0e729ho7o09xetujkxc0m","GR5x8HnNFEN6fU2UBSEIK","yirtnlj8q24yutcf3ss1xqy","eq0wc6t7wl2wv221yb68ro4","7x2fnv4j6gxts08qk0jguny","ettkt3iClONnxpbGwBVLl","7l4knev6v613tbuoskvmbdg","hvh5bud6yp7dc89tuh95tr9","4fvoqrplw0cweo554usbjos","f8qsfql0a9v8thpeo82udfa","1swsbrhqi9jk41v9eodyi5q","SQqYupi6EFddTerBA8RRD","hjNeNc1F2JUh0lTWanH4h","qf0l4wbrc9jgooyzexmbq5v","o7xruzrah5wzqetottecss7","z1zo2mp6ddji5p317i4x9xw","v06c2tjelh341x4resa50fh","0yqesk4rcffwgyuab5x8rfa","sy2vkbtyu671chkvgn1yt8j","ufixpmxoydiccoh59kphrib","alswadkx4wb05y1z9iwfzfv","1daut9dpw70xd0zh5a7j5p4"],"parent":null,"data":{},"body":"\nHi there üëã. I'm a Front-end developer.\n\n---\n\n- ÌòÑÏã§ÏùÄ Ïù∏Í∞ÑÏùò Ïó∞ÏÇ∞ÏúºÎ°ú ÏôÑÏ†ÑÌûà ÌååÏïÖÌï† Ïàò ÏóÜÎäî Î≥µÏû°Í≥Ñ. Ï£ºÏñ¥ÏßÑ ÏÉÅÌô©Í≥º Îä•Î†•ÏúºÎ°ú Ìï† Ïàò ÏûàÎäî ÏµúÏÑ†Ïùò Ï†ÅÏùëÏùÄ Îã®ÏàúÌï®Í≥º Íæ∏Ï§ÄÌï®.\n\n  - ÌååÏÇ∞ÏùÑ Î©¥ÌïòÎäî ÏÑ†ÏóêÏÑú Ïó¨Îü¨Í∞ÄÏßÄÎ•º Ìï¥Î≥¥Í≥† ÏûêÏã†ÏóêÍ≤å ÎßûÎäî Í±∏ ÏúÑÏ£ºÎ°ú Íæ∏Ï§ÄÌûà. Í∑∏Î•º ÏúÑÌï¥ Îã®Ïàú, Ìé∏Ïïà, ÏæåÏ†ÅÌï®Ïù¥ ÌïÑÏöî.\n\n- ü•± -> ü§îüí°üå± - [On The Death of Daydreaming](https://www.afterbabel.com/p/on-the-death-of-daydreaming)\n  - boredom -> easy fun -> art -> profit?\n\n> I've often described my motivation for building software to others using imagery: I like to go find a secluded beach, build a large, magnificent sand castle, and then walk away. Will anyone notice? Probably not. Will the waves eventually destroy it? Yep. Did I still get immense satisfaction? Absolutely. - [aliasxneo](https://news.ycombinator.com/item?id=41497113)\n\n> We love to see the process, not just the result. The imperfections in your work can be beautiful if they show your struggle for perfection, not a lack of care. - [ralphammer](https://ralphammer.com/is-perfection-boring/)\n\n> Simplicity, even if it sacrifices some ideal functionality has better survival characteristics than the-right-thing. - [The Rise of Worse is Better](https://www.dreamsongs.com/RiseOfWorseIsBetter.html)\n\n> [Roberto Blake was talking about making 100 crappy videos](https://www.youtube.com/watch?v=OnUBaQ1Sp_E) to get better over time. Putting in the reps and improving a little bit each time.\n>\n> Putting in the work without expecting any external reward at first (eg views, followers, likes, etc) will pay off in the long run. - [100 Scrappy Things](https://www.florin-pop.com/blog/100-scrappy-things/)\n\n> Make the difficult habitual, the habitual easy, and the easy beautiful. - [Constantin S. Stanislavski](https://www.goodreads.com/quotes/7102271-make-the-difficult-habitual-the-habitual-easy-and-the-easy)\n\n> A good match is a **structured** dance, where players aim to **score** while they are following well-defined **rules**. This **freedom within a structure** is what makes it fun. - [ralphammer](https://ralphammer.com/how-to-get-started/)\n\n- [Pivot Points](https://longform.asmartbear.com/pivot-points/)\n\n  - non-judgmental aspects of personality that can be strengths in some contexts and weaknesses in others\n  - Pivot Points are fixed in the short term\n\n- [Hedged Bets](https://longform.asmartbear.com/predict-the-future/#hedged-bets)\n  - trading slightly less maximum upside for predictable, net-positive outcomes.\n\n> ‚ÄúMotivation often comes after starting, not before. Action produces momentum.‚Äù\n> [When you start a new habit, it should take less than two minutes to do.](https://jamesclear.com/how-to-stop-procrastinating)\n>\n> - James Clear\n\n> Focus is more about **not** keeping busy when you need to wait for something.  \n> Eat the boredom for a minute.\n>\n> - [[life-tips#wodenokoto]]\n\n> [4 minutes run hard enough to push heart rate to 90%, 3 minutes recover, repeat 4 times](https://news.ycombinator.com/item?id=34213181)\n>\n> - https://www.ntnu.edu/cerg/advice\n> - [Get running with Couch to 5K](https://www.nhs.uk/live-well/exercise/running-and-aerobic-exercises/get-running-with-couch-to-5k/)\n\n> [recommended routine - bodyweightfitness](https://www.reddit.com/r/bodyweightfitness/wiki/kb/recommended_routine/) - I Don't Have This Much Time!\n>\n> - Don't workout at all (saves anywhere from 20 to 60 minutes, but really, really, really, really, really, really, really, really, really not recommended)\n\n> ÎèÑÎ¨¥ÏßÄ ÏùΩÌûàÏßÄ ÏïäÎäî Ï±Ö ÏïûÏóêÏÑú ÎÇ¥Í∞Ä ÌÉùÌïú Î∞©Î≤ïÏùÄ ÌéºÏ≥êÏßÑ ÌéòÏù¥ÏßÄ ÏïûÏóêÏÑú Î©çÎïåÎ¶¨Í∏∞Ïù¥Îã§. Îã§Î•¥Í≤å ÌëúÌòÑÌïòÎ©¥ Ïù¥Î†áÎã§. ÌéºÏ≥êÏßÑ Îëê ÌéòÏù¥ÏßÄ ÏïûÏóêÏÑú Ïò§Îûò Î®∏Î¨ºÍ∏∞.\n>\n> Ï±ÖÏùÑ ÌéºÏ≥êÎÜìÎäî Í≤ÉÏúºÎ°ú Ï∂©Î∂ÑÌïòÎã§. ÏùΩÏßÄ Î™ªÌï¥ÎèÑ Ï¢ãÎã§. Îß§Ïùº Ï†ïÌï¥ÏßÑ ÏßÑÎèÑÎ•º ÎÇòÍ∞ÄÏïº ÌïòÎäî ÌïôÍµê ÏàòÏóÖÏù¥ ÏïÑÎãàÎãàÍπå. ÌïòÏßÄÎßå ÏùΩÏßÄ ÏïäÏïÑÎèÑ Í¥úÏ∞ÆÎã§Í≥† Ìï¥ÏÑú ÌéºÏ≥êÎëêÏßÄÏ°∞Ï∞® ÏïäÏúºÎ©¥ Í≥§ÎûÄÌïòÎã§. Í∞ÄÎä•Ìïú Ìïú ÏûêÏ£º Ï±ÖÏùÑ ÌéºÏ≥êÎëêÎèÑÎ°ù ÌïòÏûê. Ï†ÑÌòÄ ÏùΩÏßÄ ÏïäÍ≥† Î©çÌïòÎãà Î∞îÎùºÎ≥¥Í≥† ÏûàÎã§Í∞Ä Îã§Ïãú ÎçÆÍ≤å ÎêòÎçîÎùºÎèÑ\n>\n> - ÎßâÎßâÌïú ÎèÖÏÑú. ÏãúÎ°úÍµ∞. P.10~13\n\n> I think it should be everyone's primary focus to sleep well, drink water, get outside, get active, and eat generally decently. I hate to say it, but if you're not eating a good amount of vegetables and fruit, decent protein, sleep, etc, no amount of XYZ will catch up to that detriment. - [CE02](https://news.ycombinator.com/item?id=35056071)\n\n> My real battle is doing good versus doing nothing. - [Deirdre Sullivan](https://www.npr.org/2005/08/08/4785079/always-go-to-the-funeral)\n\n[Kind Engineering](https://kind.engineering/) - How To Engineer Kindness\n\n> Sometimes magic is just someone spending more time on something than anyone else might reasonably expect. - [Teller](https://www.goodreads.com/quotes/6641527-sometimes-magic-is-just-someone-spending-more-time-on-something)\n\n---\n\n## What I read in past\n\n- [[What I read in 2026|read.2026.articles]]\n  - [[2025|read.2025.articles]]\n  - [[2024|read.2024.articles]]\n  - [[2023|read.2023.articles]]\n  - [[2022|read.2022.articles]]\n- üìù [Gists](https://gist.github.com/Luke-SNAW)\n- üìú [Journals](https://luke-snaw.github.io/Luke-SNAW__netlify-CMS.github.io/)\n\n---\n\n- [[journal.what-i-struggled-brag-in]]\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":false,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"dendronVersion":"0.115.0","enableFullHierarchyNoteTitle":false,"enablePersistentHistory":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Luke SNAW","description":"Personal knowledge space"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","siteUrl":"https://luke-snaw.github.io/","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true}
<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>How Pinterest scaled to 11 million users with only 6 engineers</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="How Pinterest scaled to 11 million users with only 6 engineers"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://luke-snaw.github.io//notes/bpw92y9ez0959qtnaedllfs/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="12/27/2023"/><meta property="article:modified_time" content="12/27/2023"/><link rel="canonical" href="https://luke-snaw.github.io//notes/bpw92y9ez0959qtnaedllfs/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9d8e0603730b15a3.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/vOE8u-mg___OsOsz4tjEg/_buildManifest.js" defer=""></script><script src="/_next/static/vOE8u-mg___OsOsz4tjEg/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="how-pinterest-scaled-to-11-million-users-with-only-6-engineers">How Pinterest scaled to 11 million users with only 6 engineers<a aria-hidden="true" class="anchor-heading icon-link" href="#how-pinterest-scaled-to-11-million-users-with-only-6-engineers"></a></h1>
<blockquote>
<p><a href="https://read.engineerscodex.com/p/how-pinterest-scaled-to-11-million">https://read.engineerscodex.com/p/how-pinterest-scaled-to-11-million</a></p>
</blockquote>
<p>In January 2012, Pinterest hit 11.7 million monthly unique users with only 6 engineers.</p>
<p>Having launched in March 2010, it was <a href="https://techcrunch.com/2012/02/07/pinterest-monthly-uniques/#:~:text=11.7%20million%20unique%20monthly%20U.S.%20visitors%2C%20crossing%20the%2010%20million%20mark%20faster%20than%20any%20other%20standalone%20site%20in%20history.">the fastest company to race past 10 million monthly users at the time</a>.</p>
<p><a href="https://pinterest.com/">Pinterest</a> is an image-heavy social network, where users can save or “pin” images to their boards.</p>
<blockquote>
<p>When I say “users” below, I mean “monthly active users” (MAUs).</p>
</blockquote>
<h2 id="lessons-from-scaling-pinterest"><strong>Lessons from Scaling Pinterest</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#lessons-from-scaling-pinterest"></a></h2>
<ul>
<li><strong>Use known, proven technologies.</strong> Pinterest’s dive into newer technologies at the time led to issues like data corruption.</li>
<li><strong>Keep it simple.</strong> (A recurring theme!)</li>
<li><strong>Don’t get too creative.</strong> The team settled on an architecture where they could add more of the same nodes to scale.</li>
<li><strong>Limit your options</strong>.</li>
<li><strong>Sharding databases > clustering.</strong> It reduced data transfer across nodes, which was a good thing.</li>
<li><strong>Have fun!</strong> New engineers would contribute code in their first week.</li>
</ul>
<p><a href="https://engineercodex.substack.com/p/how-instagram-scaled-to-14-million">The Instagram team had similar lessons from scaling to 14 million users with 3 engineers</a>.</p>
<h2 id="march-2010-closed-beta-launch-1-engineer"><strong>March 2010: Closed beta launch, 1 engineer</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#march-2010-closed-beta-launch-1-engineer"></a></h2>
<p>Pinterest launched in March 2010 with 1 small MySQL database, 1 small web server, and 1 engineer (along with the 2 co-founders).</p>
<p><a href="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d5d064-82c0-4d31-916d-70bfeb1527a9_1124x448.png">https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d5d064-82c0-4d31-916d-70bfeb1527a9_1124x448.png</a></p>
<h2 id="january-2011-10000-users-2-engineers"><strong>January 2011: 10,000 users, 2 engineers</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#january-2011-10000-users-2-engineers"></a></h2>
<p>Nine months later in January 2011, Pinterest’s architecture had evolved to handle more users. They were still invite-only and had 2 engineers.</p>
<p>They had:</p>
<ul>
<li>a basic web server stack (Amazon EC2, S3, and CloudFront)
<ul>
<li>Django (Python) for their backend</li>
</ul>
</li>
<li>4 web servers for redundancy</li>
<li>NGINX as their reverse proxy and load balancer.</li>
<li>1 MySQL database at this point + 1 read-only secondary</li>
<li>MongoDB for counters</li>
<li>1 task queue and 2 task processors for asynchronous tasks</li>
</ul>
<p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd7c12c-bdc0-494d-ac86-b65f09b2a2f6_1255x615.png">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd7c12c-bdc0-494d-ac86-b65f09b2a2f6_1255x615.png</a></p>
<h2 id="october-2011-32-million-users-3-engineers"><strong>October 2011: 3.2 million users, 3 engineers</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#october-2011-32-million-users-3-engineers"></a></h2>
<p>From January 2011 to October 2011, Pinterest grew extremely fast, doubling users every month and a half.</p>
<p>Their iPhone app launch in March 2011 was one of the factors fueling this growth.</p>
<p>When things grow fast, technology breaks more often than you expect.</p>
<p>Pinterest made a mistake: <strong>they over-complicated their architecture immensely.</strong></p>
<p>They had only 3 engineers, but 5 different database technologies for their data.</p>
<p>They were both manually sharding their MySQL databases and clustering their data using Cassandra and Membase (now Couchbase).</p>
<p><strong><a href="https://www.infoq.com/presentations/Pinterest/">Their “overcomplicated stack"</a>:</strong></p>
<ul>
<li>Web server stack (EC2 + S3 + CloudFront)
<ul>
<li><a href="https://www.quora.com/What-challenges-has-Pinterest-encountered-with-Flask">Pinterest started moving to Flask (Python) for their backend</a></li>
</ul>
</li>
<li>16 web servers</li>
<li>2 API engines</li>
<li>2 NGINX proxies</li>
<li>5 manually-sharded MySQL DBs + 9 read-only secondaries</li>
<li>4 Cassandra Nodes</li>
<li>15 Membase Nodes (3 separate clusters)</li>
<li>8 Memcache Nodes</li>
<li>10 Redis Nodes</li>
<li>3 Task Routers + 4 Task Processors</li>
<li>4 Elastic Search Nodes</li>
<li>3 Mongo Clusters</li>
</ul>
<p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F386fd87b-f932-4310-82cf-07860bc36e98_1357x1047.png">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F386fd87b-f932-4310-82cf-07860bc36e98_1357x1047.png</a></p>
<p>Subscribe</p>
<h3 id="️-clustering-gone-wrong">⚠️ Clustering gone wrong<a aria-hidden="true" class="anchor-heading icon-link" href="#️-clustering-gone-wrong"></a></h3>
<blockquote>
<p><strong>Database clustering</strong> is the process of connecting multiple database servers to work together as a single system.</p>
</blockquote>
<p>In theory, clustering automatically scales datastores, provides high availability, free load balancing, and doesn’t have a single point of failure.</p>
<p>Unfortunately, in practice, clustering was overly complex, had difficult upgrade mechanisms, and <strong>it had a big single point of failure.</strong></p>
<p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2198073-6421-434e-be2f-2904aa5ff975_1462x645.png">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2198073-6421-434e-be2f-2904aa5ff975_1462x645.png</a></p>
<p>Each DB has a Cluster Management Algorithm that routes from DB to DB.</p>
<p>When something goes wrong with a DB, a new DB is added to replace it.</p>
<p>In theory, the Cluster Management Algorithm should handle this just fine.</p>
<p>In reality, there was a bug in Pinterest’s Cluster Management Algorithm that <strong>corrupted data on all their nodes, broke their data rebalancing, and created some unfixable problems</strong>.</p>
<p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffabc91cd-53b2-47f2-9d71-c50e7a3824aa_1306x538.png">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffabc91cd-53b2-47f2-9d71-c50e7a3824aa_1306x538.png</a></p>
<p>Pinterest’s solution? <strong>Remove all clustering tech (Cassandra, Membase) from the system. Go all-in with MySQL + Memcached (more proven).</strong></p>
<p>MySQL and Memcached are well-proven technologies. <a href="https://engineercodex.substack.com/p/how-facebook-scaled-memcached">Facebook used the two to create the largest Memcached system in the world, which handled billions of requests per second for them with ease.</a></p>
<p>Subscribe</p>
<h2 id="january-2012-11-million-users-6-engineers"><strong>January 2012: 11 million users, 6 engineers</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#january-2012-11-million-users-6-engineers"></a></h2>
<p>In January 2012, Pinterest was handling ~11 million monthly active users, with anywhere between 12 million to 21 million daily users.</p>
<p>At this point, Pinterest had taken the time to simplify their architecture.</p>
<p>They removed less-proven ideas, like clustering and Cassandra at the time, and replaced them with proven ones, like MySQL, Memcache, and sharding.</p>
<p><strong>Their simplified stack:</strong></p>
<ul>
<li>Amazon EC2 + S3 + <a href="https://www.akamai.com/">Akamai</a> (replaced CloudFront)</li>
<li><a href="https://aws.amazon.com/elasticloadbalancing/">AWS ELB (Elastic Load Balancing)</a></li>
<li>90 Web Engines + 50 API Engines (<a href="https://www.quora.com/What-challenges-has-Pinterest-encountered-with-Flask">using Flask</a>)</li>
<li>66 MySQL DBs + 66 secondaries</li>
<li>59 Redis Instances</li>
<li>51 Memcache Instances</li>
<li>1 Redis Task Manager + 25 Task Processors</li>
<li>Sharded <a href="https://solr.apache.org/">Apache Solr</a> (replaced Elasticsearch)</li>
<li><strong>Removed Cassanda, Membase, Elasticsearch, MongoDB, NGINX</strong></li>
</ul>
<p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34e259af-7bfe-4734-ab56-f793dabe2cb2_1608x767.png">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34e259af-7bfe-4734-ab56-f793dabe2cb2_1608x767.png</a></p>
<h3 id="how-pinterest-manually-sharded-their-databases">How Pinterest manually sharded their databases<a aria-hidden="true" class="anchor-heading icon-link" href="#how-pinterest-manually-sharded-their-databases"></a></h3>
<blockquote>
<p><strong>Database sharding</strong> is a method of splitting a single dataset into multiple databases.</p>
<p><strong>Benefits:</strong> high availability, load balancing, simple algorithm for placing data, easy to split databases to add more capacity, easy to locate data</p>
</blockquote>
<p>When Pinterest first sharded their databases, they had a feature freeze. Over the span of a few months, <strong>they sharded their databases incrementally and manually:</strong></p>
<p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d7cfda-8807-4adf-a927-7e7f3e9faaf5_789x443.png">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d7cfda-8807-4adf-a927-7e7f3e9faaf5_789x443.png</a></p>
<p><a href="https://www.infoq.com/presentations/Pinterest/">Source: Scaling Pinterest</a></p>
<p>The team removed table joins and complex queries from the database layer. They added lots of caching.</p>
<p>Since it was extra effort to maintain unique constraints across databases, they kept data like usernames and emails in a huge, unsharded database.</p>
<p>All their tables existed on all their shards.</p>
<h4 id="an-small-example-of-manual-sharding">An small example of manual sharding<a aria-hidden="true" class="anchor-heading icon-link" href="#an-small-example-of-manual-sharding"></a></h4>
<p>Since they had billions of “pins”, their database indexes ran out of memory.</p>
<p>They would take the largest table on the database and move it to its own database.</p>
<p>Then, when that database ran out of space, they would shard.</p>
<p>Subscribe</p>
<h2 id="october-2012-22-million-users-40-engineers"><strong>October 2012: 22 million users, 40 engineers</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#october-2012-22-million-users-40-engineers"></a></h2>
<p>In October 2012, Pinterest had around 22 million monthly users, but their engineering team had quadrupled to 40 engineers.</p>
<p><strong>The architecture was the same. They just added more of the same systems.</strong></p>
<ul>
<li>Amazon EC2 + S3 + CDNs (EdgeCast, Akamai, Level 3)</li>
<li>180 web servers + 240 API engines (using Flask)</li>
<li>88 MySQL DBs + 88 secondaries each</li>
<li>110 Redis instances</li>
<li>200 Memcache instances</li>
<li>4 Redis Task Managers + 80 Task Processors</li>
<li>Sharded Apache Solr</li>
</ul>
<p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff18b5dd-2d71-4d8b-864a-4455e374bc62_1608x767.png">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff18b5dd-2d71-4d8b-864a-4455e374bc62_1608x767.png</a></p>
<p>They started moving from hard disk drives to SSDs.</p>
<p>An important lesson learned: <strong>limited, proven choices was a good thing</strong>.</p>
<p>Sticking with EC2 and S3 meant they had limited configuration choices, leading to less headaches and more simplicity.</p>
<p><strong>However, new instances could be ready in seconds.</strong> This meant that they could add 10 Memcache instances in a matter of minutes.</p>
<p>Subscribe</p>
<h2 id="pinterests-database-structuring"><strong>Pinterest’s Database Structuring</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#pinterests-database-structuring"></a></h2>
<h3 id="ids">IDs<a aria-hidden="true" class="anchor-heading icon-link" href="#ids"></a></h3>
<p><a href="https://engineercodex.substack.com/p/how-instagram-scaled-to-14-million">Like Instagram</a>, Pinterest had a unique ID structure because they had sharded databases.</p>
<p>Their 64-bit ID looked like:</p>
<p><a href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F269be284-a074-4898-857b-2a8903ba4b48_590x110.png">https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F269be284-a074-4898-857b-2a8903ba4b48_590x110.png</a></p>
<p><a href="https://www.infoq.com/presentations/Pinterest/">Source: Scaling Pinterest</a></p>
<blockquote>
<p><strong>Shard ID:</strong> which shard (16 bits)</p>
<p><strong>Type:</strong> object type, such as pins (10 bits)</p>
<p><strong>Local ID:</strong> position in table (38 bits)</p>
</blockquote>
<p>The lookup structure for these IDs was <strong>a simple Python dictionary.</strong></p>
<hr>
<h3 id="tables">Tables<a aria-hidden="true" class="anchor-heading icon-link" href="#tables"></a></h3>
<p>They had Object tables and Mapping tables.</p>
<p><strong>Object tables were for pins, boards, comments, users, and more.</strong> They had a Local ID mapped to a MySQL blob, like JSON.</p>
<p><strong>Mapping tables were for relational data between objects, like mapping boards to a user or likes to a pin.</strong> They had a Full ID mapped to a Full ID and a timestamp.</p>
<p>All queries were PK (primary key) or index lookups for efficiency. They cut out all JOINs.</p>
<hr>
<p><strong>This article is based on <a href="https://www.infoq.com/presentations/Pinterest/">Scaling Pinterest</a>, a talk given by the Pinterest team in 2012.</strong></p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#lessons-from-scaling-pinterest" title="Lessons from Scaling Pinterest">Lessons from Scaling Pinterest</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#march-2010-closed-beta-launch-1-engineer" title="March 2010: Closed beta launch, 1 engineer">March 2010: Closed beta launch, 1 engineer</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#january-2011-10000-users-2-engineers" title="January 2011: 10,000 users, 2 engineers">January 2011: 10,000 users, 2 engineers</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#october-2011-32-million-users-3-engineers" title="October 2011: 3.2 million users, 3 engineers">October 2011: 3.2 million users, 3 engineers</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#️-clustering-gone-wrong" title="⚠️ Clustering gone wrong">⚠️ Clustering gone wrong</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#january-2012-11-million-users-6-engineers" title="January 2012: 11 million users, 6 engineers">January 2012: 11 million users, 6 engineers</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#how-pinterest-manually-sharded-their-databases" title="How Pinterest manually sharded their databases">How Pinterest manually sharded their databases</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#an-small-example-of-manual-sharding" title="An small example of manual sharding">An small example of manual sharding</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#october-2012-22-million-users-40-engineers" title="October 2012: 22 million users, 40 engineers">October 2012: 22 million users, 40 engineers</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#pinterests-database-structuring" title="Pinterest’s Database Structuring">Pinterest’s Database Structuring</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#ids" title="IDs">IDs</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#tables" title="Tables">Tables</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"bpw92y9ez0959qtnaedllfs","title":"How Pinterest scaled to 11 million users with only 6 engineers","desc":"","updated":1703654866126,"created":1703654655270,"custom":{},"fname":"dev.software-engineering.how-pinterest-scaled-to-11-million","type":"note","vault":{"fsPath":"vault"},"contentHash":"0b6fe1749a1c03be4c02b0769fb68307","links":[],"anchors":{"lessons-from-scaling-pinterest":{"type":"header","text":"Lessons from Scaling Pinterest","value":"lessons-from-scaling-pinterest","line":18,"column":0,"depth":2},"march-2010-closed-beta-launch-1-engineer":{"type":"header","text":"March 2010: Closed beta launch, 1 engineer","value":"march-2010-closed-beta-launch-1-engineer","line":29,"column":0,"depth":2},"january-2011-10000-users-2-engineers":{"type":"header","text":"January 2011: 10,000 users, 2 engineers","value":"january-2011-10000-users-2-engineers","line":35,"column":0,"depth":2},"october-2011-32-million-users-3-engineers":{"type":"header","text":"October 2011: 3.2 million users, 3 engineers","value":"october-2011-32-million-users-3-engineers","line":51,"column":0,"depth":2},"️-clustering-gone-wrong":{"type":"header","text":"⚠️ Clustering gone wrong","value":"️-clustering-gone-wrong","line":85,"column":0,"depth":3},"january-2012-11-million-users-6-engineers":{"type":"header","text":"January 2012: 11 million users, 6 engineers","value":"january-2012-11-million-users-6-engineers","line":111,"column":0,"depth":2},"how-pinterest-manually-sharded-their-databases":{"type":"header","text":"How Pinterest manually sharded their databases","value":"how-pinterest-manually-sharded-their-databases","line":133,"column":0,"depth":3},"an-small-example-of-manual-sharding":{"type":"header","text":"An small example of manual sharding","value":"an-small-example-of-manual-sharding","line":151,"column":0,"depth":4},"october-2012-22-million-users-40-engineers":{"type":"header","text":"October 2012: 22 million users, 40 engineers","value":"october-2012-22-million-users-40-engineers","line":161,"column":0,"depth":2},"pinterests-database-structuring":{"type":"header","text":"Pinterest’s Database Structuring","value":"pinterests-database-structuring","line":187,"column":0,"depth":2},"ids":{"type":"header","text":"IDs","value":"ids","line":189,"column":0,"depth":3},"tables":{"type":"header","text":"Tables","value":"tables","line":209,"column":0,"depth":3}},"children":[],"parent":"j0N1aVKxe96dktmyADG9U","data":{}},"body":"\u003ch1 id=\"how-pinterest-scaled-to-11-million-users-with-only-6-engineers\"\u003eHow Pinterest scaled to 11 million users with only 6 engineers\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#how-pinterest-scaled-to-11-million-users-with-only-6-engineers\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://read.engineerscodex.com/p/how-pinterest-scaled-to-11-million\"\u003ehttps://read.engineerscodex.com/p/how-pinterest-scaled-to-11-million\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn January 2012, Pinterest hit 11.7 million monthly unique users with only 6 engineers.\u003c/p\u003e\n\u003cp\u003eHaving launched in March 2010, it was \u003ca href=\"https://techcrunch.com/2012/02/07/pinterest-monthly-uniques/#:~:text=11.7%20million%20unique%20monthly%20U.S.%20visitors%2C%20crossing%20the%2010%20million%20mark%20faster%20than%20any%20other%20standalone%20site%20in%20history.\"\u003ethe fastest company to race past 10 million monthly users at the time\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://pinterest.com/\"\u003ePinterest\u003c/a\u003e is an image-heavy social network, where users can save or “pin” images to their boards.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWhen I say “users” below, I mean “monthly active users” (MAUs).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"lessons-from-scaling-pinterest\"\u003e\u003cstrong\u003eLessons from Scaling Pinterest\u003c/strong\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#lessons-from-scaling-pinterest\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eUse known, proven technologies.\u003c/strong\u003e Pinterest’s dive into newer technologies at the time led to issues like data corruption.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKeep it simple.\u003c/strong\u003e (A recurring theme!)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDon’t get too creative.\u003c/strong\u003e The team settled on an architecture where they could add more of the same nodes to scale.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLimit your options\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSharding databases \u003e clustering.\u003c/strong\u003e It reduced data transfer across nodes, which was a good thing.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHave fun!\u003c/strong\u003e New engineers would contribute code in their first week.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://engineercodex.substack.com/p/how-instagram-scaled-to-14-million\"\u003eThe Instagram team had similar lessons from scaling to 14 million users with 3 engineers\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"march-2010-closed-beta-launch-1-engineer\"\u003e\u003cstrong\u003eMarch 2010: Closed beta launch, 1 engineer\u003c/strong\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#march-2010-closed-beta-launch-1-engineer\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003ePinterest launched in March 2010 with 1 small MySQL database, 1 small web server, and 1 engineer (along with the 2 co-founders).\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d5d064-82c0-4d31-916d-70bfeb1527a9_1124x448.png\"\u003ehttps://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19d5d064-82c0-4d31-916d-70bfeb1527a9_1124x448.png\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"january-2011-10000-users-2-engineers\"\u003e\u003cstrong\u003eJanuary 2011: 10,000 users, 2 engineers\u003c/strong\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#january-2011-10000-users-2-engineers\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eNine months later in January 2011, Pinterest’s architecture had evolved to handle more users. They were still invite-only and had 2 engineers.\u003c/p\u003e\n\u003cp\u003eThey had:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea basic web server stack (Amazon EC2, S3, and CloudFront)\n\u003cul\u003e\n\u003cli\u003eDjango (Python) for their backend\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e4 web servers for redundancy\u003c/li\u003e\n\u003cli\u003eNGINX as their reverse proxy and load balancer.\u003c/li\u003e\n\u003cli\u003e1 MySQL database at this point + 1 read-only secondary\u003c/li\u003e\n\u003cli\u003eMongoDB for counters\u003c/li\u003e\n\u003cli\u003e1 task queue and 2 task processors for asynchronous tasks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd7c12c-bdc0-494d-ac86-b65f09b2a2f6_1255x615.png\"\u003ehttps://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cd7c12c-bdc0-494d-ac86-b65f09b2a2f6_1255x615.png\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"october-2011-32-million-users-3-engineers\"\u003e\u003cstrong\u003eOctober 2011: 3.2 million users, 3 engineers\u003c/strong\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#october-2011-32-million-users-3-engineers\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eFrom January 2011 to October 2011, Pinterest grew extremely fast, doubling users every month and a half.\u003c/p\u003e\n\u003cp\u003eTheir iPhone app launch in March 2011 was one of the factors fueling this growth.\u003c/p\u003e\n\u003cp\u003eWhen things grow fast, technology breaks more often than you expect.\u003c/p\u003e\n\u003cp\u003ePinterest made a mistake: \u003cstrong\u003ethey over-complicated their architecture immensely.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThey had only 3 engineers, but 5 different database technologies for their data.\u003c/p\u003e\n\u003cp\u003eThey were both manually sharding their MySQL databases and clustering their data using Cassandra and Membase (now Couchbase).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.infoq.com/presentations/Pinterest/\"\u003eTheir “overcomplicated stack\"\u003c/a\u003e:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWeb server stack (EC2 + S3 + CloudFront)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.quora.com/What-challenges-has-Pinterest-encountered-with-Flask\"\u003ePinterest started moving to Flask (Python) for their backend\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e16 web servers\u003c/li\u003e\n\u003cli\u003e2 API engines\u003c/li\u003e\n\u003cli\u003e2 NGINX proxies\u003c/li\u003e\n\u003cli\u003e5 manually-sharded MySQL DBs + 9 read-only secondaries\u003c/li\u003e\n\u003cli\u003e4 Cassandra Nodes\u003c/li\u003e\n\u003cli\u003e15 Membase Nodes (3 separate clusters)\u003c/li\u003e\n\u003cli\u003e8 Memcache Nodes\u003c/li\u003e\n\u003cli\u003e10 Redis Nodes\u003c/li\u003e\n\u003cli\u003e3 Task Routers + 4 Task Processors\u003c/li\u003e\n\u003cli\u003e4 Elastic Search Nodes\u003c/li\u003e\n\u003cli\u003e3 Mongo Clusters\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F386fd87b-f932-4310-82cf-07860bc36e98_1357x1047.png\"\u003ehttps://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F386fd87b-f932-4310-82cf-07860bc36e98_1357x1047.png\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSubscribe\u003c/p\u003e\n\u003ch3 id=\"️-clustering-gone-wrong\"\u003e⚠️ Clustering gone wrong\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#️-clustering-gone-wrong\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDatabase clustering\u003c/strong\u003e is the process of connecting multiple database servers to work together as a single system.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn theory, clustering automatically scales datastores, provides high availability, free load balancing, and doesn’t have a single point of failure.\u003c/p\u003e\n\u003cp\u003eUnfortunately, in practice, clustering was overly complex, had difficult upgrade mechanisms, and \u003cstrong\u003eit had a big single point of failure.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2198073-6421-434e-be2f-2904aa5ff975_1462x645.png\"\u003ehttps://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2198073-6421-434e-be2f-2904aa5ff975_1462x645.png\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eEach DB has a Cluster Management Algorithm that routes from DB to DB.\u003c/p\u003e\n\u003cp\u003eWhen something goes wrong with a DB, a new DB is added to replace it.\u003c/p\u003e\n\u003cp\u003eIn theory, the Cluster Management Algorithm should handle this just fine.\u003c/p\u003e\n\u003cp\u003eIn reality, there was a bug in Pinterest’s Cluster Management Algorithm that \u003cstrong\u003ecorrupted data on all their nodes, broke their data rebalancing, and created some unfixable problems\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffabc91cd-53b2-47f2-9d71-c50e7a3824aa_1306x538.png\"\u003ehttps://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffabc91cd-53b2-47f2-9d71-c50e7a3824aa_1306x538.png\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePinterest’s solution? \u003cstrong\u003eRemove all clustering tech (Cassandra, Membase) from the system. Go all-in with MySQL + Memcached (more proven).\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eMySQL and Memcached are well-proven technologies. \u003ca href=\"https://engineercodex.substack.com/p/how-facebook-scaled-memcached\"\u003eFacebook used the two to create the largest Memcached system in the world, which handled billions of requests per second for them with ease.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSubscribe\u003c/p\u003e\n\u003ch2 id=\"january-2012-11-million-users-6-engineers\"\u003e\u003cstrong\u003eJanuary 2012: 11 million users, 6 engineers\u003c/strong\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#january-2012-11-million-users-6-engineers\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIn January 2012, Pinterest was handling ~11 million monthly active users, with anywhere between 12 million to 21 million daily users.\u003c/p\u003e\n\u003cp\u003eAt this point, Pinterest had taken the time to simplify their architecture.\u003c/p\u003e\n\u003cp\u003eThey removed less-proven ideas, like clustering and Cassandra at the time, and replaced them with proven ones, like MySQL, Memcache, and sharding.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTheir simplified stack:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAmazon EC2 + S3 + \u003ca href=\"https://www.akamai.com/\"\u003eAkamai\u003c/a\u003e (replaced CloudFront)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://aws.amazon.com/elasticloadbalancing/\"\u003eAWS ELB (Elastic Load Balancing)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e90 Web Engines + 50 API Engines (\u003ca href=\"https://www.quora.com/What-challenges-has-Pinterest-encountered-with-Flask\"\u003eusing Flask\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e66 MySQL DBs + 66 secondaries\u003c/li\u003e\n\u003cli\u003e59 Redis Instances\u003c/li\u003e\n\u003cli\u003e51 Memcache Instances\u003c/li\u003e\n\u003cli\u003e1 Redis Task Manager + 25 Task Processors\u003c/li\u003e\n\u003cli\u003eSharded \u003ca href=\"https://solr.apache.org/\"\u003eApache Solr\u003c/a\u003e (replaced Elasticsearch)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRemoved Cassanda, Membase, Elasticsearch, MongoDB, NGINX\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34e259af-7bfe-4734-ab56-f793dabe2cb2_1608x767.png\"\u003ehttps://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F34e259af-7bfe-4734-ab56-f793dabe2cb2_1608x767.png\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"how-pinterest-manually-sharded-their-databases\"\u003eHow Pinterest manually sharded their databases\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#how-pinterest-manually-sharded-their-databases\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDatabase sharding\u003c/strong\u003e is a method of splitting a single dataset into multiple databases.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBenefits:\u003c/strong\u003e high availability, load balancing, simple algorithm for placing data, easy to split databases to add more capacity, easy to locate data\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhen Pinterest first sharded their databases, they had a feature freeze. Over the span of a few months, \u003cstrong\u003ethey sharded their databases incrementally and manually:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d7cfda-8807-4adf-a927-7e7f3e9faaf5_789x443.png\"\u003ehttps://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F33d7cfda-8807-4adf-a927-7e7f3e9faaf5_789x443.png\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.infoq.com/presentations/Pinterest/\"\u003eSource: Scaling Pinterest\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe team removed table joins and complex queries from the database layer. They added lots of caching.\u003c/p\u003e\n\u003cp\u003eSince it was extra effort to maintain unique constraints across databases, they kept data like usernames and emails in a huge, unsharded database.\u003c/p\u003e\n\u003cp\u003eAll their tables existed on all their shards.\u003c/p\u003e\n\u003ch4 id=\"an-small-example-of-manual-sharding\"\u003eAn small example of manual sharding\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#an-small-example-of-manual-sharding\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eSince they had billions of “pins”, their database indexes ran out of memory.\u003c/p\u003e\n\u003cp\u003eThey would take the largest table on the database and move it to its own database.\u003c/p\u003e\n\u003cp\u003eThen, when that database ran out of space, they would shard.\u003c/p\u003e\n\u003cp\u003eSubscribe\u003c/p\u003e\n\u003ch2 id=\"october-2012-22-million-users-40-engineers\"\u003e\u003cstrong\u003eOctober 2012: 22 million users, 40 engineers\u003c/strong\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#october-2012-22-million-users-40-engineers\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIn October 2012, Pinterest had around 22 million monthly users, but their engineering team had quadrupled to 40 engineers.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThe architecture was the same. They just added more of the same systems.\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAmazon EC2 + S3 + CDNs (EdgeCast, Akamai, Level 3)\u003c/li\u003e\n\u003cli\u003e180 web servers + 240 API engines (using Flask)\u003c/li\u003e\n\u003cli\u003e88 MySQL DBs + 88 secondaries each\u003c/li\u003e\n\u003cli\u003e110 Redis instances\u003c/li\u003e\n\u003cli\u003e200 Memcache instances\u003c/li\u003e\n\u003cli\u003e4 Redis Task Managers + 80 Task Processors\u003c/li\u003e\n\u003cli\u003eSharded Apache Solr\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff18b5dd-2d71-4d8b-864a-4455e374bc62_1608x767.png\"\u003ehttps://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff18b5dd-2d71-4d8b-864a-4455e374bc62_1608x767.png\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThey started moving from hard disk drives to SSDs.\u003c/p\u003e\n\u003cp\u003eAn important lesson learned: \u003cstrong\u003elimited, proven choices was a good thing\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eSticking with EC2 and S3 meant they had limited configuration choices, leading to less headaches and more simplicity.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHowever, new instances could be ready in seconds.\u003c/strong\u003e This meant that they could add 10 Memcache instances in a matter of minutes.\u003c/p\u003e\n\u003cp\u003eSubscribe\u003c/p\u003e\n\u003ch2 id=\"pinterests-database-structuring\"\u003e\u003cstrong\u003ePinterest’s Database Structuring\u003c/strong\u003e\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#pinterests-database-structuring\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003ch3 id=\"ids\"\u003eIDs\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#ids\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://engineercodex.substack.com/p/how-instagram-scaled-to-14-million\"\u003eLike Instagram\u003c/a\u003e, Pinterest had a unique ID structure because they had sharded databases.\u003c/p\u003e\n\u003cp\u003eTheir 64-bit ID looked like:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F269be284-a074-4898-857b-2a8903ba4b48_590x110.png\"\u003ehttps://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F269be284-a074-4898-857b-2a8903ba4b48_590x110.png\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.infoq.com/presentations/Pinterest/\"\u003eSource: Scaling Pinterest\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eShard ID:\u003c/strong\u003e which shard (16 bits)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eType:\u003c/strong\u003e object type, such as pins (10 bits)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLocal ID:\u003c/strong\u003e position in table (38 bits)\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe lookup structure for these IDs was \u003cstrong\u003ea simple Python dictionary.\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"tables\"\u003eTables\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tables\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThey had Object tables and Mapping tables.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eObject tables were for pins, boards, comments, users, and more.\u003c/strong\u003e They had a Local ID mapped to a MySQL blob, like JSON.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMapping tables were for relational data between objects, like mapping boards to a user or likes to a pin.\u003c/strong\u003e They had a Full ID mapped to a Full ID and a timestamp.\u003c/p\u003e\n\u003cp\u003eAll queries were PK (primary key) or index lookups for efficiency. They cut out all JOINs.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eThis article is based on \u003ca href=\"https://www.infoq.com/presentations/Pinterest/\"\u003eScaling Pinterest\u003c/a\u003e, a talk given by the Pinterest team in 2012.\u003c/strong\u003e\u003c/p\u003e","noteIndex":{"id":"Iy0MoL0KnL55Br3AfTS2C","title":"Luke","desc":"","updated":1761796791487,"created":1644449449778,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"4e745570ca97988a0362cb939b760952","links":[{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"life-tips","position":{"start":{"line":41,"column":5,"offset":2603},"end":{"line":41,"column":29,"offset":2627},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"life-tips","anchorHeader":"wodenokoto"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-read-in.2025","alias":"What I read in 2025","position":{"start":{"line":70,"column":3,"offset":4333},"end":{"line":70,"column":54,"offset":4384},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-read-in.2025"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-read-in.2024","alias":"2024","position":{"start":{"line":71,"column":5,"offset":4389},"end":{"line":71,"column":41,"offset":4425},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-read-in.2024"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-read-in.2023","alias":"2023","position":{"start":{"line":72,"column":5,"offset":4430},"end":{"line":72,"column":41,"offset":4466},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-read-in.2023"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-read-in.2022","alias":"2022","position":{"start":{"line":73,"column":5,"offset":4471},"end":{"line":73,"column":41,"offset":4507},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-read-in.2022"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-struggled-brag-in","position":{"start":{"line":79,"column":3,"offset":4643},"end":{"line":79,"column":39,"offset":4679},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-struggled-brag-in"}}],"anchors":{"what-i-read-in-past":{"type":"header","text":"What I read in past","value":"what-i-read-in-past","line":74,"column":0,"depth":2}},"children":["zd4mq442jike0pr0wba1u3m","6hzeqsofq67gdk88flxlkhp","778ijii93yu5uwnrwmn5zi4","g1fngdjl25nes6fs3lip602","ZbdkdApFqLdks4Moq92R9","uoc5hhki3o4py15cesddu8q","9qf7j06jtdkm6rnx9ymvwb0","5zn10cvj7ajy2gh2is5nqmg","4qo9ma0z0yu1czns6pxl7y5","ok0e729ho7o09xetujkxc0m","GR5x8HnNFEN6fU2UBSEIK","yirtnlj8q24yutcf3ss1xqy","eq0wc6t7wl2wv221yb68ro4","7x2fnv4j6gxts08qk0jguny","ettkt3iClONnxpbGwBVLl","7l4knev6v613tbuoskvmbdg","hvh5bud6yp7dc89tuh95tr9","4fvoqrplw0cweo554usbjos","f8qsfql0a9v8thpeo82udfa","1swsbrhqi9jk41v9eodyi5q","SQqYupi6EFddTerBA8RRD","hjNeNc1F2JUh0lTWanH4h","qf0l4wbrc9jgooyzexmbq5v","uur1lkol353z9vfeqb3n5bv","cd9n1czq3ursgkby985wkmm","k1sr43vwnfqztwc0s43pkcf","wfde75rhdvq2yfa2zy2q6rv","rjcmdv60jokmbw6zoq8u2ef","ujapvww8o6v3kpmlhtryq4k","pkwewou9d5e8ystswn1j2b4"],"parent":null,"data":{},"body":"\nHi there 👋. I'm a Front-end developer.\n\n---\n\n- 단순함과 꾸준함은 가장 쉬우면서도 지키기 어려운 원칙.\n\n- 🥱 -\u003e 🤔💡🌱 - [On The Death of Daydreaming](https://www.afterbabel.com/p/on-the-death-of-daydreaming)\n  - boredom -\u003e easy fun -\u003e art -\u003e profit?\n\n\u003e I've often described my motivation for building software to others using imagery: I like to go find a secluded beach, build a large, magnificent sand castle, and then walk away. Will anyone notice? Probably not. Will the waves eventually destroy it? Yep. Did I still get immense satisfaction? Absolutely. - [aliasxneo](https://news.ycombinator.com/item?id=41497113)\n\n\u003e We love to see the process, not just the result. The imperfections in your work can be beautiful if they show your struggle for perfection, not a lack of care. - [ralphammer](https://ralphammer.com/is-perfection-boring/)\n\n\u003e Simplicity, even if it sacrifices some ideal functionality has better survival characteristics than the-right-thing. - [The Rise of Worse is Better](https://www.dreamsongs.com/RiseOfWorseIsBetter.html)\n\n\u003e [Roberto Blake was talking about making 100 crappy videos](https://www.youtube.com/watch?v=OnUBaQ1Sp_E) to get better over time. Putting in the reps and improving a little bit each time.\n\u003e\n\u003e Putting in the work without expecting any external reward at first (eg views, followers, likes, etc) will pay off in the long run. - [100 Scrappy Things](https://www.florin-pop.com/blog/100-scrappy-things/)\n\n\u003e Make the difficult habitual, the habitual easy, and the easy beautiful. - [Constantin S. Stanislavski](https://www.goodreads.com/quotes/7102271-make-the-difficult-habitual-the-habitual-easy-and-the-easy)\n\n\u003e A good match is a **structured** dance, where players aim to **score** while they are following well-defined **rules**. This **freedom within a structure** is what makes it fun. - [ralphammer](https://ralphammer.com/how-to-get-started/)\n\n- [Pivot Points](https://longform.asmartbear.com/pivot-points/)\n\n  - non-judgmental aspects of personality that can be strengths in some contexts and weaknesses in others\n  - Pivot Points are fixed in the short term\n\n- [Hedged Bets](https://longform.asmartbear.com/predict-the-future/#hedged-bets)\n  - trading slightly less maximum upside for predictable, net-positive outcomes.\n\n\u003e “Motivation often comes after starting, not before. Action produces momentum.”\n\u003e [When you start a new habit, it should take less than two minutes to do.](https://jamesclear.com/how-to-stop-procrastinating)\n\u003e\n\u003e - James Clear\n\n\u003e Focus is more about **not** keeping busy when you need to wait for something.  \n\u003e Eat the boredom for a minute.\n\u003e\n\u003e - [[life-tips#wodenokoto]]\n\n\u003e [4 minutes run hard enough to push heart rate to 90%, 3 minutes recover, repeat 4 times](https://news.ycombinator.com/item?id=34213181)\n\u003e\n\u003e - https://www.ntnu.edu/cerg/advice\n\u003e - [Get running with Couch to 5K](https://www.nhs.uk/live-well/exercise/running-and-aerobic-exercises/get-running-with-couch-to-5k/)\n\n\u003e [recommended routine - bodyweightfitness](https://www.reddit.com/r/bodyweightfitness/wiki/kb/recommended_routine/) - I Don't Have This Much Time!\n\u003e\n\u003e - Don't workout at all (saves anywhere from 20 to 60 minutes, but really, really, really, really, really, really, really, really, really not recommended)\n\n\u003e 도무지 읽히지 않는 책 앞에서 내가 택한 방법은 펼쳐진 페이지 앞에서 멍때리기이다. 다르게 표현하면 이렇다. 펼쳐진 두 페이지 앞에서 오래 머물기.\n\u003e\n\u003e 책을 펼쳐놓는 것으로 충분하다. 읽지 못해도 좋다. 매일 정해진 진도를 나가야 하는 학교 수업이 아니니까. 하지만 읽지 않아도 괜찮다고 해서 펼쳐두지조차 않으면 곤란하다. 가능한 한 자주 책을 펼쳐두도록 하자. 전혀 읽지 않고 멍하니 바라보고 있다가 다시 덮게 되더라도\n\u003e\n\u003e - 막막한 독서. 시로군. P.10~13\n\n\u003e I think it should be everyone's primary focus to sleep well, drink water, get outside, get active, and eat generally decently. I hate to say it, but if you're not eating a good amount of vegetables and fruit, decent protein, sleep, etc, no amount of XYZ will catch up to that detriment. - [CE02](https://news.ycombinator.com/item?id=35056071)\n\n\u003e My real battle is doing good versus doing nothing. - [Deirdre Sullivan](https://www.npr.org/2005/08/08/4785079/always-go-to-the-funeral)\n\n[Kind Engineering](https://kind.engineering/) - How To Engineer Kindness\n\n\u003e Sometimes magic is just someone spending more time on something than anyone else might reasonably expect. - [Teller](https://www.goodreads.com/quotes/6641527-sometimes-magic-is-just-someone-spending-more-time-on-something)\n\n---\n\n## What I read in past\n\n- [[What I read in 2025|journal.what-i-read-in.2025]]\n  - [[2024|journal.what-i-read-in.2024]]\n  - [[2023|journal.what-i-read-in.2023]]\n  - [[2022|journal.what-i-read-in.2022]]\n- 📝 [Gists](https://gist.github.com/Luke-SNAW)\n- 📜 [Journals](https://luke-snaw.github.io/Luke-SNAW__netlify-CMS.github.io/)\n\n---\n\n- [[journal.what-i-struggled-brag-in]]\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":false,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"dendronVersion":"0.115.0","enableFullHierarchyNoteTitle":false,"enablePersistentHistory":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Luke SNAW","description":"Personal knowledge space"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","siteUrl":"https://luke-snaw.github.io/","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"bpw92y9ez0959qtnaedllfs"},"buildId":"vOE8u-mg___OsOsz4tjEg","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
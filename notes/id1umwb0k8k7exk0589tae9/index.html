<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>Making the Web Faster with Service Workers and Performance Research</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="Making the Web Faster with Service Workers and Performance Research"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://luke-snaw.github.io//notes/id1umwb0k8k7exk0589tae9/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="1/3/2023"/><meta property="article:modified_time" content="1/3/2023"/><link rel="canonical" href="https://luke-snaw.github.io//notes/id1umwb0k8k7exk0589tae9/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9d8e0603730b15a3.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/wirstT2ztC8OQsbzKjhvw/_buildManifest.js" defer=""></script><script src="/_next/static/wirstT2ztC8OQsbzKjhvw/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="making-the-web-faster-with-service-workers-and-performance-research">Making the Web Faster with Service Workers and Performance Research<a aria-hidden="true" class="anchor-heading icon-link" href="#making-the-web-faster-with-service-workers-and-performance-research"></a></h1>
<blockquote>
<p><a href="https://calendar.perfplanet.com/2022/making-the-web-faster-with-service-workers-and-performance-research/">https://calendar.perfplanet.com/2022/making-the-web-faster-with-service-workers-and-performance-research/</a>)</p>
</blockquote>
<h2 id="tldr">TLDR;<a aria-hidden="true" class="anchor-heading icon-link" href="#tldr"></a></h2>
<ul>
<li>We have conducted web performance research at the University of Hamburg since 2010.</li>
<li>We came up with a approach to cache dynamic website resources like HTML and APIs which can be plugged into websites via Service Workers
<ul>
<li>automatically dealing with personalization in HTML</li>
<li>keeping caches up-to-date by crowdsourcing cache checks</li>
<li>caching dynamic resources in the complete web cache hierarchy including browser cache through Bloom filter-based cache coherence algorithms</li>
<li>adding lots of other optimizations like predictive preloading, 3rd-party caching and image optimization</li>
</ul>
</li>
<li>Impact on Core Web Vitals can be A/B tested and measured via RUM on production traffic</li>
<li>Large-scale e-commerce pages show best performance improvements</li>
</ul>
<h2 id="targeting-large-gains-in-web-performance">Targeting large gains in web performance<a aria-hidden="true" class="anchor-heading icon-link" href="#targeting-large-gains-in-web-performance"></a></h2>
<p>Twelve years ago we started our research on web performance and caching of dynamic content with the goal to make the web a much faster place.</p>
<p>In this post we share the results of this journey and explain how the approach enables large performance gains A/B tested and measured with Real User Monitoring like the one in the chart below.</p>
<p><a href="https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled.png">Largest Contentful Paint comparison from an A/B test measured via Real User Monitoring (RUM).</a></p>
<p>References to scientific publications are scattered throughout the article but for a comprehensive list you can scroll down to our publications list at the very end.</p>
<h2 id="bottlenecks-in-modern-web-performance">Bottlenecks in modern web performance<a aria-hidden="true" class="anchor-heading icon-link" href="#bottlenecks-in-modern-web-performance"></a></h2>
<p>On a high level, all pages on the web load in the same way and experience the same sources of performance bottlenecks.</p>
<p>Let’s start with the basics here, to better highlight what changes about the page loading process when Service Worker is used. If you are a web expert feel free to skip to the next section.</p>
<p>On a high level, loading a website is very simple:</p>
<ol>
<li>The client requests the HTML resource.</li>
<li>The server generates a response and sends it to the client.</li>
<li>The client loads linked resources like CSS, JS, images and other assets.</li>
<li>The browser compiles and executes JavaScript code and renders the page.</li>
</ol>
<p><a href="https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled%201.png">A simplified diagram of how web pages are loaded</a></p>
<p>These steps also make up the main performance bottlenecks; or to frame it more positively, they highlight the common areas of optimization:</p>
<ol>
<li>Improve the first request with protocol optimizations (fast DNS, 0 round-trip TLS, OCSP stapling, HTTP/3).</li>
<li>On the backend, try to keep processing overhead to a minimum, optimize database queries and slow code paths, and use caching wherever possible.</li>
<li>Keep your critical rendering path (CRP) as short as possible by reducing the number of bytes sent over the network and avoiding render- or parser-blocking resources.</li>
<li>Keep the frontend “lean”, render the page without executing JavaScript, use efficient CSS and avoid layout shift.</li>
</ol>
<p>Most likely any company operating a website has lots of optimizations like these in their backlog, some being harder to implement than others.</p>
<p><strong>With our research we went a slightly different route and did not try to solve all of a website’s bottlenecks individually. Instead we use a Service Worker and special caching mechanisms to change how the browser loads the page</strong>.</p>
<h2 id="accelerating-dynamic-resources-how-it-works">Accelerating Dynamic Resources: How it works<a aria-hidden="true" class="anchor-heading icon-link" href="#accelerating-dynamic-resources-how-it-works"></a></h2>
<p>In contrast to traditional caching solution implemented on an infrastructural or backend level, we will come in on the client side and optimize how the browser loads the page and how it makes use of available caches. This also means there are no changes to the backend, frontend or infrastructure required at all.</p>
<p>The optimisation can be applied just by hosting and installing a special Service Worker. Once activated, the service worker runs in a background thread in the user’s browser and acts as a network proxy, seeing every request that the browser sends to the network. Service workers are supported in 97% of browsers, do not require permission to be used, and are completely invisible to users.</p>
<p>We will use the service worker to reroute requests to a distributed caching network, shown as Caching <em>service</em> in the graph below. The unique part here is that not only static files like JS, CSS and images are cached, but also <strong>dynamic resources like APIs or HTML</strong> that can change at any point in time.</p>
<p>For HTML resources, we need to ensures that personalized content (e.g. login state, recommendations, cart) is still loaded from the original website servers. We also need to automatically keep the cached content up-to-date and finally enables caching of these dynamic resources on every level of the cache hierarchy including the user’s device itself.</p>
<p><a href="https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled%202.png">Caching Architecture</a></p>
<p>Caching HTML resources of websites in a generic way is quite a complex task — for e-commerce pages in particular — and we are faced with three main challenges:</p>
<ol>
<li>How can HTML be cached if it is personalized?</li>
<li>How can caches be kept up to date for millions of pages when content changes unpredictably?</li>
<li>How can dynamic content be cached in the client without an API to purge it?</li>
</ol>
<p>The next three sections cover how we tackle those three challenges.</p>
<h3 id="challenge-1-caching-personalized-content">Challenge 1: Caching personalized content<a aria-hidden="true" class="anchor-heading icon-link" href="#challenge-1-caching-personalized-content"></a></h3>
<p>Personalization and caching are usually not compatible since multiple users cannot receive the same cached response without losing their personalization. So HTML that contains personalization is generally considered uncacheable.</p>
<p>That said, in most scenarios only parts of a page’s content are personalized and large sections look the same for all users. A good example for this is product detail pages on e-commerce platforms where prices, recommendations or the cart icon may be personalized, but the product image, title and description are the same for all users.</p>
<p>We are using this fact to our advantage and load personalized (non-cachable) and common (cacheable) parts separately by changing how the browser loads the initial HTML. In e-commerce in particular, the common part usually contains important information for the user that needs to be loaded as fast as possible.</p>
<p>The loading process works like this and is shown here:</p>
<p><a href="https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/new_perso.gif">The Service Worker requests two versions of the HTML document</a></p>
<ol>
<li>A user navigates to a page</li>
<li>The Service Worker issues two requests in parallel that race each other:
<ol>
<li>One request is sent to the Caching service, loading an anonymous version of the page. The request does not include any cookies or session information and the page is the same for all users and therefore cacheable. It is the one you would see if you opened the page in an Incognito or private tab.</li>
<li>The other request is sent to the origin backend. It contains all usual cookies and returns the personalized page tailored to the exact user.</li>
</ol>
</li>
<li>Usually, the cached HTML response is much faster and wins the race. The browser receives the HTML and can immediately start loading all the dependencies and render the anonymous page.</li>
<li>Once the backend response from the origin server with the personalized HTML is received, it is merged with the already-rendered HTML and the personalized sections of the page become visible. To the user, the personalized parts are progressively rendered.</li>
</ol>
<p>While the process does incur the overhead of loading two full HTML files containing redundant information, it usually renders personalized content faster than loading it via <code>fetch</code> requests from JavaScript since the request for personalized content is issued in parallel to the initial HTML request.</p>
<p>There are a lot of important details to how this merging of two HTMLs in the browser works and how it can be tweaked which goes beyond the scope of this blog post. For example, content merging can introduce flickers and shifts which can then be fixed via CSS.</p>
<p>The most important challenge of how this works with JavaScript or even modern frontend frameworks however will be addressed in the next section.</p>
<blockquote>
<p>???? Note: The approach of using two parallel HTML requests is a means to enable caching of those resources at all. Pages that do not contain any personalized content in the HTML, do not require this logic. For them the solutions to challenge two and three are the important optimizations.</p>
</blockquote>
<h4 id="challenge-11-delaying-js-to-avoid-problems-from-merging-personalized-content">Challenge 1.1: Delaying JS to avoid problems from merging personalized content<a aria-hidden="true" class="anchor-heading icon-link" href="#challenge-11-delaying-js-to-avoid-problems-from-merging-personalized-content"></a></h4>
<p>As explained in the previous section, applying personalization requires to merge the personalized HTML into the already rendered anonymous HTML in the browser which involves exchanging DOM nodes.</p>
<p>While the browser is very efficient in only rendering things that have changed, JavaScript is not so forgiving of sudden changes in the DOM nodes. Event listeners might break, click listeners might not be attached any longer, client-side-rendered elements might disappear and the shadow DOM of frameworks like React or Vue adds another layer of complexity.</p>
<p>To prevent JavaScript from breaking, the only option is to delay it until the personalized HTML is merged and no unexpected DOM changes can appear. Afterwards the JS execution can continue.</p>
<p>The following gif shows how requesting and merging two HTML files plays together with delaying the JS execution.</p>
<p><a href="https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/js.gif">We need to delay JS execution to avoid breakage after merging anonymous and personalized HTML</a></p>
<p>The details on how JS is delayed are very important to ensure correct execution and good performance. This is how we deal with this challenge:</p>
<ol>
<li>The cached HTML needs a few preparations that are applied automatically:
<ol>
<li>The code responsible for merging the personalized HTML document is inserted at the end of the <code>body</code>.</li>
<li>Since the code insertion happens asynchronously, we then place a blocking external script after that to pause JS execution until after the document merge.</li>
<li>Next, all external scripts are re-inserted after the blocking script, preserving their order. All inline scripts are removed (they tend to contain personalization and need to be executed from the personalized HTML).</li>
<li>An inline script is placed before each external <code>script</code> tag that executes all inline scripts from the personalized HTML.</li>
</ol>
</li>
<li>Once the cached HTML is handled by the browser and the parser get towards the end of the body it will detect the blocking script and request it. As long as that request is pending, JS execution cannot continue.</li>
<li>The Service Worker will receive that special JS request and withhold the response.</li>
<li>Once the code for merging the personalized HTML into the rendered document has completed, it sends a message to the Service Worker.</li>
<li>On receiving the message, the Service Worker responds to the pending script request with an empty script causing the JS execution to continue normally.</li>
<li>The inline script between the external script now have access to the personalized HTML and can simply insert and execute the inline script in the correct order.</li>
</ol>
<p>Delaying JS execution through this approach has some important advantages. Firstly, it prevents DOM events like <code>domInteractive</code> or <code>domContentLoaded</code> from firing before the actual JS is executed. Secondly, since the external scripts are in the cached document already, the preload scanner of the browser will discover them early, then download and compile them for faster execution. Thirdly, personalized or dynamic inline scripts that are very common in e-commerce applications are easily covered by this.</p>
<p>This approach enables caching even of personalized HTML pages. The next two challenges are important optimizations independent of whether the HTML is personalized.</p>
<h3 id="challenge-2-automatic-cache-sync-via-change-detection">Challenge 2: Automatic cache sync via Change Detection<a aria-hidden="true" class="anchor-heading icon-link" href="#challenge-2-automatic-cache-sync-via-change-detection"></a></h3>
<p>Not only is personalization a challenge when caching dynamic resources like HTMLs or API responses; even the anonymous version of the main document can change at any point in time, making it necessary to update caches accordingly. With potentially millions of cache entries necessary, this is a big challenge.</p>
<p>We will tackle this challenge with a concept called <em>Change Detection</em>. It builds on the mechanism used for personalization to detect changes to the page.</p>
<p>Since every user loads both the anonymous cached version of the HTML as well as the personalized version from origin, we already have everything we need to validate the cache entry on the client side. If something in the cached version has changed, we inform the Cache service, which refreshes the resource and updates the caches only if the content has actually changed.</p>
<p>As an example: let’s say someone changes the product title for a product detail page in the CMS. The cached version of the page will contain the old product title and is no longer up-to-date. When a user navigates to the product page, they receive the stale cache entry with the old title. Once the personalized HTML with the new product title arrives in the browser, the versions are merged and the new product title appears. As both HTML versions are compared, a cache refresh is triggered because of the changed title.</p>
<p>The first user to detect the change will experience a flicker of the title when the up-to-date title appears upon merge. All other users and even the first user upon reload will not experience that flicker and receive an up-to-date cached version. This effectively crowdsources the challenge of discovering outdated cache entries among millions of cache entries without increasing the origin server load.</p>
<p>Additionally, we use the “cache hotness” information from our shared caches to sample the change detection on frequently-visited pages to reduce overhead.</p>
<p><a href="https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled%203.png">Cache synchronization is crowdsourced through “Change Detection”</a></p>
<p>Deployments of the whole application are treated separately from page-level difference because they can result in structural changes that make it impossible to merge the cached version with the new original version. We can handle new deployments in a process called <em>Deployment Detection</em>: If the site links new JS and CSS versions, the Cache services infers that a deployment was rolled out so it can purge the entire cache and rebuild it asynchronously.</p>
<p>The Cache service also has an option to define periodic refreshes (mostly used for dynamic 3rd-party scripts) and a Purge API for deeper integration. Whenever content is updated and caches are purged, the Cache service automatically pre-warms cache PoPs in regions with a lot of traffic to prevent cache misses.</p>
<p>With this automated approach to cache updates we keep the integration as easy as possible. The approach works great in production and can always be extended by API-initialized cache updates if necessary.</p>
<h3 id="challenge-3-browser-caching-without-staleness">Challenge 3: Browser caching without staleness<a aria-hidden="true" class="anchor-heading icon-link" href="#challenge-3-browser-caching-without-staleness"></a></h3>
<p>In the previous sections we have established how we can deal with personalization and still cache HTML resources and how we enable the Cache service to detect changes and update its cache in near real time. The main open question is how can dynamic content be cached in the Browser without an API to purge it?</p>
<p>Standard HTTP caching is not equipped to deal with resource changing irregularly. The standard procedure with HTTP caching is:</p>
<ol>
<li>The client sends a request to the server.</li>
<li>The server responds with a resource and attaches a <code>Cache-Control</code> header, which contains a time-to-live (TTL).</li>
<li>On the way to the client, the resource is stored by caches. We distinguish between two kinds of caches:
<ol>
<li>Invalidation-based caches which have APIs to remove (purge) content from them. This is your standard CDN or server side cache that is shared by all users. It will generally hold on to your cache entry until the TTL expires but you can send purge requests to evict cache entries immediately.</li>
<li>Expiration-based caches that hold on to resources until their TTL expires without an API to purge them. This is your client side caches like browser cache or service worker cache storage.</li>
</ol>
</li>
<li>On subsequent requests these caches serve the stored content as long as the TTL permits.</li>
</ol>
<p>It is so hard to cache HTML file in the client because issues arise when the cached content changes before the TTL expires. While invalidation-based caches can be purged, expiration-based caches will continue to send stale content back to clients.</p>
<h4 id="dynamic-browser-caching-the-bloom-filter-based-cache-sketch">Dynamic Browser Caching: The Bloom filter-based cache sketch<a aria-hidden="true" class="anchor-heading icon-link" href="#dynamic-browser-caching-the-bloom-filter-based-cache-sketch"></a></h4>
<p>Our approach is to cache all resources in the browser and set high TTL values that are <a href="https://medium.baqend.com/dynamic-cache-lifetime-with-immutable-cache-control-headers-db066cf60fea">dynamically estimated</a>. So what happens when cached resources are changed before their TTL expires?</p>
<p>When the Cache service detects a resource change that is still cached by clients (e.g. an HTML file), it does two things:</p>
<ol>
<li>It purges the content in the invalidation-based cache in the CDN and backend.</li>
<li>It adds the URL of the resource in a probabilistic set data structure called a counting <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter</a> for the remainder of the highest delivered TTL.</li>
</ol>
<p>The counting Bloom filter is then flattened into a regular Bloom filter and transferred to every connecting client.</p>
<p>When the client loads a resource, the service worker first checks whether the URL is contained in the Bloom filter. If it is not, it can be safely taken from the browser cache. If it is, the worker forwards the request to the network, loads the resource from the CDN and updates the local cache with it.</p>
<p>The great thing about Bloom filters is that they are very compact by allowing for false positives: sometimes the Bloom filter will match a URL that was not inserted. False negatives on the other hand cannot happen — a URL that was inserted will always be matched by the Bloom filter.</p>
<p>To give an example of the compactness: A Bloom filter with a false positive rate of &#x3C; 5% can store 20k different URLs in under 10kB. So it fits within the initial TCP congestion window and can usually be transferred to the client in a single round trip.</p>
<p><a href="https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled%204.png">Checking for cache hits using a Bloom filter</a></p>
<p>Of course there are lots of intricate details and trade-offs involved — Bloom filter size, update cycles, TTL estimation, distributed server implementations to name a few — to make sure the performance is maximized.</p>
<h3 id="trade-offs">Trade-offs<a aria-hidden="true" class="anchor-heading icon-link" href="#trade-offs"></a></h3>
<p>The way our caching approach works makes it a very powerful tool but as with any technology there are trade-offs to consider that make it more suitable on some pages compared to others.</p>
<ol>
<li>The way personalized content is loaded and rendered means that the biggest performance impact is on pages where the main content including the <a href="https://web.dev/lcp/">largest contentful paint (LCP)</a> element is not personalized and can be rendered from the cached HTML. If the main content is personalized, performance is still improved by Speed Kit, but the optimization is less impactful. On e-commerce pages we therefore focus mostly on home, category and product pages and exclude fully personalized pages like cart or checkout.</li>
<li>Client-side rendered sites are a nightmare when it comes to web performance with their <a href="https://web.dev/vitals/">core web vitals</a> ranging among the worst. On those sites, the approach can struggle to achieve a large performance impact since as long a the HTML is personalized, JS execution needs to be delayed which also delays rendering of the page. On the positive side, most e-commerce pages we deal with already use <a href="https://web.dev/rendering-on-the-web/">server side rendering (SSR)</a> and we are testing ways to render above the fold content on our cache servers for page that cannot migrate easily.</li>
<li>First loads of a completely new user are not accelerated in this approach because the service worker needs to be installed on the first page load. It is very persistent afterwards so returning users and any navigation on the site are fully accelerated.</li>
<li>Change Detection is a great technology that works extremely well in product due to an important trade off in its design. It takes one user to load a stale cache entry to detect the change and update the cache. This means that one user will see a flicker from old content to new content when the fresh document from the original server is merged. The flicker is fixed for other users and further reloads due to the change detection. In many scenarios this is a worthwhile trade-off. Content where this is not acceptable needs to be hidden until after the merge.</li>
</ol>
<h2 id="advanced-frontend-optimizations">Advanced frontend optimizations<a aria-hidden="true" class="anchor-heading icon-link" href="#advanced-frontend-optimizations"></a></h2>
<p>The use of Service Worker combined with access to Real User Monitoring (RUM) data enable other powerful optimizations:</p>
<ol>
<li><strong>Predictive Preloads</strong>: RUM data enables learning algorithms to predict where the user can navigate next. This means the cached HTML can already be preloaded before the navigation. Usually servers for shop systems cannot cope with the added load of such preloads, but cache servers do not mind the extra traffic.</li>
<li><strong>3rd-Party Caching</strong>: 3rd-party resources can be cached by the Service Worker as well to optimize their caching, save on the additional TLS connection and use a connection that already has bigger bandwidth avoiding <a href="https://developer.mozilla.org/en-US/docs/Glossary/TCP_slow_start">TCP slow start</a> for the new connection.</li>
<li><strong>Image Optimization</strong>: By running on the user’s device, the service worker has access to the screen size and DPR of the user. It attaches this information to the image requests and uses an image service to transcode, resize and recompress images automatically.</li>
</ol>
<h2 id="evaluating-performance-with-ab-tests-and-rum">Evaluating performance with A/B tests and RUM<a aria-hidden="true" class="anchor-heading icon-link" href="#evaluating-performance-with-ab-tests-and-rum"></a></h2>
<p>Of course, performance bottlenecks vary from site to site. That is why we constantly evaluate and test how this caching approach can improve performance on new websites using A/B tests and Real User Monitoring (RUM) data.</p>
<p>Since the integration is based on JavaScript in the client, you can easily run an A/B test to try out such a solution with 50% of users accelerated and the other 50% serving as the control group. In both group you can gather performance data with via a <a href="https://aws.amazon.com/de/blogs/big-data/how-baqend-built-a-real-time-web-analytics-platform-using-amazon-kinesis-data-analytics-for-apache-flink/">RUM tool</a>.</p>
<p>After a few weeks of data collection, you can compare the performance between the two groups and are able to prove the significant improvements to our customers. The RUM tool also stays in place after the A/B test to monitor performance and work with our clients to achieve optimal performance in the long run.</p>
<p>We also use the same methodology of A/B tests with RUM tracking to research and evaluate new features and optimizations. <strong>Access to detailed RUM data from about 300 million users per month is what enabled us to perfect the approach and achieve great performance improvements.</strong></p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#tldr" title="TLDR;">TLDR;</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#targeting-large-gains-in-web-performance" title="Targeting large gains in web performance">Targeting large gains in web performance</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#bottlenecks-in-modern-web-performance" title="Bottlenecks in modern web performance">Bottlenecks in modern web performance</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#accelerating-dynamic-resources-how-it-works" title="Accelerating Dynamic Resources: How it works">Accelerating Dynamic Resources: How it works</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#challenge-1-caching-personalized-content" title="Challenge 1: Caching personalized content">Challenge 1: Caching personalized content</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#challenge-11-delaying-js-to-avoid-problems-from-merging-personalized-content" title="Challenge 1.1: Delaying JS to avoid problems from merging personalized content">Challenge 1.1: Delaying JS to avoid problems from merging personalized content</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#challenge-2-automatic-cache-sync-via-change-detection" title="Challenge 2: Automatic cache sync via Change Detection">Challenge 2: Automatic cache sync via Change Detection</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#challenge-3-browser-caching-without-staleness" title="Challenge 3: Browser caching without staleness">Challenge 3: Browser caching without staleness</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#dynamic-browser-caching-the-bloom-filter-based-cache-sketch" title="Dynamic Browser Caching: The Bloom filter-based cache sketch">Dynamic Browser Caching: The Bloom filter-based cache sketch</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#trade-offs" title="Trade-offs">Trade-offs</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#advanced-frontend-optimizations" title="Advanced frontend optimizations">Advanced frontend optimizations</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#evaluating-performance-with-ab-tests-and-rum" title="Evaluating performance with A/B tests and RUM">Evaluating performance with A/B tests and RUM</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"id1umwb0k8k7exk0589tae9","title":"Making the Web Faster with Service Workers and Performance Research","desc":"","updated":1672708357794,"created":1672707051806,"custom":{},"fname":"dev.web.performance.making-the-web-faster-with-service-workers-and-performance-research","type":"note","vault":{"fsPath":"vault"},"contentHash":"97ec1369a3d6c82b0f2f3057b1967970","links":[],"anchors":{"tldr":{"type":"header","text":"TLDR;","value":"tldr","line":10,"column":0,"depth":2},"targeting-large-gains-in-web-performance":{"type":"header","text":"Targeting large gains in web performance","value":"targeting-large-gains-in-web-performance","line":21,"column":0,"depth":2},"bottlenecks-in-modern-web-performance":{"type":"header","text":"Bottlenecks in modern web performance","value":"bottlenecks-in-modern-web-performance","line":31,"column":0,"depth":2},"accelerating-dynamic-resources-how-it-works":{"type":"header","text":"Accelerating Dynamic Resources: How it works","value":"accelerating-dynamic-resources-how-it-works","line":57,"column":0,"depth":2},"challenge-1-caching-personalized-content":{"type":"header","text":"Challenge 1: Caching personalized content","value":"challenge-1-caching-personalized-content","line":77,"column":0,"depth":3},"challenge-11-delaying-js-to-avoid-problems-from-merging-personalized-content":{"type":"header","text":"Challenge 1.1: Delaying JS to avoid problems from merging personalized content","value":"challenge-11-delaying-js-to-avoid-problems-from-merging-personalized-content","line":104,"column":0,"depth":4},"challenge-2-automatic-cache-sync-via-change-detection":{"type":"header","text":"Challenge 2: Automatic cache sync via Change Detection","value":"challenge-2-automatic-cache-sync-via-change-detection","line":133,"column":0,"depth":3},"challenge-3-browser-caching-without-staleness":{"type":"header","text":"Challenge 3: Browser caching without staleness","value":"challenge-3-browser-caching-without-staleness","line":155,"column":0,"depth":3},"dynamic-browser-caching-the-bloom-filter-based-cache-sketch":{"type":"header","text":"Dynamic Browser Caching: The Bloom filter-based cache sketch","value":"dynamic-browser-caching-the-bloom-filter-based-cache-sketch","line":170,"column":0,"depth":4},"trade-offs":{"type":"header","text":"Trade-offs","value":"trade-offs","line":191,"column":0,"depth":3},"advanced-frontend-optimizations":{"type":"header","text":"Advanced frontend optimizations","value":"advanced-frontend-optimizations","line":200,"column":0,"depth":2},"evaluating-performance-with-ab-tests-and-rum":{"type":"header","text":"Evaluating performance with A/B tests and RUM","value":"evaluating-performance-with-ab-tests-and-rum","line":208,"column":0,"depth":2}},"children":[],"parent":"pa1bstln0xt54o30z6taebc","data":{}},"body":"\u003ch1 id=\"making-the-web-faster-with-service-workers-and-performance-research\"\u003eMaking the Web Faster with Service Workers and Performance Research\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#making-the-web-faster-with-service-workers-and-performance-research\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://calendar.perfplanet.com/2022/making-the-web-faster-with-service-workers-and-performance-research/\"\u003ehttps://calendar.perfplanet.com/2022/making-the-web-faster-with-service-workers-and-performance-research/\u003c/a\u003e)\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"tldr\"\u003eTLDR;\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#tldr\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eWe have conducted web performance research at the University of Hamburg since 2010.\u003c/li\u003e\n\u003cli\u003eWe came up with a approach to cache dynamic website resources like HTML and APIs which can be plugged into websites via Service Workers\n\u003cul\u003e\n\u003cli\u003eautomatically dealing with personalization in HTML\u003c/li\u003e\n\u003cli\u003ekeeping caches up-to-date by crowdsourcing cache checks\u003c/li\u003e\n\u003cli\u003ecaching dynamic resources in the complete web cache hierarchy including browser cache through Bloom filter-based cache coherence algorithms\u003c/li\u003e\n\u003cli\u003eadding lots of other optimizations like predictive preloading, 3rd-party caching and image optimization\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eImpact on Core Web Vitals can be A/B tested and measured via RUM on production traffic\u003c/li\u003e\n\u003cli\u003eLarge-scale e-commerce pages show best performance improvements\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"targeting-large-gains-in-web-performance\"\u003eTargeting large gains in web performance\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#targeting-large-gains-in-web-performance\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eTwelve years ago we started our research on web performance and caching of dynamic content with the goal to make the web a much faster place.\u003c/p\u003e\n\u003cp\u003eIn this post we share the results of this journey and explain how the approach enables large performance gains A/B tested and measured with Real User Monitoring like the one in the chart below.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled.png\"\u003eLargest Contentful Paint comparison from an A/B test measured via Real User Monitoring (RUM).\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eReferences to scientific publications are scattered throughout the article but for a comprehensive list you can scroll down to our publications list at the very end.\u003c/p\u003e\n\u003ch2 id=\"bottlenecks-in-modern-web-performance\"\u003eBottlenecks in modern web performance\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#bottlenecks-in-modern-web-performance\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eOn a high level, all pages on the web load in the same way and experience the same sources of performance bottlenecks.\u003c/p\u003e\n\u003cp\u003eLet’s start with the basics here, to better highlight what changes about the page loading process when Service Worker is used. If you are a web expert feel free to skip to the next section.\u003c/p\u003e\n\u003cp\u003eOn a high level, loading a website is very simple:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe client requests the HTML resource.\u003c/li\u003e\n\u003cli\u003eThe server generates a response and sends it to the client.\u003c/li\u003e\n\u003cli\u003eThe client loads linked resources like CSS, JS, images and other assets.\u003c/li\u003e\n\u003cli\u003eThe browser compiles and executes JavaScript code and renders the page.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled%201.png\"\u003eA simplified diagram of how web pages are loaded\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThese steps also make up the main performance bottlenecks; or to frame it more positively, they highlight the common areas of optimization:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eImprove the first request with protocol optimizations (fast DNS, 0 round-trip TLS, OCSP stapling, HTTP/3).\u003c/li\u003e\n\u003cli\u003eOn the backend, try to keep processing overhead to a minimum, optimize database queries and slow code paths, and use caching wherever possible.\u003c/li\u003e\n\u003cli\u003eKeep your critical rendering path (CRP) as short as possible by reducing the number of bytes sent over the network and avoiding render- or parser-blocking resources.\u003c/li\u003e\n\u003cli\u003eKeep the frontend “lean”, render the page without executing JavaScript, use efficient CSS and avoid layout shift.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMost likely any company operating a website has lots of optimizations like these in their backlog, some being harder to implement than others.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWith our research we went a slightly different route and did not try to solve all of a website’s bottlenecks individually. Instead we use a Service Worker and special caching mechanisms to change how the browser loads the page\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"accelerating-dynamic-resources-how-it-works\"\u003eAccelerating Dynamic Resources: How it works\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#accelerating-dynamic-resources-how-it-works\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eIn contrast to traditional caching solution implemented on an infrastructural or backend level, we will come in on the client side and optimize how the browser loads the page and how it makes use of available caches. This also means there are no changes to the backend, frontend or infrastructure required at all.\u003c/p\u003e\n\u003cp\u003eThe optimisation can be applied just by hosting and installing a special Service Worker. Once activated, the service worker runs in a background thread in the user’s browser and acts as a network proxy, seeing every request that the browser sends to the network. Service workers are supported in 97% of browsers, do not require permission to be used, and are completely invisible to users.\u003c/p\u003e\n\u003cp\u003eWe will use the service worker to reroute requests to a distributed caching network, shown as Caching \u003cem\u003eservice\u003c/em\u003e in the graph below. The unique part here is that not only static files like JS, CSS and images are cached, but also \u003cstrong\u003edynamic resources like APIs or HTML\u003c/strong\u003e that can change at any point in time.\u003c/p\u003e\n\u003cp\u003eFor HTML resources, we need to ensures that personalized content (e.g. login state, recommendations, cart) is still loaded from the original website servers. We also need to automatically keep the cached content up-to-date and finally enables caching of these dynamic resources on every level of the cache hierarchy including the user’s device itself.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled%202.png\"\u003eCaching Architecture\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCaching HTML resources of websites in a generic way is quite a complex task — for e-commerce pages in particular — and we are faced with three main challenges:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eHow can HTML be cached if it is personalized?\u003c/li\u003e\n\u003cli\u003eHow can caches be kept up to date for millions of pages when content changes unpredictably?\u003c/li\u003e\n\u003cli\u003eHow can dynamic content be cached in the client without an API to purge it?\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe next three sections cover how we tackle those three challenges.\u003c/p\u003e\n\u003ch3 id=\"challenge-1-caching-personalized-content\"\u003eChallenge 1: Caching personalized content\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#challenge-1-caching-personalized-content\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003ePersonalization and caching are usually not compatible since multiple users cannot receive the same cached response without losing their personalization. So HTML that contains personalization is generally considered uncacheable.\u003c/p\u003e\n\u003cp\u003eThat said, in most scenarios only parts of a page’s content are personalized and large sections look the same for all users. A good example for this is product detail pages on e-commerce platforms where prices, recommendations or the cart icon may be personalized, but the product image, title and description are the same for all users.\u003c/p\u003e\n\u003cp\u003eWe are using this fact to our advantage and load personalized (non-cachable) and common (cacheable) parts separately by changing how the browser loads the initial HTML. In e-commerce in particular, the common part usually contains important information for the user that needs to be loaded as fast as possible.\u003c/p\u003e\n\u003cp\u003eThe loading process works like this and is shown here:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/new_perso.gif\"\u003eThe Service Worker requests two versions of the HTML document\u003c/a\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eA user navigates to a page\u003c/li\u003e\n\u003cli\u003eThe Service Worker issues two requests in parallel that race each other:\n\u003col\u003e\n\u003cli\u003eOne request is sent to the Caching service, loading an anonymous version of the page. The request does not include any cookies or session information and the page is the same for all users and therefore cacheable. It is the one you would see if you opened the page in an Incognito or private tab.\u003c/li\u003e\n\u003cli\u003eThe other request is sent to the origin backend. It contains all usual cookies and returns the personalized page tailored to the exact user.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eUsually, the cached HTML response is much faster and wins the race. The browser receives the HTML and can immediately start loading all the dependencies and render the anonymous page.\u003c/li\u003e\n\u003cli\u003eOnce the backend response from the origin server with the personalized HTML is received, it is merged with the already-rendered HTML and the personalized sections of the page become visible. To the user, the personalized parts are progressively rendered.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWhile the process does incur the overhead of loading two full HTML files containing redundant information, it usually renders personalized content faster than loading it via \u003ccode\u003efetch\u003c/code\u003e requests from JavaScript since the request for personalized content is issued in parallel to the initial HTML request.\u003c/p\u003e\n\u003cp\u003eThere are a lot of important details to how this merging of two HTMLs in the browser works and how it can be tweaked which goes beyond the scope of this blog post. For example, content merging can introduce flickers and shifts which can then be fixed via CSS.\u003c/p\u003e\n\u003cp\u003eThe most important challenge of how this works with JavaScript or even modern frontend frameworks however will be addressed in the next section.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e???? Note: The approach of using two parallel HTML requests is a means to enable caching of those resources at all. Pages that do not contain any personalized content in the HTML, do not require this logic. For them the solutions to challenge two and three are the important optimizations.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4 id=\"challenge-11-delaying-js-to-avoid-problems-from-merging-personalized-content\"\u003eChallenge 1.1: Delaying JS to avoid problems from merging personalized content\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#challenge-11-delaying-js-to-avoid-problems-from-merging-personalized-content\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eAs explained in the previous section, applying personalization requires to merge the personalized HTML into the already rendered anonymous HTML in the browser which involves exchanging DOM nodes.\u003c/p\u003e\n\u003cp\u003eWhile the browser is very efficient in only rendering things that have changed, JavaScript is not so forgiving of sudden changes in the DOM nodes. Event listeners might break, click listeners might not be attached any longer, client-side-rendered elements might disappear and the shadow DOM of frameworks like React or Vue adds another layer of complexity.\u003c/p\u003e\n\u003cp\u003eTo prevent JavaScript from breaking, the only option is to delay it until the personalized HTML is merged and no unexpected DOM changes can appear. Afterwards the JS execution can continue.\u003c/p\u003e\n\u003cp\u003eThe following gif shows how requesting and merging two HTML files plays together with delaying the JS execution.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/js.gif\"\u003eWe need to delay JS execution to avoid breakage after merging anonymous and personalized HTML\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe details on how JS is delayed are very important to ensure correct execution and good performance. This is how we deal with this challenge:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe cached HTML needs a few preparations that are applied automatically:\n\u003col\u003e\n\u003cli\u003eThe code responsible for merging the personalized HTML document is inserted at the end of the \u003ccode\u003ebody\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eSince the code insertion happens asynchronously, we then place a blocking external script after that to pause JS execution until after the document merge.\u003c/li\u003e\n\u003cli\u003eNext, all external scripts are re-inserted after the blocking script, preserving their order. All inline scripts are removed (they tend to contain personalization and need to be executed from the personalized HTML).\u003c/li\u003e\n\u003cli\u003eAn inline script is placed before each external \u003ccode\u003escript\u003c/code\u003e tag that executes all inline scripts from the personalized HTML.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eOnce the cached HTML is handled by the browser and the parser get towards the end of the body it will detect the blocking script and request it. As long as that request is pending, JS execution cannot continue.\u003c/li\u003e\n\u003cli\u003eThe Service Worker will receive that special JS request and withhold the response.\u003c/li\u003e\n\u003cli\u003eOnce the code for merging the personalized HTML into the rendered document has completed, it sends a message to the Service Worker.\u003c/li\u003e\n\u003cli\u003eOn receiving the message, the Service Worker responds to the pending script request with an empty script causing the JS execution to continue normally.\u003c/li\u003e\n\u003cli\u003eThe inline script between the external script now have access to the personalized HTML and can simply insert and execute the inline script in the correct order.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eDelaying JS execution through this approach has some important advantages. Firstly, it prevents DOM events like \u003ccode\u003edomInteractive\u003c/code\u003e or \u003ccode\u003edomContentLoaded\u003c/code\u003e from firing before the actual JS is executed. Secondly, since the external scripts are in the cached document already, the preload scanner of the browser will discover them early, then download and compile them for faster execution. Thirdly, personalized or dynamic inline scripts that are very common in e-commerce applications are easily covered by this.\u003c/p\u003e\n\u003cp\u003eThis approach enables caching even of personalized HTML pages. The next two challenges are important optimizations independent of whether the HTML is personalized.\u003c/p\u003e\n\u003ch3 id=\"challenge-2-automatic-cache-sync-via-change-detection\"\u003eChallenge 2: Automatic cache sync via Change Detection\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#challenge-2-automatic-cache-sync-via-change-detection\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eNot only is personalization a challenge when caching dynamic resources like HTMLs or API responses; even the anonymous version of the main document can change at any point in time, making it necessary to update caches accordingly. With potentially millions of cache entries necessary, this is a big challenge.\u003c/p\u003e\n\u003cp\u003eWe will tackle this challenge with a concept called \u003cem\u003eChange Detection\u003c/em\u003e. It builds on the mechanism used for personalization to detect changes to the page.\u003c/p\u003e\n\u003cp\u003eSince every user loads both the anonymous cached version of the HTML as well as the personalized version from origin, we already have everything we need to validate the cache entry on the client side. If something in the cached version has changed, we inform the Cache service, which refreshes the resource and updates the caches only if the content has actually changed.\u003c/p\u003e\n\u003cp\u003eAs an example: let’s say someone changes the product title for a product detail page in the CMS. The cached version of the page will contain the old product title and is no longer up-to-date. When a user navigates to the product page, they receive the stale cache entry with the old title. Once the personalized HTML with the new product title arrives in the browser, the versions are merged and the new product title appears. As both HTML versions are compared, a cache refresh is triggered because of the changed title.\u003c/p\u003e\n\u003cp\u003eThe first user to detect the change will experience a flicker of the title when the up-to-date title appears upon merge. All other users and even the first user upon reload will not experience that flicker and receive an up-to-date cached version. This effectively crowdsources the challenge of discovering outdated cache entries among millions of cache entries without increasing the origin server load.\u003c/p\u003e\n\u003cp\u003eAdditionally, we use the “cache hotness” information from our shared caches to sample the change detection on frequently-visited pages to reduce overhead.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled%203.png\"\u003eCache synchronization is crowdsourced through “Change Detection”\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDeployments of the whole application are treated separately from page-level difference because they can result in structural changes that make it impossible to merge the cached version with the new original version. We can handle new deployments in a process called \u003cem\u003eDeployment Detection\u003c/em\u003e: If the site links new JS and CSS versions, the Cache services infers that a deployment was rolled out so it can purge the entire cache and rebuild it asynchronously.\u003c/p\u003e\n\u003cp\u003eThe Cache service also has an option to define periodic refreshes (mostly used for dynamic 3rd-party scripts) and a Purge API for deeper integration. Whenever content is updated and caches are purged, the Cache service automatically pre-warms cache PoPs in regions with a lot of traffic to prevent cache misses.\u003c/p\u003e\n\u003cp\u003eWith this automated approach to cache updates we keep the integration as easy as possible. The approach works great in production and can always be extended by API-initialized cache updates if necessary.\u003c/p\u003e\n\u003ch3 id=\"challenge-3-browser-caching-without-staleness\"\u003eChallenge 3: Browser caching without staleness\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#challenge-3-browser-caching-without-staleness\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eIn the previous sections we have established how we can deal with personalization and still cache HTML resources and how we enable the Cache service to detect changes and update its cache in near real time. The main open question is how can dynamic content be cached in the Browser without an API to purge it?\u003c/p\u003e\n\u003cp\u003eStandard HTTP caching is not equipped to deal with resource changing irregularly. The standard procedure with HTTP caching is:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe client sends a request to the server.\u003c/li\u003e\n\u003cli\u003eThe server responds with a resource and attaches a \u003ccode\u003eCache-Control\u003c/code\u003e header, which contains a time-to-live (TTL).\u003c/li\u003e\n\u003cli\u003eOn the way to the client, the resource is stored by caches. We distinguish between two kinds of caches:\n\u003col\u003e\n\u003cli\u003eInvalidation-based caches which have APIs to remove (purge) content from them. This is your standard CDN or server side cache that is shared by all users. It will generally hold on to your cache entry until the TTL expires but you can send purge requests to evict cache entries immediately.\u003c/li\u003e\n\u003cli\u003eExpiration-based caches that hold on to resources until their TTL expires without an API to purge them. This is your client side caches like browser cache or service worker cache storage.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eOn subsequent requests these caches serve the stored content as long as the TTL permits.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIt is so hard to cache HTML file in the client because issues arise when the cached content changes before the TTL expires. While invalidation-based caches can be purged, expiration-based caches will continue to send stale content back to clients.\u003c/p\u003e\n\u003ch4 id=\"dynamic-browser-caching-the-bloom-filter-based-cache-sketch\"\u003eDynamic Browser Caching: The Bloom filter-based cache sketch\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#dynamic-browser-caching-the-bloom-filter-based-cache-sketch\"\u003e\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eOur approach is to cache all resources in the browser and set high TTL values that are \u003ca href=\"https://medium.baqend.com/dynamic-cache-lifetime-with-immutable-cache-control-headers-db066cf60fea\"\u003edynamically estimated\u003c/a\u003e. So what happens when cached resources are changed before their TTL expires?\u003c/p\u003e\n\u003cp\u003eWhen the Cache service detects a resource change that is still cached by clients (e.g. an HTML file), it does two things:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIt purges the content in the invalidation-based cache in the CDN and backend.\u003c/li\u003e\n\u003cli\u003eIt adds the URL of the resource in a probabilistic set data structure called a counting \u003ca href=\"https://en.wikipedia.org/wiki/Bloom_filter\"\u003eBloom filter\u003c/a\u003e for the remainder of the highest delivered TTL.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe counting Bloom filter is then flattened into a regular Bloom filter and transferred to every connecting client.\u003c/p\u003e\n\u003cp\u003eWhen the client loads a resource, the service worker first checks whether the URL is contained in the Bloom filter. If it is not, it can be safely taken from the browser cache. If it is, the worker forwards the request to the network, loads the resource from the CDN and updates the local cache with it.\u003c/p\u003e\n\u003cp\u003eThe great thing about Bloom filters is that they are very compact by allowing for false positives: sometimes the Bloom filter will match a URL that was not inserted. False negatives on the other hand cannot happen — a URL that was inserted will always be matched by the Bloom filter.\u003c/p\u003e\n\u003cp\u003eTo give an example of the compactness: A Bloom filter with a false positive rate of \u0026#x3C; 5% can store 20k different URLs in under 10kB. So it fits within the initial TCP congestion window and can usually be transferred to the client in a single round trip.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://calendar.perfplanet.com/wp-content/uploads/2022/12/erik/Untitled%204.png\"\u003eChecking for cache hits using a Bloom filter\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eOf course there are lots of intricate details and trade-offs involved — Bloom filter size, update cycles, TTL estimation, distributed server implementations to name a few — to make sure the performance is maximized.\u003c/p\u003e\n\u003ch3 id=\"trade-offs\"\u003eTrade-offs\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#trade-offs\"\u003e\u003c/a\u003e\u003c/h3\u003e\n\u003cp\u003eThe way our caching approach works makes it a very powerful tool but as with any technology there are trade-offs to consider that make it more suitable on some pages compared to others.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe way personalized content is loaded and rendered means that the biggest performance impact is on pages where the main content including the \u003ca href=\"https://web.dev/lcp/\"\u003elargest contentful paint (LCP)\u003c/a\u003e element is not personalized and can be rendered from the cached HTML. If the main content is personalized, performance is still improved by Speed Kit, but the optimization is less impactful. On e-commerce pages we therefore focus mostly on home, category and product pages and exclude fully personalized pages like cart or checkout.\u003c/li\u003e\n\u003cli\u003eClient-side rendered sites are a nightmare when it comes to web performance with their \u003ca href=\"https://web.dev/vitals/\"\u003ecore web vitals\u003c/a\u003e ranging among the worst. On those sites, the approach can struggle to achieve a large performance impact since as long a the HTML is personalized, JS execution needs to be delayed which also delays rendering of the page. On the positive side, most e-commerce pages we deal with already use \u003ca href=\"https://web.dev/rendering-on-the-web/\"\u003eserver side rendering (SSR)\u003c/a\u003e and we are testing ways to render above the fold content on our cache servers for page that cannot migrate easily.\u003c/li\u003e\n\u003cli\u003eFirst loads of a completely new user are not accelerated in this approach because the service worker needs to be installed on the first page load. It is very persistent afterwards so returning users and any navigation on the site are fully accelerated.\u003c/li\u003e\n\u003cli\u003eChange Detection is a great technology that works extremely well in product due to an important trade off in its design. It takes one user to load a stale cache entry to detect the change and update the cache. This means that one user will see a flicker from old content to new content when the fresh document from the original server is merged. The flicker is fixed for other users and further reloads due to the change detection. In many scenarios this is a worthwhile trade-off. Content where this is not acceptable needs to be hidden until after the merge.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"advanced-frontend-optimizations\"\u003eAdvanced frontend optimizations\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#advanced-frontend-optimizations\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eThe use of Service Worker combined with access to Real User Monitoring (RUM) data enable other powerful optimizations:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003ePredictive Preloads\u003c/strong\u003e: RUM data enables learning algorithms to predict where the user can navigate next. This means the cached HTML can already be preloaded before the navigation. Usually servers for shop systems cannot cope with the added load of such preloads, but cache servers do not mind the extra traffic.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e3rd-Party Caching\u003c/strong\u003e: 3rd-party resources can be cached by the Service Worker as well to optimize their caching, save on the additional TLS connection and use a connection that already has bigger bandwidth avoiding \u003ca href=\"https://developer.mozilla.org/en-US/docs/Glossary/TCP_slow_start\"\u003eTCP slow start\u003c/a\u003e for the new connection.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImage Optimization\u003c/strong\u003e: By running on the user’s device, the service worker has access to the screen size and DPR of the user. It attaches this information to the image requests and uses an image service to transcode, resize and recompress images automatically.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"evaluating-performance-with-ab-tests-and-rum\"\u003eEvaluating performance with A/B tests and RUM\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#evaluating-performance-with-ab-tests-and-rum\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eOf course, performance bottlenecks vary from site to site. That is why we constantly evaluate and test how this caching approach can improve performance on new websites using A/B tests and Real User Monitoring (RUM) data.\u003c/p\u003e\n\u003cp\u003eSince the integration is based on JavaScript in the client, you can easily run an A/B test to try out such a solution with 50% of users accelerated and the other 50% serving as the control group. In both group you can gather performance data with via a \u003ca href=\"https://aws.amazon.com/de/blogs/big-data/how-baqend-built-a-real-time-web-analytics-platform-using-amazon-kinesis-data-analytics-for-apache-flink/\"\u003eRUM tool\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAfter a few weeks of data collection, you can compare the performance between the two groups and are able to prove the significant improvements to our customers. The RUM tool also stays in place after the A/B test to monitor performance and work with our clients to achieve optimal performance in the long run.\u003c/p\u003e\n\u003cp\u003eWe also use the same methodology of A/B tests with RUM tracking to research and evaluate new features and optimizations. \u003cstrong\u003eAccess to detailed RUM data from about 300 million users per month is what enabled us to perfect the approach and achieve great performance improvements.\u003c/strong\u003e\u003c/p\u003e","noteIndex":{"id":"Iy0MoL0KnL55Br3AfTS2C","title":"Luke","desc":"","updated":1766965759366,"created":1644449449778,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"a724de3efd251cf89fe82a5860d9008b","links":[{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"life-tips","position":{"start":{"line":43,"column":5,"offset":2710},"end":{"line":43,"column":29,"offset":2734},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"life-tips","anchorHeader":"wodenokoto"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2026.articles","alias":"What I read in 2026","position":{"start":{"line":72,"column":3,"offset":4440},"end":{"line":72,"column":45,"offset":4482},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2026.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2025.articles","alias":"2025","position":{"start":{"line":73,"column":5,"offset":4487},"end":{"line":73,"column":32,"offset":4514},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2025.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2024.articles","alias":"2024","position":{"start":{"line":74,"column":5,"offset":4519},"end":{"line":74,"column":32,"offset":4546},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2024.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2023.articles","alias":"2023","position":{"start":{"line":75,"column":5,"offset":4551},"end":{"line":75,"column":32,"offset":4578},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2023.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"read.2022.articles","alias":"2022","position":{"start":{"line":76,"column":5,"offset":4583},"end":{"line":76,"column":32,"offset":4610},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"read.2022.articles"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-struggled-brag-in","position":{"start":{"line":82,"column":3,"offset":4746},"end":{"line":82,"column":39,"offset":4782},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-struggled-brag-in"}}],"anchors":{"what-i-read-in-past":{"type":"header","text":"What I read in past","value":"what-i-read-in-past","line":76,"column":0,"depth":2}},"children":["zd4mq442jike0pr0wba1u3m","6hzeqsofq67gdk88flxlkhp","778ijii93yu5uwnrwmn5zi4","g1fngdjl25nes6fs3lip602","ZbdkdApFqLdks4Moq92R9","uoc5hhki3o4py15cesddu8q","9qf7j06jtdkm6rnx9ymvwb0","5zn10cvj7ajy2gh2is5nqmg","4qo9ma0z0yu1czns6pxl7y5","ok0e729ho7o09xetujkxc0m","GR5x8HnNFEN6fU2UBSEIK","yirtnlj8q24yutcf3ss1xqy","eq0wc6t7wl2wv221yb68ro4","7x2fnv4j6gxts08qk0jguny","ettkt3iClONnxpbGwBVLl","7l4knev6v613tbuoskvmbdg","hvh5bud6yp7dc89tuh95tr9","4fvoqrplw0cweo554usbjos","f8qsfql0a9v8thpeo82udfa","1swsbrhqi9jk41v9eodyi5q","SQqYupi6EFddTerBA8RRD","hjNeNc1F2JUh0lTWanH4h","qf0l4wbrc9jgooyzexmbq5v","o7xruzrah5wzqetottecss7","z1zo2mp6ddji5p317i4x9xw","v06c2tjelh341x4resa50fh","0yqesk4rcffwgyuab5x8rfa","sy2vkbtyu671chkvgn1yt8j","ufixpmxoydiccoh59kphrib","alswadkx4wb05y1z9iwfzfv","1daut9dpw70xd0zh5a7j5p4"],"parent":null,"data":{},"body":"\nHi there 👋. I'm a Front-end developer.\n\n---\n\n- 현실은 인간의 연산으로 완전히 파악할 수 없는 복잡계. 주어진 상황과 능력으로 할 수 있는 최선의 적응은 단순함과 꾸준함.\n\n  - 파산을 면하는 선에서 여러가지를 해보고 자신에게 맞는 걸 위주로 꾸준히. 그를 위해 단순, 편안, 쾌적함이 필요.\n\n- 🥱 -\u003e 🤔💡🌱 - [On The Death of Daydreaming](https://www.afterbabel.com/p/on-the-death-of-daydreaming)\n  - boredom -\u003e easy fun -\u003e art -\u003e profit?\n\n\u003e I've often described my motivation for building software to others using imagery: I like to go find a secluded beach, build a large, magnificent sand castle, and then walk away. Will anyone notice? Probably not. Will the waves eventually destroy it? Yep. Did I still get immense satisfaction? Absolutely. - [aliasxneo](https://news.ycombinator.com/item?id=41497113)\n\n\u003e We love to see the process, not just the result. The imperfections in your work can be beautiful if they show your struggle for perfection, not a lack of care. - [ralphammer](https://ralphammer.com/is-perfection-boring/)\n\n\u003e Simplicity, even if it sacrifices some ideal functionality has better survival characteristics than the-right-thing. - [The Rise of Worse is Better](https://www.dreamsongs.com/RiseOfWorseIsBetter.html)\n\n\u003e [Roberto Blake was talking about making 100 crappy videos](https://www.youtube.com/watch?v=OnUBaQ1Sp_E) to get better over time. Putting in the reps and improving a little bit each time.\n\u003e\n\u003e Putting in the work without expecting any external reward at first (eg views, followers, likes, etc) will pay off in the long run. - [100 Scrappy Things](https://www.florin-pop.com/blog/100-scrappy-things/)\n\n\u003e Make the difficult habitual, the habitual easy, and the easy beautiful. - [Constantin S. Stanislavski](https://www.goodreads.com/quotes/7102271-make-the-difficult-habitual-the-habitual-easy-and-the-easy)\n\n\u003e A good match is a **structured** dance, where players aim to **score** while they are following well-defined **rules**. This **freedom within a structure** is what makes it fun. - [ralphammer](https://ralphammer.com/how-to-get-started/)\n\n- [Pivot Points](https://longform.asmartbear.com/pivot-points/)\n\n  - non-judgmental aspects of personality that can be strengths in some contexts and weaknesses in others\n  - Pivot Points are fixed in the short term\n\n- [Hedged Bets](https://longform.asmartbear.com/predict-the-future/#hedged-bets)\n  - trading slightly less maximum upside for predictable, net-positive outcomes.\n\n\u003e “Motivation often comes after starting, not before. Action produces momentum.”\n\u003e [When you start a new habit, it should take less than two minutes to do.](https://jamesclear.com/how-to-stop-procrastinating)\n\u003e\n\u003e - James Clear\n\n\u003e Focus is more about **not** keeping busy when you need to wait for something.  \n\u003e Eat the boredom for a minute.\n\u003e\n\u003e - [[life-tips#wodenokoto]]\n\n\u003e [4 minutes run hard enough to push heart rate to 90%, 3 minutes recover, repeat 4 times](https://news.ycombinator.com/item?id=34213181)\n\u003e\n\u003e - https://www.ntnu.edu/cerg/advice\n\u003e - [Get running with Couch to 5K](https://www.nhs.uk/live-well/exercise/running-and-aerobic-exercises/get-running-with-couch-to-5k/)\n\n\u003e [recommended routine - bodyweightfitness](https://www.reddit.com/r/bodyweightfitness/wiki/kb/recommended_routine/) - I Don't Have This Much Time!\n\u003e\n\u003e - Don't workout at all (saves anywhere from 20 to 60 minutes, but really, really, really, really, really, really, really, really, really not recommended)\n\n\u003e 도무지 읽히지 않는 책 앞에서 내가 택한 방법은 펼쳐진 페이지 앞에서 멍때리기이다. 다르게 표현하면 이렇다. 펼쳐진 두 페이지 앞에서 오래 머물기.\n\u003e\n\u003e 책을 펼쳐놓는 것으로 충분하다. 읽지 못해도 좋다. 매일 정해진 진도를 나가야 하는 학교 수업이 아니니까. 하지만 읽지 않아도 괜찮다고 해서 펼쳐두지조차 않으면 곤란하다. 가능한 한 자주 책을 펼쳐두도록 하자. 전혀 읽지 않고 멍하니 바라보고 있다가 다시 덮게 되더라도\n\u003e\n\u003e - 막막한 독서. 시로군. P.10~13\n\n\u003e I think it should be everyone's primary focus to sleep well, drink water, get outside, get active, and eat generally decently. I hate to say it, but if you're not eating a good amount of vegetables and fruit, decent protein, sleep, etc, no amount of XYZ will catch up to that detriment. - [CE02](https://news.ycombinator.com/item?id=35056071)\n\n\u003e My real battle is doing good versus doing nothing. - [Deirdre Sullivan](https://www.npr.org/2005/08/08/4785079/always-go-to-the-funeral)\n\n[Kind Engineering](https://kind.engineering/) - How To Engineer Kindness\n\n\u003e Sometimes magic is just someone spending more time on something than anyone else might reasonably expect. - [Teller](https://www.goodreads.com/quotes/6641527-sometimes-magic-is-just-someone-spending-more-time-on-something)\n\n---\n\n## What I read in past\n\n- [[What I read in 2026|read.2026.articles]]\n  - [[2025|read.2025.articles]]\n  - [[2024|read.2024.articles]]\n  - [[2023|read.2023.articles]]\n  - [[2022|read.2022.articles]]\n- 📝 [Gists](https://gist.github.com/Luke-SNAW)\n- 📜 [Journals](https://luke-snaw.github.io/Luke-SNAW__netlify-CMS.github.io/)\n\n---\n\n- [[journal.what-i-struggled-brag-in]]\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":false,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"dendronVersion":"0.115.0","enableFullHierarchyNoteTitle":false,"enablePersistentHistory":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Luke SNAW","description":"Personal knowledge space"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","siteUrl":"https://luke-snaw.github.io/","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"id1umwb0k8k7exk0589tae9"},"buildId":"wirstT2ztC8OQsbzKjhvw","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
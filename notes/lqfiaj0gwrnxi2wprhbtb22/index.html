<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="icon" href="/favicon.ico"/><title>My favourite API is a zipfile on the European Central Bank&#x27;s website</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Personal knowledge space"/><meta property="og:title" content="My favourite API is a zipfile on the European Central Bank&#x27;s website"/><meta property="og:description" content="Personal knowledge space"/><meta property="og:url" content="https://luke-snaw.github.io//notes/lqfiaj0gwrnxi2wprhbtb22/"/><meta property="og:type" content="article"/><meta property="article:published_time" content="9/19/2023"/><meta property="article:modified_time" content="9/19/2023"/><link rel="canonical" href="https://luke-snaw.github.io//notes/lqfiaj0gwrnxi2wprhbtb22/"/><meta name="next-head-count" content="14"/><link rel="preload" href="/_next/static/css/8e7b7e4bce421c0a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/8e7b7e4bce421c0a.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-3d209faeb64f2f97.js" defer=""></script><script src="/_next/static/chunks/framework-28c999baf2863c3d.js" defer=""></script><script src="/_next/static/chunks/main-104451f3d1a5c4bc.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9d8e0603730b15a3.js" defer=""></script><script src="/_next/static/chunks/935-4dee79e80b8641c6.js" defer=""></script><script src="/_next/static/chunks/6-50972def09142ee2.js" defer=""></script><script src="/_next/static/chunks/pages/notes/%5Bid%5D-78d472fa3b924116.js" defer=""></script><script src="/_next/static/vOE8u-mg___OsOsz4tjEg/_buildManifest.js" defer=""></script><script src="/_next/static/vOE8u-mg___OsOsz4tjEg/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><section class="ant-layout" style="width:100%;min-height:100%"><header class="ant-layout-header" style="position:fixed;isolation:isolate;z-index:1;width:100%;border-bottom:1px solid #d4dadf;height:64px;padding:0 24px 0 2px"><div class="ant-row ant-row-center" style="max-width:992px;justify-content:space-between;margin:0 auto"><div style="display:flex" class="ant-col ant-col-xs-20 ant-col-sm-4"></div><div class="ant-col gutter-row ant-col-xs-0 ant-col-sm-20 ant-col-md-20 ant-col-lg-19"><div class="ant-select ant-select-lg ant-select-auto-complete ant-select-single ant-select-allow-clear ant-select-show-search" style="width:100%"><div class="ant-select-selector"><span class="ant-select-selection-search"><input type="search" autoComplete="off" class="ant-select-selection-search-input" role="combobox" aria-haspopup="listbox" aria-owns="undefined_list" aria-autocomplete="list" aria-controls="undefined_list" aria-activedescendant="undefined_list_0" value=""/></span><span class="ant-select-selection-placeholder">For full text search please use the &#x27;?&#x27; prefix. e.g. ? Onboarding</span></div></div></div><div style="display:none;align-items:center;justify-content:center" class="ant-col ant-col-xs-4 ant-col-sm-4 ant-col-md-0 ant-col-lg-0"><span role="img" aria-label="menu" style="font-size:24px" tabindex="-1" class="anticon anticon-menu"><svg viewBox="64 64 896 896" focusable="false" data-icon="menu" width="1em" height="1em" fill="currentColor" aria-hidden="true"><path d="M904 160H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0 624H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8zm0-312H120c-4.4 0-8 3.6-8 8v64c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-64c0-4.4-3.6-8-8-8z"></path></svg></span></div></div></header><section class="ant-layout" style="margin-top:64px;display:flex;flex-direction:row"><div class="site-layout-sidebar" style="flex:0 0 auto;width:calc(max((100% - 992px) / 2, 0px) + 200px);min-width:200px;padding-left:calc((100% - 992px) / 2)"><aside class="ant-layout-sider ant-layout-sider-dark" style="position:fixed;overflow:auto;height:calc(100vh - 64px);background-color:transparent;flex:0 0 200px;max-width:200px;min-width:200px;width:200px"><div class="ant-layout-sider-children"></div></aside></div><main class="ant-layout-content side-layout-main" style="max-width:1200px;min-width:0;display:block"><div style="padding:0 24px"><div class="main-content" role="main"><div class="ant-row"><div class="ant-col ant-col-24"><div class="ant-row" style="margin-left:-10px;margin-right:-10px"><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-24 ant-col-md-18"><div><h1 id="my-favourite-api-is-a-zipfile-on-the-european-central-banks-website">My favourite API is a zipfile on the European Central Bank's website<a aria-hidden="true" class="anchor-heading icon-link" href="#my-favourite-api-is-a-zipfile-on-the-european-central-banks-website"></a></h1>
<blockquote>
<p><a href="https://csvbase.com/blog/5">Simple data pipeline powertools: sqlite, pandas, gnuplot and friends</a></p>
</blockquote>
<p>Why my favourite API is a zipfile on the European Central Bank's website</p>
<p>2023-09-13</p>
<p>by <a href="https://calpaterson.com/">Cal Paterson</a></p>
<p>When was the Dollar highest against the Euro?</p>
<p>Here is a small program that calculates it:</p>
<pre class="language-shell"><code class="language-shell"><span class="token function">curl</span> -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip <span class="token punctuation">\</span>
<span class="token operator">|</span> gunzip <span class="token punctuation">\</span>
<span class="token operator">|</span> sqlite3 -csv <span class="token string">':memory:'</span> <span class="token string">'.import /dev/stdin stdin'</span> <span class="token punctuation">\</span>
<span class="token string">"select Date from stdin order by USD asc limit 1;"</span>
</code></pre>
<p>The output: <code>2000-10-26</code>. (Try running it yourself.)</p>
<p>How it works:</p>
<p>The <code>curl</code> bit downloads the <a href="https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html">official historical data that the European Central Bank publishes</a> on the position of the Euro against other currencies. (The <code>-s</code> flag just removes some noise from standard error.)</p>
<p>That data comes as a zipfile, which <code>gunzip</code> will decompress.</p>
<p><code>sqlite3</code> queries the csv inside. <code>:memory</code> tells sqlite to use an in-memory file. After that, <code>.import /dev/stdin stdin</code> tells sqlite to load standard input into a table called <code>stdin</code>. The string that follows that is a SQL query.</p>
<h2 id="cleanup-in-column-42">Cleanup in column 42<a aria-hidden="true" class="anchor-heading icon-link" href="#cleanup-in-column-42"></a></h2>
<p>Although pulling out a simple max is easy, the data shape is not ideal. It's in "wide" format - a <code>Date</code> column, and then an extra column for every currency. Here's the csv header for that file:</p>
<blockquote>
<p>Date,USD,JPY,BGN,CYP,CZK,DKK,EEK,GBP,HUF,LTL,LVL,MTL,[and on, and on]</p>
</blockquote>
<p>When doing filters and aggregations, life is easier if the data is in "long" format, like this:</p>
<blockquote>
<p>Date,Currency,Rate</p>
</blockquote>
<p>Switching from wide to long is a simple operation, commonly called a "melt". Unfortunately, it's not available in SQL.</p>
<p>No matter, you can melt with pandas:</p>
<pre class="language-shell"><code class="language-shell"><span class="token function">curl</span> -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip <span class="token operator">|</span> <span class="token punctuation">\</span>
gunzip <span class="token operator">|</span> <span class="token punctuation">\</span>
python3 -c <span class="token string">'import sys, pandas as pd
pd.read_csv(sys.stdin).melt("Date").to_csv(sys.stdout, index=False)'</span>
</code></pre>
<p>There is one more problem. The file mungers at ECB have wrongly put a trailing comma at the end of every line. The makes csv parsers pick up an extra, blank column at the end. Our sqlite query didn't notice, but these commas interfere with the melt, creating a whole set of junk rows at the end:</p>
<p><a href="https://csvbase.com/blog-static/melts-are-for-melts.png">screenshot of csv file in terminal with junk at the end</a></p>
<p>The effects of that extra comma can be removed via pandas by adding one more thing to our method chain: <code>.iloc[:, :-1]</code>, which effectively says "give me all rows ("<code>:</code>") and all but the last column ("<code>:-1</code>"). So:</p>
<pre class="language-shell"><code class="language-shell"><span class="token function">curl</span> -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip <span class="token operator">|</span> <span class="token punctuation">\</span>
gunzip <span class="token operator">|</span> <span class="token punctuation">\</span>
python3 -c <span class="token string">'import sys, pandas as pd
pd.read_csv(sys.stdin).iloc\[:, :-1\].melt("Date")\
.to_csv(sys.stdout, index=False)'</span>
</code></pre>
<p>Does everyone who uses this file have to repeat this data shitwork?</p>
<p>Tragically, the answer is yes. As they say: "data janitor: nobody's dream, everyone's job".</p>
<p>In full fairness, though, the ECB foreign exchange data is probably in the top 10% of all open data releases. Usually, getting viable tabular data out of someone is a much more tortuous and involved process.</p>
<p>Some things we didn't have to do in this case: negotiate access (for example by paying money or talking to a salesman); deposit our email address/company name/job title into someone's database of qualified leads, observe any quota; authenticate (often a substantial side-quest of its own), read any API docs at all or deal with any issues more serious than basic formatting and shape.</p>
<p>So <code>eurofxref-hist.zip</code> is, relatively speaking, pretty nice actually.</p>
<p>But anyway - I'll put my cleaned up copy <a href="https://csvbase.com/calpaterson/eurofxref-hist">into a csvbase table</a> so you, dear reader, can skip the tedium and just have fun.</p>
<p>Here's how I do that:</p>
<pre class="language-shell"><code class="language-shell"><span class="token function">curl</span> -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip <span class="token operator">|</span> <span class="token punctuation">\</span>
gunzip <span class="token operator">|</span> <span class="token punctuation">\</span>
python3 -c <span class="token string">'import sys, pandas as pd
pd.read_csv(sys.stdin).iloc\[:, :-1\].melt("Date")\
.to_csv(sys.stdout, index=False)'</span> <span class="token operator">|</span> <span class="token punctuation">\</span>
<span class="token comment"># this is the new bit: \</span>
<span class="token function">curl</span> -n --upload-file - <span class="token punctuation">\</span>
<span class="token string">'https://csvbase.com/calpaterson/eurofxref-hist?public=yes'</span>
</code></pre>
<p>All I've done is add another <code>curl</code>, to HTTP PUT the csv file into csvbase. <code>--upload-file -</code> uploads from standard input to the given url (via HTTP PUT). If the table doesn't already exist in csvbase, it is created. <code>-n</code> adds my <a href="https://csvbase.com/calpaterson/eurofxref-hist/docs#authentication">credentials</a> from my <code>~/.netrc</code>. That's it. Simples.</p>
<h2 id="drawing-pretty-graphs">Drawing pretty graphs<a aria-hidden="true" class="anchor-heading icon-link" href="#drawing-pretty-graphs"></a></h2>
<p>Alright, now the data cleaning phase is over, let's do some more interesting stuff.</p>
<p>Let's graph the data:</p>
<pre class="language-shell"><code class="language-shell"><span class="token function">curl</span> -s https://csvbase.com/calpaterson/eurofxref-hist <span class="token operator">|</span> <span class="token punctuation">\</span>
<span class="token function">grep</span> USD <span class="token operator">|</span> <span class="token punctuation">\</span>
<span class="token function">cut</span> --delim<span class="token punctuation">\</span><span class="token operator">=</span>, -f <span class="token number">2,4</span> <span class="token operator">|</span> <span class="token punctuation">\</span>
gnuplot -e <span class="token string">"set datafile separator ','; set term dumb; \
plot '-' using 1:2 with lines title 'usd'"</span>
</code></pre>
<p><a href="https://csvbase.com/blog-static/dumb-term.png">a gnuplot graph in drawn in the
terminal</a></p>
<p>That's somewhat legible for over 6000 datapoints in an 80x25 character terminal. You can make out the broad trend. A reasonable <a href="https://infovis-wiki.net/wiki/Data-Ink_Ratio">data-ink ratio</a>.</p>
<p>(If you're wondering how <a href="https://csvbase.com/calpaterson/eurofxref-hist">https://csvbase.com/calpaterson/eurofxref-hist</a> returns a webpage to your browser but a csv file to curl, see an <a href="https://csvbase.com/blog/2">earlier blogpost</a>.)</p>
<p><code>gnuplot</code> is like a little mini-programming language of it's own. Here's what the above snippet does:</p>
<ul>
<li><code>set datafile separator ','</code> - says it's a csv</li>
<li><code>set term dumb</code> - draw ascii-art!</li>
<li><code>plot -</code> plot the data coming from standard input</li>
<li><code>using 1:2 with lines</code> draw lines from columns 1 and 2 (the date and the rate respectively)</li>
<li><code>title 'usd'</code> name the line</li>
</ul>
<p>You can, of course, also draw graphs to proper images:</p>
<pre class="language-shell"><code class="language-shell"><span class="token function">curl</span> -s https://csvbase.com/calpaterson/eurofxref-hist <span class="token operator">|</span> <span class="token punctuation">\</span>
<span class="token function">grep</span> USD <span class="token operator">|</span> <span class="token punctuation">\</span>
<span class="token function">cut</span> --delim<span class="token punctuation">\</span><span class="token operator">=</span>, -f <span class="token number">2,4</span> <span class="token operator">|</span> <span class="token punctuation">\</span>
gnuplot -e <span class="token string">"set datafile separator ','; set term svg; \
set output 'usd.svg'; set xdata time; set timefmt '%Y-%m-%d'; \
set format x '%Y-%m-%d'; set xtics rotate; \
plot '-' using 1:2 with lines title 'usd'"</span>
</code></pre>
<p><a href="https://csvbase.com/blog-static/usd.svg">a gnuplot graph of usd:eur</a></p>
<p>Outputting to SVG is only a bit more complicated than ascii art. In order for it look decent you need to help gnuplot understand that it's "timeseries" data - ie: that the x axis is time; give a format for that time and then tell it to rotate the markings on the x axis so that they are readable. It's a bit wordy though: let's bind it to bash function so we can reuse it:</p>
<pre><code>plot_timeseries_to_svg () {
# $1 is the first param
gnuplot -e "set datafile separator ','; set term svg; \
set output '$1.svg'; set xdata time; set timefmt '%Y-%m-%d'; \
set format x '%Y-%m-%d'; set xtics rotate; \
plot '-' using 1:2 with lines title '$1'"
}
</code></pre>
<h2 id="rolling-averages-and-new-tools">Rolling averages and new tools<a aria-hidden="true" class="anchor-heading icon-link" href="#rolling-averages-and-new-tools"></a></h2>
<p>So far, so good. But it would be nice to try out more sophisticated analyses: let's try putting a nice rolling average in so that we can see a trend line:</p>
<pre class="language-shell"><code class="language-shell"><span class="token function">curl</span> -s https://csvbase.com/calpaterson/eurofxref-hist <span class="token operator">|</span> <span class="token punctuation">\</span>
duckdb -csv -c <span class="token string">"select Date, avg(value) over \
(order by date rows between 100 preceding and current row) \
as rolling from read_csv_auto('/dev/stdin')
where variable = 'USD';"</span> <span class="token operator">|</span> <span class="token punctuation">\</span>
plot_timeseries_to_svg rolling
</code></pre>
<p><a href="https://csvbase.com/blog-static/rolling.svg">a rolling averaged gnuplot graph of usd:eur</a></p>
<p>Smooth. If you don't have <code>duckdb</code> installed, it's not hard to adapt the above for <code>sqlite3</code> (the query is the same). DuckDB is a tool I wanted to show because it's a lot like sqlite but instead is columnar (rather than row-oriented). However for me the main value is that it has a lot of easy ergonomics.</p>
<p>Here is one of them: you can load csvs into table files straight from HTTP:</p>
<pre class="language-sql"><code class="language-sql"><span class="token comment">-- it works with csvbase!</span>
<span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> eurofxref_hist <span class="token keyword">AS</span> <span class="token keyword">SELECT</span> <span class="token operator">*</span> <span class="token keyword">FROM</span>
read_csv_auto<span class="token punctuation">(</span><span class="token string">"https://csvbase.com/calpaterson/eurofxref-hist"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p><a href="https://csvbase.com/blog-static/duckdb-download.png">eurofxref-hist in duckdb</a></p>
<p>That's pretty easy, and DuckDB does a reasonable job of inferring types. There are a lot of other usability niceties too: for example, it helpfully detects your terminal size and abridges tables by default rather than flooding your terminal with an enormous resultset. It has a progress bar for big queries! It can output markdown tables! Etc!</p>
<h2 id="open-data-is-also-an-open-api">Open data is also an open API<a aria-hidden="true" class="anchor-heading icon-link" href="#open-data-is-also-an-open-api"></a></h2>
<p>A lot is possible with a zipfile of data and just the programs that are either already installed or a quick <code>brew install</code>/<code>apt install</code> away. I remember how impressed I was when I was first shown this <code>eurofxref-hist.zip</code> by an old hand from foreign exchange when I worked in a bank. It was so simple: the simplest cross-organisation data interchange protocol I had then seen (and probably since).</p>
<p>A mere zipfile with a csv in it seems so diminutive, but in fact an enormous mass of financial applications use this particular zipfile every day. I'm pretty sure that's why they've left those commas in - if they removed them now they'd break a lot of code.</p>
<p>When open data is made really easily available, it also functions double duty as an open API. After all, for the largeish fraction of APIs in which are less about calling remote functions than about exchanging data, what is the functional difference?</p>
<p>So I think the ECB's zipfile is a pretty good starting point for a data interchange format. I love the simplicity - and I've tried to keep that with csvbase.</p>
<p>In csvbase, every table has a single url, following the form:</p>
<p><code>https://csvbase.com/&#x3C;username>/&#x3C;table_name></code></p>
<p>eg</p>
<p><a href="https://csvbase.com/calpaterson/eurofxref-hist">https://csvbase.com/calpaterson/eurofxref-hist</a></p>
<p>And on each url, there are four main verbs:</p>
<p>When you <code>GET</code>: you get a csv (<a href="https://csvbase.com/blog/2">or a web page, if you're in a browser</a>).</p>
<p>When you <code>PUT</code> a new csv: you create a new table, or overwrite the existing one.</p>
<p>When you <code>POST</code> a new csv: you bulk add more rows to an existing table.</p>
<p>When you <code>DELETE</code>: that table is no more.</p>
<p>To authenticate, just use <a href="https://csvbase.com/calpaterson/eurofxref-hist/docs#authentication">HTTP Basic Auth</a>.</p>
<p>Could it be any simpler? If you can think of a way: <a href="/mailto:cal@calpaterson.com">write me an email</a>.</p>
<h2 id="notes">Notes<a aria-hidden="true" class="anchor-heading icon-link" href="#notes"></a></h2>
<p>I said above that most SQL databases don't have a "melt" operation. The ones that I know of that do are <a href="https://docs.snowflake.com/en/sql-reference/constructs/unpivot">Snowflake</a> and <a href="https://learn.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-ver16">MS SQL Server</a>. One question that SQL-knowers frequently ask is: why does anyone use R or Pandas at all when SQL already exists? A key reason is that R and Pandas are very strong on data cleanup.</p>
<p>One under-appreciated feature of bash pipelines is that they are multi-process. Each program runs independently, in it's own process. While curl is downloading data from the web, grep is filtering it, sqlite is querying it and perhaps curl is uploading it again, etc. All in parallel, which can, surprisingly, make it <a href="https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">very competitive with fancy cloud alternatives</a>.</p>
<p>Why was the Euro so weak back in 2000? It was launched, without coins or notes, in January 1999. The Euro was, initially, a sort of in-game currency for the European Union. It existed only inside banks - so there were no notes or coins for it. That all came later. So did belief - early on it didn't look like the little Euro was going to make it: so the rate against the Dollar was 0.8252. That means that in October 2000, a Dollar would buy you 1.21 Euros (to reverse exchange rates, do <code>1/rate</code>). Nowadays the Euro is much stronger: a Dollar would buy you less than 1 Euro.</p></div></div><div style="padding-left:10px;padding-right:10px" class="ant-col ant-col-xs-0 ant-col-md-6"><div><div class=""><div class="ant-anchor-wrapper dendron-toc" style="max-height:calc(100vh - 64px);z-index:1"><div class="ant-anchor"><div class="ant-anchor-ink"><span class="ant-anchor-ink-ball"></span></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#cleanup-in-column-42" title="Cleanup in column 42">Cleanup in column 42</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#drawing-pretty-graphs" title="Drawing pretty graphs">Drawing pretty graphs</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#rolling-averages-and-new-tools" title="Rolling averages and new tools">Rolling averages and new tools</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#open-data-is-also-an-open-api" title="Open data is also an open API">Open data is also an open API</a></div><div class="ant-anchor-link"><a class="ant-anchor-link-title" href="#notes" title="Notes">Notes</a></div></div></div></div></div></div></div></div></div></div></div><div class="ant-divider ant-divider-horizontal" role="separator"></div><footer class="ant-layout-footer" style="padding:0 24px 24px"></footer></main></section></section></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"note":{"id":"lqfiaj0gwrnxi2wprhbtb22","title":"My favourite API is a zipfile on the European Central Bank's website","desc":"","updated":1695099010782,"created":1695098751489,"custom":{},"fname":"dev.back-end.my-favourite-api-is-a-zipfile","type":"note","vault":{"fsPath":"vault"},"contentHash":"ebd936774408b4f98774bc6be9dbf313","links":[],"anchors":{"cleanup-in-column-42":{"type":"header","text":"Cleanup in column 42","value":"cleanup-in-column-42","line":37,"column":0,"depth":2},"drawing-pretty-graphs":{"type":"header","text":"Drawing pretty graphs","value":"drawing-pretty-graphs","line":99,"column":0,"depth":2},"rolling-averages-and-new-tools":{"type":"header","text":"Rolling averages and new tools","value":"rolling-averages-and-new-tools","line":154,"column":0,"depth":2},"open-data-is-also-an-open-api":{"type":"header","text":"Open data is also an open API","value":"open-data-is-also-an-open-api","line":183,"column":0,"depth":2},"notes":{"type":"header","text":"Notes","value":"notes","line":215,"column":0,"depth":2}},"children":[],"parent":"6w36dgapan7eummy1q6jpx1","data":{}},"body":"\u003ch1 id=\"my-favourite-api-is-a-zipfile-on-the-european-central-banks-website\"\u003eMy favourite API is a zipfile on the European Central Bank's website\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#my-favourite-api-is-a-zipfile-on-the-european-central-banks-website\"\u003e\u003c/a\u003e\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003ca href=\"https://csvbase.com/blog/5\"\u003eSimple data pipeline powertools: sqlite, pandas, gnuplot and friends\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhy my favourite API is a zipfile on the European Central Bank's website\u003c/p\u003e\n\u003cp\u003e2023-09-13\u003c/p\u003e\n\u003cp\u003eby \u003ca href=\"https://calpaterson.com/\"\u003eCal Paterson\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWhen was the Dollar highest against the Euro?\u003c/p\u003e\n\u003cp\u003eHere is a small program that calculates it:\u003c/p\u003e\n\u003cpre class=\"language-shell\"\u003e\u003ccode class=\"language-shell\"\u003e\u003cspan class=\"token function\"\u003ecurl\u003c/span\u003e -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"token operator\"\u003e|\u003c/span\u003e gunzip \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"token operator\"\u003e|\u003c/span\u003e sqlite3 -csv \u003cspan class=\"token string\"\u003e':memory:'\u003c/span\u003e \u003cspan class=\"token string\"\u003e'.import /dev/stdin stdin'\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"token string\"\u003e\"select Date from stdin order by USD asc limit 1;\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe output: \u003ccode\u003e2000-10-26\u003c/code\u003e. (Try running it yourself.)\u003c/p\u003e\n\u003cp\u003eHow it works:\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ecurl\u003c/code\u003e bit downloads the \u003ca href=\"https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html\"\u003eofficial historical data that the European Central Bank publishes\u003c/a\u003e on the position of the Euro against other currencies. (The \u003ccode\u003e-s\u003c/code\u003e flag just removes some noise from standard error.)\u003c/p\u003e\n\u003cp\u003eThat data comes as a zipfile, which \u003ccode\u003egunzip\u003c/code\u003e will decompress.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esqlite3\u003c/code\u003e queries the csv inside. \u003ccode\u003e:memory\u003c/code\u003e tells sqlite to use an in-memory file. After that, \u003ccode\u003e.import /dev/stdin stdin\u003c/code\u003e tells sqlite to load standard input into a table called \u003ccode\u003estdin\u003c/code\u003e. The string that follows that is a SQL query.\u003c/p\u003e\n\u003ch2 id=\"cleanup-in-column-42\"\u003eCleanup in column 42\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#cleanup-in-column-42\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAlthough pulling out a simple max is easy, the data shape is not ideal. It's in \"wide\" format - a \u003ccode\u003eDate\u003c/code\u003e column, and then an extra column for every currency. Here's the csv header for that file:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDate,USD,JPY,BGN,CYP,CZK,DKK,EEK,GBP,HUF,LTL,LVL,MTL,[and on, and on]\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWhen doing filters and aggregations, life is easier if the data is in \"long\" format, like this:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDate,Currency,Rate\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSwitching from wide to long is a simple operation, commonly called a \"melt\". Unfortunately, it's not available in SQL.\u003c/p\u003e\n\u003cp\u003eNo matter, you can melt with pandas:\u003c/p\u003e\n\u003cpre class=\"language-shell\"\u003e\u003ccode class=\"language-shell\"\u003e\u003cspan class=\"token function\"\u003ecurl\u003c/span\u003e -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\ngunzip \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\npython3 -c \u003cspan class=\"token string\"\u003e'import sys, pandas as pd\npd.read_csv(sys.stdin).melt(\"Date\").to_csv(sys.stdout, index=False)'\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThere is one more problem. The file mungers at ECB have wrongly put a trailing comma at the end of every line. The makes csv parsers pick up an extra, blank column at the end. Our sqlite query didn't notice, but these commas interfere with the melt, creating a whole set of junk rows at the end:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://csvbase.com/blog-static/melts-are-for-melts.png\"\u003escreenshot of csv file in terminal with junk at the end\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe effects of that extra comma can be removed via pandas by adding one more thing to our method chain: \u003ccode\u003e.iloc[:, :-1]\u003c/code\u003e, which effectively says \"give me all rows (\"\u003ccode\u003e:\u003c/code\u003e\") and all but the last column (\"\u003ccode\u003e:-1\u003c/code\u003e\"). So:\u003c/p\u003e\n\u003cpre class=\"language-shell\"\u003e\u003ccode class=\"language-shell\"\u003e\u003cspan class=\"token function\"\u003ecurl\u003c/span\u003e -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\ngunzip \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\npython3 -c \u003cspan class=\"token string\"\u003e'import sys, pandas as pd\npd.read_csv(sys.stdin).iloc\\[:, :-1\\].melt(\"Date\")\\\n.to_csv(sys.stdout, index=False)'\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDoes everyone who uses this file have to repeat this data shitwork?\u003c/p\u003e\n\u003cp\u003eTragically, the answer is yes. As they say: \"data janitor: nobody's dream, everyone's job\".\u003c/p\u003e\n\u003cp\u003eIn full fairness, though, the ECB foreign exchange data is probably in the top 10% of all open data releases. Usually, getting viable tabular data out of someone is a much more tortuous and involved process.\u003c/p\u003e\n\u003cp\u003eSome things we didn't have to do in this case: negotiate access (for example by paying money or talking to a salesman); deposit our email address/company name/job title into someone's database of qualified leads, observe any quota; authenticate (often a substantial side-quest of its own), read any API docs at all or deal with any issues more serious than basic formatting and shape.\u003c/p\u003e\n\u003cp\u003eSo \u003ccode\u003eeurofxref-hist.zip\u003c/code\u003e is, relatively speaking, pretty nice actually.\u003c/p\u003e\n\u003cp\u003eBut anyway - I'll put my cleaned up copy \u003ca href=\"https://csvbase.com/calpaterson/eurofxref-hist\"\u003einto a csvbase table\u003c/a\u003e so you, dear reader, can skip the tedium and just have fun.\u003c/p\u003e\n\u003cp\u003eHere's how I do that:\u003c/p\u003e\n\u003cpre class=\"language-shell\"\u003e\u003ccode class=\"language-shell\"\u003e\u003cspan class=\"token function\"\u003ecurl\u003c/span\u003e -s https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\ngunzip \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\npython3 -c \u003cspan class=\"token string\"\u003e'import sys, pandas as pd\npd.read_csv(sys.stdin).iloc\\[:, :-1\\].melt(\"Date\")\\\n.to_csv(sys.stdout, index=False)'\u003c/span\u003e \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"token comment\"\u003e# this is the new bit: \\\u003c/span\u003e\n\u003cspan class=\"token function\"\u003ecurl\u003c/span\u003e -n --upload-file - \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"token string\"\u003e'https://csvbase.com/calpaterson/eurofxref-hist?public=yes'\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll I've done is add another \u003ccode\u003ecurl\u003c/code\u003e, to HTTP PUT the csv file into csvbase. \u003ccode\u003e--upload-file -\u003c/code\u003e uploads from standard input to the given url (via HTTP PUT). If the table doesn't already exist in csvbase, it is created. \u003ccode\u003e-n\u003c/code\u003e adds my \u003ca href=\"https://csvbase.com/calpaterson/eurofxref-hist/docs#authentication\"\u003ecredentials\u003c/a\u003e from my \u003ccode\u003e~/.netrc\u003c/code\u003e. That's it. Simples.\u003c/p\u003e\n\u003ch2 id=\"drawing-pretty-graphs\"\u003eDrawing pretty graphs\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#drawing-pretty-graphs\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eAlright, now the data cleaning phase is over, let's do some more interesting stuff.\u003c/p\u003e\n\u003cp\u003eLet's graph the data:\u003c/p\u003e\n\u003cpre class=\"language-shell\"\u003e\u003ccode class=\"language-shell\"\u003e\u003cspan class=\"token function\"\u003ecurl\u003c/span\u003e -s https://csvbase.com/calpaterson/eurofxref-hist \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"token function\"\u003egrep\u003c/span\u003e USD \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"token function\"\u003ecut\u003c/span\u003e --delim\u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e, -f \u003cspan class=\"token number\"\u003e2,4\u003c/span\u003e \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\ngnuplot -e \u003cspan class=\"token string\"\u003e\"set datafile separator ','; set term dumb; \\\nplot '-' using 1:2 with lines title 'usd'\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://csvbase.com/blog-static/dumb-term.png\"\u003ea gnuplot graph in drawn in the\nterminal\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThat's somewhat legible for over 6000 datapoints in an 80x25 character terminal. You can make out the broad trend. A reasonable \u003ca href=\"https://infovis-wiki.net/wiki/Data-Ink_Ratio\"\u003edata-ink ratio\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e(If you're wondering how \u003ca href=\"https://csvbase.com/calpaterson/eurofxref-hist\"\u003ehttps://csvbase.com/calpaterson/eurofxref-hist\u003c/a\u003e returns a webpage to your browser but a csv file to curl, see an \u003ca href=\"https://csvbase.com/blog/2\"\u003eearlier blogpost\u003c/a\u003e.)\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egnuplot\u003c/code\u003e is like a little mini-programming language of it's own. Here's what the above snippet does:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eset datafile separator ','\u003c/code\u003e - says it's a csv\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eset term dumb\u003c/code\u003e - draw ascii-art!\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eplot -\u003c/code\u003e plot the data coming from standard input\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eusing 1:2 with lines\u003c/code\u003e draw lines from columns 1 and 2 (the date and the rate respectively)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etitle 'usd'\u003c/code\u003e name the line\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can, of course, also draw graphs to proper images:\u003c/p\u003e\n\u003cpre class=\"language-shell\"\u003e\u003ccode class=\"language-shell\"\u003e\u003cspan class=\"token function\"\u003ecurl\u003c/span\u003e -s https://csvbase.com/calpaterson/eurofxref-hist \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"token function\"\u003egrep\u003c/span\u003e USD \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\n\u003cspan class=\"token function\"\u003ecut\u003c/span\u003e --delim\u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e, -f \u003cspan class=\"token number\"\u003e2,4\u003c/span\u003e \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\ngnuplot -e \u003cspan class=\"token string\"\u003e\"set datafile separator ','; set term svg; \\\nset output 'usd.svg'; set xdata time; set timefmt '%Y-%m-%d'; \\\nset format x '%Y-%m-%d'; set xtics rotate; \\\nplot '-' using 1:2 with lines title 'usd'\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://csvbase.com/blog-static/usd.svg\"\u003ea gnuplot graph of usd:eur\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eOutputting to SVG is only a bit more complicated than ascii art. In order for it look decent you need to help gnuplot understand that it's \"timeseries\" data - ie: that the x axis is time; give a format for that time and then tell it to rotate the markings on the x axis so that they are readable. It's a bit wordy though: let's bind it to bash function so we can reuse it:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eplot_timeseries_to_svg () {\n# $1 is the first param\ngnuplot -e \"set datafile separator ','; set term svg; \\\nset output '$1.svg'; set xdata time; set timefmt '%Y-%m-%d'; \\\nset format x '%Y-%m-%d'; set xtics rotate; \\\nplot '-' using 1:2 with lines title '$1'\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"rolling-averages-and-new-tools\"\u003eRolling averages and new tools\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#rolling-averages-and-new-tools\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eSo far, so good. But it would be nice to try out more sophisticated analyses: let's try putting a nice rolling average in so that we can see a trend line:\u003c/p\u003e\n\u003cpre class=\"language-shell\"\u003e\u003ccode class=\"language-shell\"\u003e\u003cspan class=\"token function\"\u003ecurl\u003c/span\u003e -s https://csvbase.com/calpaterson/eurofxref-hist \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\nduckdb -csv -c \u003cspan class=\"token string\"\u003e\"select Date, avg(value) over \\\n(order by date rows between 100 preceding and current row) \\\nas rolling from read_csv_auto('/dev/stdin')\nwhere variable = 'USD';\"\u003c/span\u003e \u003cspan class=\"token operator\"\u003e|\u003c/span\u003e \u003cspan class=\"token punctuation\"\u003e\\\u003c/span\u003e\nplot_timeseries_to_svg rolling\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://csvbase.com/blog-static/rolling.svg\"\u003ea rolling averaged gnuplot graph of usd:eur\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSmooth. If you don't have \u003ccode\u003educkdb\u003c/code\u003e installed, it's not hard to adapt the above for \u003ccode\u003esqlite3\u003c/code\u003e (the query is the same). DuckDB is a tool I wanted to show because it's a lot like sqlite but instead is columnar (rather than row-oriented). However for me the main value is that it has a lot of easy ergonomics.\u003c/p\u003e\n\u003cp\u003eHere is one of them: you can load csvs into table files straight from HTTP:\u003c/p\u003e\n\u003cpre class=\"language-sql\"\u003e\u003ccode class=\"language-sql\"\u003e\u003cspan class=\"token comment\"\u003e-- it works with csvbase!\u003c/span\u003e\n\u003cspan class=\"token keyword\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"token keyword\"\u003eTABLE\u003c/span\u003e eurofxref_hist \u003cspan class=\"token keyword\"\u003eAS\u003c/span\u003e \u003cspan class=\"token keyword\"\u003eSELECT\u003c/span\u003e \u003cspan class=\"token operator\"\u003e*\u003c/span\u003e \u003cspan class=\"token keyword\"\u003eFROM\u003c/span\u003e\nread_csv_auto\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\"https://csvbase.com/calpaterson/eurofxref-hist\"\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://csvbase.com/blog-static/duckdb-download.png\"\u003eeurofxref-hist in duckdb\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThat's pretty easy, and DuckDB does a reasonable job of inferring types. There are a lot of other usability niceties too: for example, it helpfully detects your terminal size and abridges tables by default rather than flooding your terminal with an enormous resultset. It has a progress bar for big queries! It can output markdown tables! Etc!\u003c/p\u003e\n\u003ch2 id=\"open-data-is-also-an-open-api\"\u003eOpen data is also an open API\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#open-data-is-also-an-open-api\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eA lot is possible with a zipfile of data and just the programs that are either already installed or a quick \u003ccode\u003ebrew install\u003c/code\u003e/\u003ccode\u003eapt install\u003c/code\u003e away. I remember how impressed I was when I was first shown this \u003ccode\u003eeurofxref-hist.zip\u003c/code\u003e by an old hand from foreign exchange when I worked in a bank. It was so simple: the simplest cross-organisation data interchange protocol I had then seen (and probably since).\u003c/p\u003e\n\u003cp\u003eA mere zipfile with a csv in it seems so diminutive, but in fact an enormous mass of financial applications use this particular zipfile every day. I'm pretty sure that's why they've left those commas in - if they removed them now they'd break a lot of code.\u003c/p\u003e\n\u003cp\u003eWhen open data is made really easily available, it also functions double duty as an open API. After all, for the largeish fraction of APIs in which are less about calling remote functions than about exchanging data, what is the functional difference?\u003c/p\u003e\n\u003cp\u003eSo I think the ECB's zipfile is a pretty good starting point for a data interchange format. I love the simplicity - and I've tried to keep that with csvbase.\u003c/p\u003e\n\u003cp\u003eIn csvbase, every table has a single url, following the form:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ehttps://csvbase.com/\u0026#x3C;username\u003e/\u0026#x3C;table_name\u003e\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eeg\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://csvbase.com/calpaterson/eurofxref-hist\"\u003ehttps://csvbase.com/calpaterson/eurofxref-hist\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAnd on each url, there are four main verbs:\u003c/p\u003e\n\u003cp\u003eWhen you \u003ccode\u003eGET\u003c/code\u003e: you get a csv (\u003ca href=\"https://csvbase.com/blog/2\"\u003eor a web page, if you're in a browser\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eWhen you \u003ccode\u003ePUT\u003c/code\u003e a new csv: you create a new table, or overwrite the existing one.\u003c/p\u003e\n\u003cp\u003eWhen you \u003ccode\u003ePOST\u003c/code\u003e a new csv: you bulk add more rows to an existing table.\u003c/p\u003e\n\u003cp\u003eWhen you \u003ccode\u003eDELETE\u003c/code\u003e: that table is no more.\u003c/p\u003e\n\u003cp\u003eTo authenticate, just use \u003ca href=\"https://csvbase.com/calpaterson/eurofxref-hist/docs#authentication\"\u003eHTTP Basic Auth\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCould it be any simpler? If you can think of a way: \u003ca href=\"/mailto:cal@calpaterson.com\"\u003ewrite me an email\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"notes\"\u003eNotes\u003ca aria-hidden=\"true\" class=\"anchor-heading icon-link\" href=\"#notes\"\u003e\u003c/a\u003e\u003c/h2\u003e\n\u003cp\u003eI said above that most SQL databases don't have a \"melt\" operation. The ones that I know of that do are \u003ca href=\"https://docs.snowflake.com/en/sql-reference/constructs/unpivot\"\u003eSnowflake\u003c/a\u003e and \u003ca href=\"https://learn.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot?view=sql-server-ver16\"\u003eMS SQL Server\u003c/a\u003e. One question that SQL-knowers frequently ask is: why does anyone use R or Pandas at all when SQL already exists? A key reason is that R and Pandas are very strong on data cleanup.\u003c/p\u003e\n\u003cp\u003eOne under-appreciated feature of bash pipelines is that they are multi-process. Each program runs independently, in it's own process. While curl is downloading data from the web, grep is filtering it, sqlite is querying it and perhaps curl is uploading it again, etc. All in parallel, which can, surprisingly, make it \u003ca href=\"https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html\"\u003every competitive with fancy cloud alternatives\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhy was the Euro so weak back in 2000? It was launched, without coins or notes, in January 1999. The Euro was, initially, a sort of in-game currency for the European Union. It existed only inside banks - so there were no notes or coins for it. That all came later. So did belief - early on it didn't look like the little Euro was going to make it: so the rate against the Dollar was 0.8252. That means that in October 2000, a Dollar would buy you 1.21 Euros (to reverse exchange rates, do \u003ccode\u003e1/rate\u003c/code\u003e). Nowadays the Euro is much stronger: a Dollar would buy you less than 1 Euro.\u003c/p\u003e","noteIndex":{"id":"Iy0MoL0KnL55Br3AfTS2C","title":"Luke","desc":"","updated":1761796791487,"created":1644449449778,"custom":{"nav_order":0,"permalink":"/"},"fname":"root","type":"note","vault":{"fsPath":"vault"},"contentHash":"4e745570ca97988a0362cb939b760952","links":[{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"life-tips","position":{"start":{"line":41,"column":5,"offset":2603},"end":{"line":41,"column":29,"offset":2627},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"life-tips","anchorHeader":"wodenokoto"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-read-in.2025","alias":"What I read in 2025","position":{"start":{"line":70,"column":3,"offset":4333},"end":{"line":70,"column":54,"offset":4384},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-read-in.2025"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-read-in.2024","alias":"2024","position":{"start":{"line":71,"column":5,"offset":4389},"end":{"line":71,"column":41,"offset":4425},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-read-in.2024"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-read-in.2023","alias":"2023","position":{"start":{"line":72,"column":5,"offset":4430},"end":{"line":72,"column":41,"offset":4466},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-read-in.2023"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-read-in.2022","alias":"2022","position":{"start":{"line":73,"column":5,"offset":4471},"end":{"line":73,"column":41,"offset":4507},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-read-in.2022"}},{"type":"wiki","from":{"fname":"root","id":"Iy0MoL0KnL55Br3AfTS2C","vaultName":"vault"},"value":"journal.what-i-struggled-brag-in","position":{"start":{"line":79,"column":3,"offset":4643},"end":{"line":79,"column":39,"offset":4679},"indent":[]},"xvault":false,"sameFile":false,"to":{"fname":"journal.what-i-struggled-brag-in"}}],"anchors":{"what-i-read-in-past":{"type":"header","text":"What I read in past","value":"what-i-read-in-past","line":74,"column":0,"depth":2}},"children":["zd4mq442jike0pr0wba1u3m","6hzeqsofq67gdk88flxlkhp","778ijii93yu5uwnrwmn5zi4","g1fngdjl25nes6fs3lip602","ZbdkdApFqLdks4Moq92R9","uoc5hhki3o4py15cesddu8q","9qf7j06jtdkm6rnx9ymvwb0","5zn10cvj7ajy2gh2is5nqmg","4qo9ma0z0yu1czns6pxl7y5","ok0e729ho7o09xetujkxc0m","GR5x8HnNFEN6fU2UBSEIK","yirtnlj8q24yutcf3ss1xqy","eq0wc6t7wl2wv221yb68ro4","7x2fnv4j6gxts08qk0jguny","ettkt3iClONnxpbGwBVLl","7l4knev6v613tbuoskvmbdg","hvh5bud6yp7dc89tuh95tr9","4fvoqrplw0cweo554usbjos","f8qsfql0a9v8thpeo82udfa","1swsbrhqi9jk41v9eodyi5q","SQqYupi6EFddTerBA8RRD","hjNeNc1F2JUh0lTWanH4h","qf0l4wbrc9jgooyzexmbq5v","uur1lkol353z9vfeqb3n5bv","cd9n1czq3ursgkby985wkmm","k1sr43vwnfqztwc0s43pkcf","wfde75rhdvq2yfa2zy2q6rv","rjcmdv60jokmbw6zoq8u2ef","ujapvww8o6v3kpmlhtryq4k","pkwewou9d5e8ystswn1j2b4"],"parent":null,"data":{},"body":"\nHi there 👋. I'm a Front-end developer.\n\n---\n\n- 단순함과 꾸준함은 가장 쉬우면서도 지키기 어려운 원칙.\n\n- 🥱 -\u003e 🤔💡🌱 - [On The Death of Daydreaming](https://www.afterbabel.com/p/on-the-death-of-daydreaming)\n  - boredom -\u003e easy fun -\u003e art -\u003e profit?\n\n\u003e I've often described my motivation for building software to others using imagery: I like to go find a secluded beach, build a large, magnificent sand castle, and then walk away. Will anyone notice? Probably not. Will the waves eventually destroy it? Yep. Did I still get immense satisfaction? Absolutely. - [aliasxneo](https://news.ycombinator.com/item?id=41497113)\n\n\u003e We love to see the process, not just the result. The imperfections in your work can be beautiful if they show your struggle for perfection, not a lack of care. - [ralphammer](https://ralphammer.com/is-perfection-boring/)\n\n\u003e Simplicity, even if it sacrifices some ideal functionality has better survival characteristics than the-right-thing. - [The Rise of Worse is Better](https://www.dreamsongs.com/RiseOfWorseIsBetter.html)\n\n\u003e [Roberto Blake was talking about making 100 crappy videos](https://www.youtube.com/watch?v=OnUBaQ1Sp_E) to get better over time. Putting in the reps and improving a little bit each time.\n\u003e\n\u003e Putting in the work without expecting any external reward at first (eg views, followers, likes, etc) will pay off in the long run. - [100 Scrappy Things](https://www.florin-pop.com/blog/100-scrappy-things/)\n\n\u003e Make the difficult habitual, the habitual easy, and the easy beautiful. - [Constantin S. Stanislavski](https://www.goodreads.com/quotes/7102271-make-the-difficult-habitual-the-habitual-easy-and-the-easy)\n\n\u003e A good match is a **structured** dance, where players aim to **score** while they are following well-defined **rules**. This **freedom within a structure** is what makes it fun. - [ralphammer](https://ralphammer.com/how-to-get-started/)\n\n- [Pivot Points](https://longform.asmartbear.com/pivot-points/)\n\n  - non-judgmental aspects of personality that can be strengths in some contexts and weaknesses in others\n  - Pivot Points are fixed in the short term\n\n- [Hedged Bets](https://longform.asmartbear.com/predict-the-future/#hedged-bets)\n  - trading slightly less maximum upside for predictable, net-positive outcomes.\n\n\u003e “Motivation often comes after starting, not before. Action produces momentum.”\n\u003e [When you start a new habit, it should take less than two minutes to do.](https://jamesclear.com/how-to-stop-procrastinating)\n\u003e\n\u003e - James Clear\n\n\u003e Focus is more about **not** keeping busy when you need to wait for something.  \n\u003e Eat the boredom for a minute.\n\u003e\n\u003e - [[life-tips#wodenokoto]]\n\n\u003e [4 minutes run hard enough to push heart rate to 90%, 3 minutes recover, repeat 4 times](https://news.ycombinator.com/item?id=34213181)\n\u003e\n\u003e - https://www.ntnu.edu/cerg/advice\n\u003e - [Get running with Couch to 5K](https://www.nhs.uk/live-well/exercise/running-and-aerobic-exercises/get-running-with-couch-to-5k/)\n\n\u003e [recommended routine - bodyweightfitness](https://www.reddit.com/r/bodyweightfitness/wiki/kb/recommended_routine/) - I Don't Have This Much Time!\n\u003e\n\u003e - Don't workout at all (saves anywhere from 20 to 60 minutes, but really, really, really, really, really, really, really, really, really not recommended)\n\n\u003e 도무지 읽히지 않는 책 앞에서 내가 택한 방법은 펼쳐진 페이지 앞에서 멍때리기이다. 다르게 표현하면 이렇다. 펼쳐진 두 페이지 앞에서 오래 머물기.\n\u003e\n\u003e 책을 펼쳐놓는 것으로 충분하다. 읽지 못해도 좋다. 매일 정해진 진도를 나가야 하는 학교 수업이 아니니까. 하지만 읽지 않아도 괜찮다고 해서 펼쳐두지조차 않으면 곤란하다. 가능한 한 자주 책을 펼쳐두도록 하자. 전혀 읽지 않고 멍하니 바라보고 있다가 다시 덮게 되더라도\n\u003e\n\u003e - 막막한 독서. 시로군. P.10~13\n\n\u003e I think it should be everyone's primary focus to sleep well, drink water, get outside, get active, and eat generally decently. I hate to say it, but if you're not eating a good amount of vegetables and fruit, decent protein, sleep, etc, no amount of XYZ will catch up to that detriment. - [CE02](https://news.ycombinator.com/item?id=35056071)\n\n\u003e My real battle is doing good versus doing nothing. - [Deirdre Sullivan](https://www.npr.org/2005/08/08/4785079/always-go-to-the-funeral)\n\n[Kind Engineering](https://kind.engineering/) - How To Engineer Kindness\n\n\u003e Sometimes magic is just someone spending more time on something than anyone else might reasonably expect. - [Teller](https://www.goodreads.com/quotes/6641527-sometimes-magic-is-just-someone-spending-more-time-on-something)\n\n---\n\n## What I read in past\n\n- [[What I read in 2025|journal.what-i-read-in.2025]]\n  - [[2024|journal.what-i-read-in.2024]]\n  - [[2023|journal.what-i-read-in.2023]]\n  - [[2022|journal.what-i-read-in.2022]]\n- 📝 [Gists](https://gist.github.com/Luke-SNAW)\n- 📜 [Journals](https://luke-snaw.github.io/Luke-SNAW__netlify-CMS.github.io/)\n\n---\n\n- [[journal.what-i-struggled-brag-in]]\n"},"collectionChildren":null,"customHeadContent":null,"config":{"version":5,"dev":{"enablePreviewV2":true},"commands":{"lookup":{"note":{"selectionMode":"extract","confirmVaultOnCreate":true,"vaultSelectionModeOnCreate":"smart","leaveTrace":false,"bubbleUpCreateNew":true,"fuzzThreshold":0.2}},"randomNote":{},"insertNoteLink":{"aliasMode":"none","enableMultiSelect":false},"insertNoteIndex":{"enableMarker":false},"copyNoteLink":{"aliasMode":"title"},"templateHierarchy":"template"},"workspace":{"vaults":[{"fsPath":"vault"}],"journal":{"dailyDomain":"daily","name":"journal","dateFormat":"y.MM.dd","addBehavior":"childOfDomain"},"scratch":{"name":"scratch","dateFormat":"y.MM.dd.HHmmss","addBehavior":"asOwnDomain"},"task":{"name":"","dateFormat":"","addBehavior":"childOfCurrent","statusSymbols":{"":" ","wip":"w","done":"x","assigned":"a","moved":"m","blocked":"b","delegated":"l","dropped":"d","pending":"y"},"prioritySymbols":{"H":"high","M":"medium","L":"low"},"todoIntegration":false,"createTaskSelectionType":"selection2link","taskCompleteStatus":["done","x"]},"graph":{"zoomSpeed":1,"createStub":false},"enableAutoCreateOnDefinition":false,"enableXVaultWikiLink":false,"enableRemoteVaultInit":true,"enableUserTags":false,"enableHashTags":true,"workspaceVaultSyncMode":"noCommit","enableAutoFoldFrontmatter":false,"enableEditorDecorations":true,"maxPreviewsCached":10,"maxNoteLength":204800,"dendronVersion":"0.115.0","enableFullHierarchyNoteTitle":false,"enablePersistentHistory":false},"preview":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"automaticallyShowPreview":false,"enableFrontmatterTags":true,"enableHashesForFMTags":false},"publishing":{"enableFMTitle":true,"enableNoteTitleForLink":true,"enablePrettyRefs":true,"enableKatex":true,"copyAssets":true,"siteHierarchies":["root"],"writeStubs":false,"siteRootDir":"docs","seo":{"title":"Luke SNAW","description":"Personal knowledge space"},"github":{"enableEditLink":false,"editLinkText":"Edit this page on GitHub","editBranch":"main","editViewMode":"tree"},"enableSiteLastModified":true,"enableFrontmatterTags":true,"enableHashesForFMTags":false,"enableRandomlyColoredTags":true,"enableTaskNotes":true,"enablePrettyLinks":true,"searchMode":"lookup","siteUrl":"https://luke-snaw.github.io/","duplicateNoteBehavior":{"action":"useVault","payload":["vault"]},"siteFaviconPath":"favicon.ico","siteIndex":"root"}}},"__N_SSG":true},"page":"/notes/[id]","query":{"id":"lqfiaj0gwrnxi2wprhbtb22"},"buildId":"vOE8u-mg___OsOsz4tjEg","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
<h1 id="almost-every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup">(Almost) Every infrastructure decision I endorse or regret after 4 years running infrastructure at a startup<a aria-hidden="true" class="anchor-heading icon-link" href="#almost-every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup"></a></h1>
<blockquote>
<p><a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/">https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/</a></p>
</blockquote>
<p>I’ve led infrastructure at a <a href="https://cresta.com/careers/">startup</a> for the past 4 years that has had to scale quickly. From the beginning I made some core decisions that the company has had to stick to, for better or worse, these past four years. This post will list some of the major decisions made and if I endorse them for your startup, or if I regret them and advise you to pick something else.</p>
<h2 id="aws-link-to-heading">AWS <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#aws">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#aws-link-to-heading"></a></h2>
<h3 id="picking-aws-over-google-cloud-link-to-heading">Picking AWS over Google Cloud <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#picking-aws-over-google-cloud">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#picking-aws-over-google-cloud-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Early on, we were using both GCP and AWS. During that time, I had no idea who my “account manager” was for Google Cloud, while at the same time I had regular cadence meetings with our AWS account manager. There is a feel that Google lives on robots and automation, while Amazon lives with a customer focus. This support has helped us when evaluating new AWS services. Besides support, AWS has done a great job around stability and minimizing backwards incompatible API changes.</p>
<p>There was a time when Google Cloud was the choice for Kubernetes clusters, especially when there was ambiguity around if AWS would invest in EKS over <a href="https://aws.amazon.com/ecs/">ECS</a>. Now though, with all the extra Kubernetes integrations around AWS services (external-dns, external-secrets, etc), this is not much of any issue anymore.</p>
<h3 id="eks-link-to-heading"><a href="https://aws.amazon.com/eks/">EKS</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#ekshttpsawsamazoncomeks">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#eks-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Unless you’re penny-pinching (and your time is free), there’s no reason to run your own control plane rather than use EKS. The main advantage of using an alternative in AWS, like ECS, is the deep integration into AWS services. Luckily, Kubernetes has caught up in many ways: for example, using external-dns to integrate with Route53.</p>
<h3 id="eks-managed-addons-link-to-heading">EKS <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html">managed addons</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#eks-managed-addonshttpsdocsawsamazoncomekslatestuserguideeks-add-onshtml">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#eks-managed-addons-link-to-heading"></a></h3>
<p>🟧 <em>Regret</em></p>
<p>We started with EKS managed addons because I thought it was the “right” way to use EKS. Unfortunately, we always ran into a situation where we needed to customize the installation itself. Maybe the CPU requests, the image tag, or some configmap. We’ve since switched to using helm charts for what were add-ons and things are running much better with promotions that fit similar to our existing GitOps pipelines.</p>
<h3 id="rds-link-to-heading"><a href="https://aws.amazon.com/rds/">RDS</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#rdshttpsawsamazoncomrds">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#rds-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Data is the most critical part of your infrastructure. You lose your network: that’s downtime. You lose your data: that’s a company ending event. The markup cost of using RDS (or any managed database) is worth it.</p>
<h3 id="redis-elasticache-link-to-heading"><a href="https://aws.amazon.com/elasticache/redis/">Redis ElastiCache</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#redis-elasticachehttpsawsamazoncomelasticacheredis">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#redis-elasticache-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Redis has worked very well as a cache and general use product. It’s fast, the API is simple and well documented, and the implementation is battle tested. Unlike other cache options, like Memcached, Redis has a lot of features that make it useful for more than just caching. It’s a great swiss army knife of “do fast data thing”.</p>
<p>Part of me is unsure what the state of Redis is for Cloud Providers, but I feel it’s so widely used by AWS customers that AWS will continue to support it well.</p>
<h3 id="ecr-link-to-heading"><a href="https://aws.amazon.com/ecr/">ECR</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#ecrhttpsawsamazoncomecr">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#ecr-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>We originally hosted on <a href="https://quay.io/">quay.io</a>. It was a hot mess of stability problems. Since moving to ECR, things have been much more stable. The deeper permission integrations with EKS nodes or dev servers has also been a big plus.</p>
<h3 id="aws-vpn-link-to-heading"><a href="https://aws.amazon.com/vpn/">AWS VPN</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#aws-vpnhttpsawsamazoncomvpn">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#aws-vpn-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>There are Zero Trust VPN alternatives from companies like CloudFlare. I’m sure these products work well, but a VPN is just so dead simple to setup and understand (“simplicity is preferable” is my mantra). We use Okta to manage our VPN access and it’s been a great experience.</p>
<h3 id="aws-premium-support-link-to-heading">AWS <a href="https://aws.amazon.com/premiumsupport/">premium support</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#aws-premium-supporthttpsawsamazoncompremiumsupport">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#aws-premium-support-link-to-heading"></a></h3>
<p>🟧 <em>Regret</em></p>
<p>It’s super expensive: almost (if not more) than the cost of another engineer. I think if we had very little in house knowledge of AWS, it would be worth it.</p>
<h3 id="control-tower-account-factory-for-terraform-link-to-heading"><a href="https://docs.aws.amazon.com/controltower/latest/userguide/aft-overview.html">Control Tower Account Factory for Terraform</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#control-tower-account-factory-for-terraformhttpsdocsawsamazoncomcontroltowerlatestuserguideaft-overviewhtml">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#control-tower-account-factory-for-terraform-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Before integrating AFT, using control tower was a pain mostly because it was very difficult to automate. We’ve since integrated AFT into our stack and spinning up accounts has worked well since. Another thing AFT makes easier is standardizing tags for our accounts. For example, our production accounts have a tag that we can then use to make peering decisions. Tags work better than organizations for us because the decision of “what properties describe this account” isn’t always a tree structure.</p>
<h2 id="process-link-to-heading">Process <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#process">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#process-link-to-heading"></a></h2>
<h3 id="automating-post-mortem-process-with-a-slack-bot-link-to-heading">Automating post-mortem process with a slack bot <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#automating-post-mortem-process-with-a-slack-bot">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#automating-post-mortem-process-with-a-slack-bot-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Everyone is busy. It can feel like you’re the “bad guy” reminding people to fill out the post-mortem. Making a robot be the bad guy had been great. It streamlines the process by nudging people to follow the SEV and post-mortem procedure.</p>
<p>It doesn’t have to be too complex to start. Just the basics of “It’s been an hour of no messages. Someone post an update” or “It’s been a day with no calendar invite. Someone schedule the post-mortem meeting” can go a long ways.</p>
<h2 id="using-pager-dutys-incident-templates-link-to-heading">Using pager duty’s incident <a href="https://response.pagerduty.com/after/post_mortem_template/">templates</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#using-pager-dutys-incident-templateshttpsresponsepagerdutycomafterpost_mortem_template">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#using-pager-dutys-incident-templates-link-to-heading"></a></h2>
<p>🟩 <em>Endorse</em></p>
<p>Why reinvent the wheel? PagerDuty publishes a template of what to do during an incident. We’ve customized it a bit, which is where the flexibility of Notion comes in handy, but it’s been a great starting point.</p>
<h3 id="reviewing-pager-duty-tickets-on-a-regular-cadence-link-to-heading">Reviewing pager duty tickets on a regular cadence <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#reviewing-pager-duty-tickets-on-a-regular-cadence">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#reviewing-pager-duty-tickets-on-a-regular-cadence-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Alerting for a company goes like this:</p>
<ol>
<li>There are no alerts at all. We need alerts.</li>
<li>We have alerts. There are too many alerts, so we ignore them.</li>
<li>We’ve prioritized the alerts. Now only the critical ones wake me up.</li>
<li>We ignore the non-critical alerts.</li>
</ol>
<p>We have a two tiered alerting setup: critical and non-critical. Critical alerts wake people up. Non-critical alerts are expected to ping the on-call async (email). The problem is that non-critical alerts are often ignored. To resolve this, we have regular (usually every 2 weeks) PagerDuty review meetings where we go over all our alerts. For the critical alerts, we discuss if it should stay critical. Then, we iterate the non-critical alerts (usually picking a few each meeting) and discuss what we can do to clear those out as well (usually tweaking the threshold or creating some automation).</p>
<h3 id="monthly-cost-tracking-meetings-link-to-heading">Monthly cost tracking meetings <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#monthly-cost-tracking-meetings">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#monthly-cost-tracking-meetings-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Early on, I set up a monthly meeting to go over all of our SaaS cost (AWS, DataDog, etc). Previously, this was just something reviewed from a finance perspective, but it’s hard for them to answer general questions around “does this cost number seem right”. During these meetings, usually attended by both finance and engineering, we go over every software related bill we get and do a gut check of “does this cost sound right”. We dive into the numbers of each of the high bills and try to break them down.</p>
<p>For example, with AWS we group items by tag and separate them by account. These two dimensions, combined with the general service name (EC2, RDS, etc) gives us a good idea of where the major cost drivers are. Some things we do with this data are go deeper into spot instance usage or which accounts contribute to networking costs the most. But don’t stop at just AWS: go into all the major spend sinks your company has.</p>
<h3 id="manging-post-mortems-in-datadog-or-pager-duty-link-to-heading">Manging post mortems in datadog or pager duty <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#manging-post-mortems-in-datadog-or-pager-duty">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#manging-post-mortems-in-datadog-or-pager-duty-link-to-heading"></a></h3>
<p>🟥 <em>Regret</em></p>
<p>Everyone should do post-mortems. Both <a href="https://www.datadoghq.com/product/incident-management/">DataDog</a> and PagerDuty have integrations to manage writing post-mortems and we tried each. Unfortunately, they both make it hard to customize the post-mortem process. Given how powerful wiki-ish tools like Notion are, I think it’s better to use a tool like that to manage post-mortems.</p>
<h3 id="not-using-function-as-a-servicefaas-more-link-to-heading">Not using Function as a Service(FaaS) more <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#not-using-function-as-a-servicefaas-more">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#not-using-function-as-a-servicefaas-more-link-to-heading"></a></h3>
<p>🟥 <em>Regret</em></p>
<p>There are no great FaaS options for running GPU workloads, which is why we could never go fully FaaS. However, many CPU workloads could be FaaS (<a href="https://aws.amazon.com/lambda/">lambda</a>, etc). The biggest counter-point people bring up is the cost. They’ll say something like “This EC2 instance type running 24/7 at full load is way less expensive than a Lambda running”. This is true, but it’s also a false comparison. Nobody runs a service at 100% CPU utilization and moves on with their life. It’s always on some scaler that says “Never reach 100%. At 70% scale up another”. And it’s always unclear when to scale back down, instead it’s a heuristic of “If we’ve been at 10% for 10 minutes, scale down”. Then, people assume spot instances when they aren’t always on market.</p>
<p>Another hidden benefit of Lambda is that it’s very easy to track costs with high accuracy. When deploying services in Kubernetes, cost can get hidden behind other per node objects or other services running on the same node.</p>
<h3 id="gitops-link-to-heading"><a href="https://www.gitops.tech/">GitOps</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#gitopshttpswwwgitopstech">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#gitops-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>GitOps has so far scaled pretty well and we use it for many parts of our infrastructure: services, terraform, and config to name a few. The main downside is that pipeline oriented workflows give a clear picture of “here is the box that means you did a commit and here are arrows that go from that box to the end of the pipeline”. With GitOps we’ve had to invest in tooling to help people answer questions like “I did a commit: why isn’t it deployed yet”.</p>
<p>Even still, the flexibility of GitOps has been a huge win and I strongly recommend it for your company.</p>
<h3 id="prioritizing-team-efficiency-over-external-demands-link-to-heading">Prioritizing team efficiency over external demands <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#prioritizing-team-efficiency-over-external-demands">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#prioritizing-team-efficiency-over-external-demands-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Most likely, your company is not selling the infrastructure itself, but another product. This puts pressure on the team to deliver features and not scale your own workload. But just like airplanes tell you to put your own mask on first, you need to make sure your team is efficient. With rare exception, I have never regretted prioritizing taking time to write some automation or documentation.</p>
<h3 id="multiple-applications-sharing-a-database-link-to-heading">Multiple applications sharing a database <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#multiple-applications-sharing-a-database">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#multiple-applications-sharing-a-database-link-to-heading"></a></h3>
<p>🟥 <em>Regret</em></p>
<p>Like most tech debt, we didn’t <em>make</em> this decision, we just did not <em>not</em> make this decision. Eventually, someone wants the product to do a new thing and makes a new table. This feels good because there are now foreign keys between the two tables. But since <em>everything</em> is owned by <em>someone</em> and that <em>someone</em> is a row in a table, you’ve got foreign keys between all objects in the entire stack.</p>
<p>Since the database is used by <em>everyone</em>, it becomes cared for by <em>no one</em>. Startups don’t have the luxury of a DBA, and everything owned by <em>no one</em> is owned by <em>infrastructure</em> eventually.</p>
<p>The biggest problem with a shared database are:</p>
<ul>
<li>Crud accumulates in the database, and it’s unclear if it can be deleted.</li>
<li>When there are performance issues, infrastructure (without deep product knowledge) has to debug the database and figure out who to redirect to</li>
<li>Database users can push bad code that does bad things to the database. These bad things may PagerDuty alert the infrastructure team (since they own the database). It feels bad to wake up one team for another team’s issue. With application owned databases, the application team is the first responder.</li>
</ul>
<p>All that said, I’m not against stacks that want to share a single database either. Just be aware of the tradeoffs above and have a good story for how you’ll manage them.</p>
<h2 id="saas-link-to-heading">SaaS <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#saas">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#saas-link-to-heading"></a></h2>
<h3 id="not-adopting-an-identity-platform-early-on-link-to-heading">Not adopting an identity platform early on <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#not-adopting-an-identity-platform-early-on">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#not-adopting-an-identity-platform-early-on-link-to-heading"></a></h3>
<p>🟥 <em>Regret</em></p>
<p>I stuck with Google Workspace at the start, using it to create groups for employees as a way to assign permissions. It just isn’t flexible enough. In retrospect, I wish we had picked up <a href="https://www.okta.com/">Okta</a> much sooner. It’s worked very well, has integrations for almost everything, and solves a lot of compliance/security aspects. Just lean into an identity solution early on and only accept SaaS vendors that integrate with it.</p>
<h3 id="notion-link-to-heading"><a href="https://www.notion.so/">Notion</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#notionhttpswwwnotionso">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#notion-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Every company needs a place to put documentation. Notion has been a great choice and worked much easier than things I’ve used in the past (Wikis, Google Docs, Confluence, etc). Their Database concept for page organization has also allowed me to create pretty sophisticated organizations of pages.</p>
<h3 id="slack-link-to-heading"><a href="https://slack.com/">Slack</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#slackhttpsslackcom">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#slack-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Thank god I don’t have to use HipChat anymore. Slack is great as a default communication tool, but to reduce stress and noise I recommend:</p>
<ul>
<li>Using threads to condense communication</li>
<li>Communicating expectations that people may not respond quickly to messages</li>
<li>Discourage private messages and encourage public channels.</li>
</ul>
<h3 id="moving-off-jira-onto-linear-link-to-heading">Moving off <a href="https://www.atlassian.com/software/jira">JIRA</a> onto <a href="https://linear.app/">linear</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#moving-off-jirahttpswwwatlassiancomsoftwarejira-onto-linearhttpslinearapp">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#moving-off-jira-onto-linear-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Not even close. JIRA is so bloated I’m worried running it in an AI company it would just turn fully sentient. When I’m using Linear, I will often think “I wonder if I can do X” and then I’ll try and I can!</p>
<h3 id="not-using-terraform-cloud-link-to-heading">Not using <a href="https://www.terraform.io/cloud">Terraform Cloud</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#not-using-terraform-cloudhttpswwwterraformiocloud">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#not-using-terraform-cloud-link-to-heading"></a></h3>
<p>🟩 <em>No Regrets</em></p>
<p>Early on, I tried to migrate our terraform to Terraform Cloud. The biggest downside was that I couldn’t justify the cost. I’ve since moved us to <a href="https://www.runatlantis.io/">Atlantis</a>, and it has worked well enough. Where atlantis falls short, we’ve written a bit of automation in our CI/CD pipelines to make up for it.</p>
<h3 id="github-actions-for-cicd-link-to-heading"><a href="https://docs.github.com/en/actions">GitHub actions</a> for CI/CD <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#github-actionshttpsdocsgithubcomenactions-for-cicd">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#github-actions-for-cicd-link-to-heading"></a></h3>
<p>🟧 <em>Endorse-ish</em></p>
<p>We, like most companies, host our code on GitHub. While originally using CircleCI, we’ve switched to Github actions for CI/CD. The marketplace of actions available to use for your workflows is large and the syntax is easy to read. The main downside of Github actions is their support for self-hosted workflows is very limited. We’re using EKS and <a href="https://github.com/actions/actions-runner-controller">actions-runner-controller</a> for our self-hosted runners hosted in EKS, but the integration is often buggy (but nothing we cannot work around). I hope GitHub takes Kuberentes self-hosting more seriously in the future.</p>
<h3 id="datadog-link-to-heading"><a href="https://www.datadoghq.com/">Datadog</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#datadoghttpswwwdatadoghqcom">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#datadog-link-to-heading"></a></h3>
<p>🟥 <em>Regret</em></p>
<p>Datadog is a great product, but it’s expensive. More than just expensive, I’m worried their cost model is especially bad for Kubernetes clusters and for AI companies. Kubernetes clusters are most cost-effective when you can rapidly spin up and down many nodes, as well as use spot instances. Datadog’s pricing model is based on the number of instances you have and that means even if we have no more than 10 instances up at once, if we spin up and down 20 instances in that hour, we pay for 20 instances. Similarly, AI companies tend to use GPUs heavily. While a CPU node could have dozens of services running at once, spreading the per node Datadog cost between many use cases, a GPU node is likely to have only one service using it, making the per <strong>service</strong> Datadog cost much higher.</p>
<h3 id="pagerduty-link-to-heading"><a href="https://www.pagerduty.com/">Pagerduty</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#pagerdutyhttpswwwpagerdutycom">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#pagerduty-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Pagerduty is a great product and well priced. We’ve never regretted picking it.</p>
<h2 id="software-link-to-heading">Software <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#software">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#software-link-to-heading"></a></h2>
<h3 id="schema-migration-by-diff-link-to-heading">Schema migration by Diff <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#schema-migration-by-diff">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#schema-migration-by-diff-link-to-heading"></a></h3>
<p>🟧 <em>Endorse-ish</em></p>
<p>Schema management is hard no matter how you do it, mostly because of how scary it is. Data is important and a bad schema migration can delete data. Of all the scary ways to solve this hard problem, I’ve been very happy with the idea of checking in the entire schema into git and then using a <a href="https://michaelsogos.github.io/pg-diff/">tool</a> to generate the SQL to sync the database to the schema.</p>
<h2 id="ubuntu-for-dev-servers-link-to-heading">Ubuntu for dev servers <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#ubuntu-for-dev-servers">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#ubuntu-for-dev-servers-link-to-heading"></a></h2>
<p>🟩 <em>Endorse</em></p>
<p>Originally I tried making the dev servers the same base OS that our Kubernetes nodes ran on, thinking this would make the development environment closer to prod. In retrospect, the effort isn’t worth it. I’m happy we are sticking with Ubuntu for development servers. It’s a well-supported OS and has most of the packages we need.</p>
<h3 id="appsmith-link-to-heading"><a href="https://www.appsmith.com/">AppSmith</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#appsmithhttpswwwappsmithcom">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#appsmith-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>We frequently need to automate some process for an internal engineer: restart/promote/diagnose/etc. It’s easy enough for us to make APIs to solve these problems, but it’s a bit annoying debugging someone’s specific installation of a CLI/os/dependencies/etc. Being able to make a simple UI for engineers to interact with our scripts is very useful.</p>
<p>We self-host our AppSmith. It works … well enough. Of course there are things we would change, but it is enough for the “free” price point. I originally explored deeper integration with <a href="https://retool.com/">retool</a>, but I couldn’t justify the price point for what, at the time, was just a few integrations.</p>
<h3 id="helm-link-to-heading"><a href="https://helm.sh/">helm</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#helmhttpshelmsh">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#helm-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Helm v2 got a bad reputation (for good reason), but helm v3 has worked … well enough. There are still issues with deploying CRDs and educating developers on why their helm chart did not deploy correctly. Overall, however, helm works well enough as a way to package and deploy versioned Kubernetes objetcs and the Go templating language is difficult to debug, but powerful.</p>
<h3 id="helm-charts-in-ecroci-link-to-heading">helm charts in ECR(oci) <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#helm-charts-in-ecroci">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#helm-charts-in-ecroci-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Originally our helm charts were hosted inside S3 and downloaded with a plugin. The main downsides were needing to install a custom helm plugin and manually managing lifecycles. We’ve since switched to OCI stored helm charts and haven’t had any issues with this setup.</p>
<h3 id="bazel-link-to-heading"><a href="https://bazel.build/">bazel</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#bazelhttpsbazelbuild">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#bazel-link-to-heading"></a></h3>
<p>🟧 <em>Unsure</em></p>
<p>To be fair, a lot of smart people like bazel, so I’m sure it’s not a <em>bad</em> choice to make.</p>
<p>When deploying Go services, bazel personally feels like overkill. I think Bazel is a great choice if your last company used bazel, and you feel home sick. Otherwise, we have a build system that only a few engineers can dive deeply into, compared to GitHub Actions, where it seems everyone knows how to get their hands dirty.</p>
<h3 id="not-using-open-telemetry-early-link-to-heading">Not using open <a href="https://opentelemetry.io/">telemetry</a> early <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#not-using-open-telemetryhttpsopentelemetryio-early">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#not-using-open-telemetry-early-link-to-heading"></a></h3>
<p>🟥 <em>Regret</em></p>
<p>We started off sending metrics directly to DataDog using DataDog’s API. This has made it very hard to rip them out.</p>
<p>Open telemetry wasn’t as mature 4 years ago, but it’s gotten much better. I think the metrics telemetry is still a bit immature, but the tracing is great. I recommend using it from the start for any company.</p>
<h3 id="picking-renovatebot-over-dependabot-link-to-heading">Picking <a href="https://docs.renovatebot.com/">renovatebot</a> over <a href="https://github.com/dependabot">dependabot</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#picking-renovatebothttpsdocsrenovatebotcom-over-dependabothttpsgithubcomdependabot">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#picking-renovatebot-over-dependabot-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>I honestly wish we had thought about “keep your dependencies up to date” sooner. When you wait on this too long, you end up with versions so old the upgrade process is long and inevitably buggy. Renovatebot has worked well with the flexibility to customize it to your needs. The biggest, and it’s pretty big, downside is that it’s VERY complicated to setup and debug. I guess it’s the best of all the bad options.</p>
<h3 id="kubernetes-link-to-heading"><a href="https://kubernetes.io/">Kubernetes</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#kuberneteshttpskubernetesio">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#kubernetes-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>You need <strong>something</strong> to host your long running services. Kuberentes is a popular choice and it’s worked well for us. The Kubernetes community has done a great job integrating AWS services (like load balancers, DNS, etc) into the Kubernetes ecosystem. The biggest downside with any flexible system is that there are a lot of ways to use it, and any system with a lot of ways to use has a lot of ways to use wrong.</p>
<blockquote>
<p>any system with a lot of ways to use has a lot of ways to use wrong</p>
<ul>
<li>Jack Lindamood</li>
</ul>
</blockquote>
<h3 id="buying-our-own-ips-link-to-heading">Buying our own IPs <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#buying-our-own-ips">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#buying-our-own-ips-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>If you work with external partners, you’ll frequently need to publish a whitelist of your IPs for them. Unfortunately, you may later develop more systems that need their own IPs. Buying your own IP block is a great way to avoid this by giving the external partner a larger CIDR block to whitelist.</p>
<h3 id="picking-flux-for-k8s-gitops-link-to-heading">Picking <a href="https://fluxcd.io/">Flux</a> for k8s GitOps <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#picking-fluxhttpsfluxcdio-for-k8s-gitops">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#picking-flux-for-k8s-gitops-link-to-heading"></a></h3>
<p>🟩 <em>No Regrets</em></p>
<p>An early GitOps choice for Kubernetes was to decide between ArgoCD and Flux: I went with Flux (v1 at the time). It’s worked very well. We’re currently using Flux 2. The only downside is we’ve had to make our own tooling to help people understand the state of their deployments.</p>
<p>I hear great things about ArgoCD, so I’m sure if you picked that you’re also safe.</p>
<h3 id="karpenter-for-node-management-link-to-heading"><a href="https://karpenter.sh/">Karpenter</a> for node management <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#karpenterhttpskarpentersh-for-node-management">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#karpenter-for-node-management-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>If you’re using EKS (and not fully on Fargate), you should be using Karpenter. 100% full stop. We’ve used other autoscalers, including the default Kubernetes autoscaler and <a href="https://spot.io/">SpotInst</a>. Between them all, Karpenter has been the most reliable and the most cost-effective.</p>
<h3 id="using-sealedsecrets-to-manage-k8s-secrets-link-to-heading">Using <a href="https://github.com/bitnami-labs/sealed-secrets">SealedSecrets</a> to manage k8s secrets <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#using-sealedsecretshttpsgithubcombitnami-labssealed-secrets-to-manage-k8s-secrets">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#using-sealedsecrets-to-manage-k8s-secrets-link-to-heading"></a></h3>
<p>🟥 <em>Regret</em></p>
<p>My original thought was to push secret management to something GitOps styled. The two main drawbacks of using sealed-secrets were:</p>
<ul>
<li>It was more complicated for less infra knowledgeable developers to create/update secrets</li>
<li>We lost all the existing automations that AWS has around rotating secrets (<a href="https://aws.amazon.com/blogs/security/rotate-amazon-rds-database-credentials-automatically-with-aws-secrets-manager/">for example</a>)</li>
</ul>
<h3 id="using-externalsecrets-to-manage-k8s-secrets-link-to-heading">Using <a href="https://github.com/external-secrets/external-secrets">ExternalSecrets</a> to manage k8s secrets <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#using-externalsecretshttpsgithubcomexternal-secretsexternal-secrets-to-manage-k8s-secrets">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#using-externalsecrets-to-manage-k8s-secrets-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>ExternalSecrets has worked very well to sync AWS -> Kubernetes secrets. The process is simple for developers to understand and lets us take advantage of terraform as a way to easily create/update the secrets inside AWS, as well as give users a UI to use to create/update the secrets.</p>
<h3 id="using-externaldns-to-manage-dns-link-to-heading">Using <a href="https://github.com/kubernetes-sigs/external-dns">ExternalDNS</a> to manage DNS <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#using-externaldnshttpsgithubcomkubernetes-sigsexternal-dns-to-manage-dns">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#using-externaldns-to-manage-dns-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>ExternalDNS is a great product. It syncs our Kubernetes -> Route53 DNS entries and has given us very little problems in the past 4 years.</p>
<h3 id="using-cert-manager-to-manage-ssl-certificates-link-to-heading">Using <a href="https://github.com/cert-manager/cert-manager">cert-manager</a> to manage SSL certificates <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#using-cert-managerhttpsgithubcomcert-managercert-manager-to-manage-ssl-certificates">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#using-cert-manager-to-manage-ssl-certificates-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Very intuitive to configure and has worked well with no issues. Highly recommend using it to create your Let’s Encrypt certificates for Kubernetes. The only downside is we sometimes have ANCIENT (SaaS problems am I right?) tech stack customers that don’t trust Let’s Encrypt, and you need to go get a paid cert for those.</p>
<h3 id="bottlerocket-for-eks-link-to-heading"><a href="https://aws.amazon.com/bottlerocket/">Bottlerocket</a> for EKS <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#bottlerockethttpsawsamazoncombottlerocket-for-eks">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#bottlerocket-for-eks-link-to-heading"></a></h3>
<p>🟥 <em>Regret</em></p>
<p>Our EKS cluster used to run on Bottlerocket. The main downside was we frequently ran into networking CSI issues and debugging the bottlerocket images were much harder than debugging the standard EKS AMIs. Using the <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html">EKS optimized</a> AMIs for our nodes has given us no problems, and we still have a backdoor to debug the node itself when there are strange networking issues.</p>
<h3 id="picking-terraform-over-cloudformation-link-to-heading">Picking <a href="https://www.terraform.io/">Terraform</a> over <a href="https://aws.amazon.com/pm/cloudformation/">Cloudformation</a> <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#picking-terraformhttpswwwterraformio-over-cloudformationhttpsawsamazoncompmcloudformation">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#picking-terraform-over-cloudformation-link-to-heading"></a></h3>
<p>🟩 <em>Endorse</em></p>
<p>Using Infrastructure as Code is a must for any company. Being in AWS, the two main choices are Cloudformation and Terraform. I’ve used both and don’t regret sticking with Terraform. It’s been easy to extend for other SaaS providers (like Pagerduty), the syntax is easier to read than CloudFormation, and hasn’t been a blocker or slowdown for us.</p>
<h3 id="not-using-more-code-ish-iac-solutions-pulumi-cdk-etc-link-to-heading">Not using more code-ish IaC solutions (<a href="https://www.pulumi.com/">Pulumi</a>, <a href="https://aws.amazon.com/cdk/">CDK</a>, etc) <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#not-using-more-code-ish-iac-solutions-pulumihttpswwwpulumicom-cdkhttpsawsamazoncomcdk-etc">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#not-using-more-code-ish-iac-solutions-pulumi-cdk-etc-link-to-heading"></a></h3>
<p>🟩 <em>No Regrets</em></p>
<p>While Terraform and CloudFormation are data files (HCL and YAML/JSON) that describe your infrastructure, solutions like Pulumi or CDK allow you to write code that does the same. Code is of course powerful, but I’ve found the restrictive nature of Terraform’s HCL to be a benefit with reduced complexity. It’s not that it’s impossible to write complex Terraform: it’s just that it’s more obvious when it’s happening.</p>
<p>Some of these solutions, like Pulumi, were invented many years ago while Terraform lacked a lot of the features it has today. Newer versions of Terraform have integrated a lot of the features that we can use to reduce complexity. We instead use a middleground that generates basic skeletons of our Terraform code for parts we want to abstract away.</p>
<h3 id="not-using-a-network-mesh-istiolinkerdetc-link-to-heading">Not using a network mesh (<a href="https://istio.io/">istio</a>/<a href="https://linkerd.io/">linkerd</a>/etc) <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#not-using-a-network-mesh-istiohttpsistioiolinkerdhttpslinkerdioetc">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#not-using-a-network-mesh-istiolinkerdetc-link-to-heading"></a></h3>
<p>🟩 <em>No regrets</em></p>
<p>Network meshes are really cool and a lot of smart people tend to endorse them, so I’m convinced they are fine ideas. Unfortunately, I think companies underestimate the complexity of things. My general infrastructure advice is “less is better”.</p>
<h3 id="nginx-load-balancer-for-eks-ingress-link-to-heading"><a href="https://github.com/kubernetes/ingress-nginx">Nginx</a> load balancer for EKS ingress <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#nginxhttpsgithubcomkubernetesingress-nginx-load-balancer-for-eks-ingress">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#nginx-load-balancer-for-eks-ingress-link-to-heading"></a></h3>
<p>🟩 <em>No Regrets</em></p>
<p>Nginx is old, it’s stable, and it’s battle tested.</p>
<h2 id="homebrew-for-company-scripts-link-to-heading"><a href="https://brew.sh/">homebrew</a> for company scripts <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#homebrewhttpsbrewsh-for-company-scripts">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#homebrew-for-company-scripts-link-to-heading"></a></h2>
<p>🟩 <em>Endorse</em></p>
<p>Your company will likely need a way to distribute scripts and binaries for your engineers to use. Homebrew has worked <em>well enough</em> for both linux and Mac users as a way to distribute scripts and binaries.</p>
<h2 id="go-for-services-link-to-heading"><a href="https://go.dev/">Go</a> for services <a href="https://cep.dev/posts/every-infrastructure-decision-i-endorse-or-regret-after-4-years-running-infrastructure-at-a-startup/#gohttpsgodev-for-services">Link to heading</a><a aria-hidden="true" class="anchor-heading icon-link" href="#go-for-services-link-to-heading"></a></h2>
<p>🟩 <em>Endorse</em></p>
<p>Go has been easy for new engineers to pick up and is a great choice overall. For non-GPU services that are mostly network IO bound, Go should be your default language choice.</p>
<h1 id="the-7-most-influential-papers-in-computer-science-history">The 7 Most Influential Papers in Computer Science History<a aria-hidden="true" class="anchor-heading icon-link" href="#the-7-most-influential-papers-in-computer-science-history"></a></h1>
<blockquote>
<p><a href="https://terriblesoftware.org/2025/01/22/the-7-most-influential-papers-in-computer-science-history/">https://terriblesoftware.org/2025/01/22/the-7-most-influential-papers-in-computer-science-history/</a></p>
</blockquote>
<p>Before we begin, let me be clear: yes, this <em>is</em> a subjective list. It’s not meant to end the debate — but to start it. These seven papers (sorted by date) stand out to me mostly because of their impact in today’s world. Honestly, each one deserves a blog post (or even a book!) of its own — but let’s keep it short for now. If your favorite doesn’t show up here, don’t worry, stick around for the bonus section at the end, where I’ll call out a few more that came <em>this</em> close to making the main list. So let’s dive in!</p>
<h3 id="1-on-computable-numbers-with-an-application-to-the-entscheidungsproblem-1936"><strong>1. “On Computable Numbers, with an Application to the Entscheidungsproblem” (1936)</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#1-on-computable-numbers-with-an-application-to-the-entscheidungsproblem-1936"></a></h3>
<p><strong>Author:</strong> Alan Turing</p>
<p>It’s the 1930s, and a “programmable machine” sounds like something out of a sci-fi novel. Then along comes Alan Turing, laying the groundwork for what computers can theoretically do. He sketches out a hypothetical “Turing Machine,” proving that, if something is computable at all, a machine (in principle) can handle it.</p>
<h4 id="the-big-idea"><strong>The big idea</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#the-big-idea"></a></h4>
<p>Turing’s simple model — just a tape, a head for reading/writing, and a finite set of states, turned into the granddaddy of all modern computation. It defined what’s solvable (and what’s not) in a purely mechanical sense, basically giving us the “rules of the game” for digital problem-solving.</p>
<h4 id="why-it-matters-today"><strong>Why it matters today</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#why-it-matters-today"></a></h4>
<p>Every single programming language, every single piece of code out there, is playing by Turing’s rules. Even when we talk about quantum computing, we’re still referencing the boundaries Turing described. That’s a huge testament to the power of one paper published in the mid-1930s.</p>
<h4 id="learn-more"><strong>Learn more</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#learn-more"></a></h4>
<ul>
<li><a href="https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf">https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Turing%27s_proof">https://en.wikipedia.org/wiki/Turing%27s_proof</a></li>
<li><a href="https://www.youtube.com/watch?v=dNRDvLACg5Q">https://www.youtube.com/watch?v=dNRDvLACg5Q</a></li>
</ul>
<h3 id="2-a-mathematical-theory-of-communication-1948"><strong>2. “A Mathematical Theory of Communication” (1948)</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#2-a-mathematical-theory-of-communication-1948"></a></h3>
<p><strong>Author:</strong> Claude Shannon</p>
<p>Now that Turing showed us what machines can (and can’t) do, how do we actually move information around? Enter Claude Shannon, who basically invented <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a> so we could talk about bits, entropy, and noisy channels in a rigorous way.</p>
<h4 id="the-big-idea-1"><strong>The big idea</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#the-big-idea-1"></a></h4>
<p>Shannon took the abstract notion of “information” and turned it into something a little <em>bit</em> (pun intended) more measurable. This helped us figure out how to pack data more efficiently (compression) and how to protect it from errors (error-correcting codes), whether we’re sending signals into space or streaming Netflix on a Friday night.</p>
<h4 id="why-it-matters-today-1"><strong>Why it matters today</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#why-it-matters-today-1"></a></h4>
<p>Every single time you send a text, stream a video, or call your mom on FaceTime, you’re using Shannon’s ideas. Without them, you’d be dealing with a lot more scrambled audio and jumbled data, trust me.</p>
<h4 id="learn-more-1"><strong>Learn more</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#learn-more-1"></a></h4>
<ul>
<li><a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication">https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication</a></li>
<li><a href="https://www.youtube.com/watch?v=b6VdGHSV6qg">https://www.youtube.com/watch?v=b6VdGHSV6qg</a></li>
<li><a href="https://www.youtube.com/watch?v=kP0zi5lX-Fo">https://www.youtube.com/watch?v=kP0zi5lX-Fo</a></li>
</ul>
<h3 id="3-a-relational-model-of-data-for-large-shared-data-banks-1970"><strong>3. “A Relational Model of Data for Large Shared Data Banks” (1970)</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#3-a-relational-model-of-data-for-large-shared-data-banks-1970"></a></h3>
<p><strong>Author:</strong> Edgar F. Codd</p>
<p>So, we can compute and communicate — awesome. But eventually, we’re buried under mountains of data. Edgar F. Codd saw this coming and introduced the relational model, which is basically the reason we’re able to store and query data.</p>
<h4 id="the-big-idea-2"><strong>The big idea</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#the-big-idea-2"></a></h4>
<p>Codd said, “Let’s store data in tables and manipulate it with logical operations.” This might sound obvious now, but at the time it was revolutionary. His blueprint led to SQL and the huge family of relational databases that power, oh, basically every bank, retail website, and enterprise system you can imagine.</p>
<h4 id="why-it-matters-today-2"><strong>Why it matters today</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#why-it-matters-today-2"></a></h4>
<p>Even in the NoSQL era, the underlying concepts of how we organize data (tables, schemas, consistency) trace right back to Codd. If you ever wrote a SQL query in your life — it’s all thanks to him.</p>
<h4 id="learn-more-2"><strong>Learn more</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#learn-more-2"></a></h4>
<ul>
<li><a href="https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf">https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Codd%27s_12_rules">https://en.wikipedia.org/wiki/Codd%27s_12_rules</a></li>
</ul>
<h3 id="4-the-complexity-of-theorem-proving-procedures-1971"><strong>4. “The Complexity of Theorem-Proving Procedures” (1971)</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#4-the-complexity-of-theorem-proving-procedures-1971"></a></h3>
<p><strong>Author:</strong> Stephen A. Cook</p>
<p>Now that we’re storing data efficiently, what about the computation itself? Turns out some problems are just…painfully hard. Stephen Cook’s paper introduced NP-completeness, a concept that basically says, “Yep, some tasks are so difficult that even supercomputers sweat.”</p>
<h4 id="the-big-idea-3"><strong>The big idea</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#the-big-idea-3"></a></h4>
<p>Cook showed that the <a href="https://en.wikipedia.org/wiki/Boolean_satisfiability_problem">Boolean satisfiability problem</a> (SAT) is NP-complete, meaning if you magically solve SAT quickly, you’ve instantly cracked a whole bunch of other seemingly impossible problems. This created a universal language for talking about problem difficulty.</p>
<h4 id="why-it-matters-today-3"><strong>Why it matters today</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#why-it-matters-today-3"></a></h4>
<p>Whenever you see “NP-hard” in a problem description, or wonder why route optimization kills your CPU, that’s Cook’s legacy. It led to huge developments in algorithms, cryptography, and the hunt for efficient solutions (or at least decent approximations).</p>
<h4 id="learn-more-3"><strong>Learn more</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#learn-more-3"></a></h4>
<ul>
<li><a href="https://www.inf.unibz.it/~calvanese/teaching/14-15-tc/material/cook-1971-NP-completeness-of-SAT.pdf">https://www.inf.unibz.it/~calvanese/teaching/14-15-tc/material/cook-1971-NP-completeness-of-SAT.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/P_versus_NP_problem">https://en.wikipedia.org/wiki/P_versus_NP_problem</a></li>
<li><a href="https://www.youtube.com/watch?v=dJUEkjxylBw">https://www.youtube.com/watch?v=dJUEkjxylBw</a></li>
</ul>
<h3 id="5-a-protocol-for-packet-network-intercommunication-1974"><strong>5. “A Protocol for Packet Network Intercommunication” (1974)</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#5-a-protocol-for-packet-network-intercommunication-1974"></a></h3>
<p><strong>Authors:</strong> Vinton G. Cerf and Robert E. Kahn</p>
<p>Great, we have tough problems to solve and data to store — but how do we hook all these computers together? Cerf and Kahn’s TCP turned isolated networks into an interconnected web, letting data hop around the planet in tiny packets.</p>
<h3 id="the-big-idea-4"><strong>The big idea</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#the-big-idea-4"></a></h3>
<p>They created a universal language for different networks to talk. Packets get split up, zipped through various routes, and reassembled on the other side. This flexibility opened the door for global connectivity — no single monolithic network required.</p>
<h3 id="why-it-matters-today-4"><strong>Why it matters today</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#why-it-matters-today-4"></a></h3>
<p>Short answer? Pretty much the entire internet. Whenever you browse the web, send an email, or securely log in to your bank’s website, you’re leaning on TCP/IP to move those bits around reliably. Sure, some real-time applications might use UDP, but the core idea of IP-based networking — laid out by Cerf and Kahn — still unites all our devices under one global network.</p>
<h3 id="learn-more-4"><strong>Learn more</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#learn-more-4"></a></h3>
<ul>
<li><a href="https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf">https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf</a></li>
<li><a href="https://www.youtube.com/watch?v=PG9oKZdFb7w">https://www.youtube.com/watch?v=PG9oKZdFb7w</a></li>
</ul>
<h3 id="6-information-management-a-proposal-1989"><strong>6. “Information Management: A Proposal” (1989)</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#6-information-management-a-proposal-1989"></a></h3>
<p><strong>Author:</strong> Tim Berners-Lee</p>
<p>And speaking of TCP/IP — once machines could talk to each other easily, Tim Berners-Lee asked, “How about we make this thing friendly for everyone?” That’s where the World Wide Web was born.</p>
<h3 id="the-big-idea-5"><strong>The big idea</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#the-big-idea-5"></a></h3>
<p>Berners-Lee pitched a global hypertext system, complete with hyperlinks, URLs, and HTTP. Suddenly, documents around the world were no longer isolated; they were “webbed” together, turning the internet into something normal people (not just scientists) could navigate.</p>
<h3 id="why-it-matters-today-5"><strong>Why it matters today</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#why-it-matters-today-5"></a></h3>
<p>We live on the web. Whether it’s social media, online shopping, or reading obscure blog posts at 3 a.m., the seeds of it all came from this straightforward proposal. It changed how we share knowledge forever.</p>
<h3 id="learn-more-5">Learn more<a aria-hidden="true" class="anchor-heading icon-link" href="#learn-more-5"></a></h3>
<ul>
<li><a href="https://cds.cern.ch/record/369245/files/dd-89-001.pdf">https://cds.cern.ch/record/369245/files/dd-89-001.pdf</a></li>
<li><a href="https://time.com/21039/tim-berners-lee-web-proposal-at-25/">https://time.com/21039/tim-berners-lee-web-proposal-at-25/</a></li>
<li><a href="https://www.youtube.com/watch?v=qJNrvVv7SdU">https://www.youtube.com/watch?v=qJNrvVv7SdU</a></li>
</ul>
<h3 id="7-the-anatomy-of-a-large-scale-hypertextual-web-search-engine-1998"><strong>7. “The Anatomy of a Large-Scale Hypertextual Web Search Engine” (1998)</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#7-the-anatomy-of-a-large-scale-hypertextual-web-search-engine-1998"></a></h3>
<p><strong>Authors:</strong> Sergey Brin and Larry Page</p>
<p>Once Berners-Lee’s web blew up, it became a jungle of links, pages, and cat memes. Sergey Brin and Larry Page decided to tame that jungle. Their approach, based on link analysis, evolved into the search engine we now call “Google.”</p>
<h3 id="the-big-idea-6"><strong>The big idea</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#the-big-idea-6"></a></h3>
<p>They introduced PageRank, which viewed links as votes of confidence rather than just a new dimension to count keywords. The result was a seismic jump in relevant search results, making the web feel, well…searchable.</p>
<h3 id="why-it-matters-today-6"><strong>Why it matters today</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#why-it-matters-today-6"></a></h3>
<p>Type a question into Google and get an instant answer? That’s PageRank (and a lot of subsequent innovation) at work. It redefined how we navigate information online and kicked off a new era of data-driven tech—ads, analytics, machine learning, you name it.</p>
<h3 id="learn-more-6">Learn more<a aria-hidden="true" class="anchor-heading icon-link" href="#learn-more-6"></a></h3>
<ul>
<li><a href="https://snap.stanford.edu/class/cs224w-readings/Brin98Anatomy.pdf">https://snap.stanford.edu/class/cs224w-readings/Brin98Anatomy.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/PageRank">https://en.wikipedia.org/wiki/PageRank</a></li>
<li><a href="https://www.youtube.com/watch?v=v7n7wZhHJj8&#x26;t=184s">https://www.youtube.com/watch?v=v7n7wZhHJj8&#x26;t=184s</a></li>
</ul>
<hr>
<h2 id="bonuses-5-that-almost-made-the-list">Bonuses (5 That Almost Made the List)<a aria-hidden="true" class="anchor-heading icon-link" href="#bonuses-5-that-almost-made-the-list"></a></h2>
<h3 id="1-recursive-functions-of-symbolic-expressions-and-their-computation-by-machine-1960--john-mccarthy">1. <strong>“Recursive Functions of Symbolic Expressions and Their Computation by Machine” (1960) – John McCarthy</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#1-recursive-functions-of-symbolic-expressions-and-their-computation-by-machine-1960--john-mccarthy"></a></h3>
<p>Introduced Lisp and the functional programming style that still sneaks into modern languages and frameworks.</p>
<h3 id="2-go-to-statement-considered-harmful-1968--edsger-dijkstra">2. <strong>“Go To Statement Considered Harmful” (1968) – Edsger Dijkstra</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#2-go-to-statement-considered-harmful-1968--edsger-dijkstra"></a></h3>
<p>A short but fiery editorial that argued <strong>goto</strong> leads to messy, unstructured code, sparking the structured programming revolution.</p>
<h3 id="3-time-clocks-and-the-ordering-of-events-in-a-distributed-system-1978--leslie-lamport">3. <strong>“Time, Clocks, and the Ordering of Events in a Distributed System” (1978) – Leslie Lamport</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#3-time-clocks-and-the-ordering-of-events-in-a-distributed-system-1978--leslie-lamport"></a></h3>
<p>You can’t sync real clocks perfectly in distributed systems, so you need logical ones. This is a must-read if you’re into distributed computing.</p>
<h3 id="4-no-silver-bulletessence-and-accident-in-software-engineering-1986--fred-brooks">4. <strong>“No Silver Bullet—Essence and Accident in Software Engineering” (1986) – Fred Brooks</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#4-no-silver-bulletessence-and-accident-in-software-engineering-1986--fred-brooks"></a></h3>
<p>Brooks argued that there’s no single magical fix for the inherent complexity of software development. Decades later, as we chase the “next big thing” in frameworks or methodologies, his message remains a sobering reminder that some problems are just hard.</p>
<h3 id="5-attention-is-all-you-need-2017--vaswani-et-al">5. <strong>“Attention Is All You Need” (2017) – Vaswani et al.</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#5-attention-is-all-you-need-2017--vaswani-et-al"></a></h3>
<p>The transformer architecture behind GPT and other big-name AI models. If you’re impressed by large language models, here’s your blueprint.</p>
<h2 id="conclusion"><strong>Conclusion</strong><a aria-hidden="true" class="anchor-heading icon-link" href="#conclusion"></a></h2>
<p>These days, we’re flooded with new stuff: fresh languages, mind-blowing AI breakthroughs, quantum leaps, and the JavaScript framework of the week. It’s all super exciting, but here’s the thing: foundations matter. Without them, we’re just piling on new toys without fully understanding the ground we’re building on. The papers in this post are a reminder of where our core concepts — data structures, algorithms, the very web — came from.</p>